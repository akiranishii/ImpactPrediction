{"paper_id": "u3xwwfHmBC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. Innovative Approach: TTformer introduces a unique concept of tense-specific attention modules to capture different dimensions of traffic flow, which appears to be a novel and effective method for improving traffic forecasting models.\n2. Robustness to Incomplete Data: The model's use of contrastive learning with negative filtering provides robustness against incomplete traffic data, potentially increasing the model's applicability in real-world scenarios where data may be missing or noisy.\n3. Improved Forecasting Accuracy: Empirical results show that TTformer significantly outperforms existing models, highlighting its effectiveness in capturing spatio-temporal dependencies and improving traffic forecasting accuracy.", "Weaknesses": "1. Clarity and Completeness: The paper's explanation of the tri-tense attention modules and negative filtering technique could be more detailed to fully convey the methodology.\n2. Comparisons and Benchmarks: While the model demonstrates performance improvements, it's unclear how it compares across diverse datasets or in different traffic conditions beyond those tested.\n3. Computational Complexity: The addition of three tense-specific attention modules may increase computational demands, yet the impact on model efficiency and scalability is not addressed or quantified in the paper.", "Questions": "1. How does TTformer handle the balance between capturing long-term dependencies and computational efficiency, given the potential overhead from three separate attention modules?\n2. What specific datasets and evaluation metrics were used to demonstrate the model's performance? Are there plans to validate the model on additional or more varied traffic datasets?\n3. Can the approach be generalized or adapted for use in other time series forecasting applications beyond traffic flow? If so, what modifications would be necessary?"}}
{"paper_id": "YKvBiRWdQC", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel benchmark specifically designed to evaluate zero-shot cooperation abilities of reinforcement learning agents in novel settings, which is crucial for improving human-AI cooperation. It leverages state-of-the-art dual curriculum design methods, offering a new way to assess AI capabilities in a multi-agent cooperative environment. The benchmark is open-source and GPU-accelerated, promoting accessibility and broad use within the community.", "Weaknesses": "The paper reports that state-of-the-art DCD algorithms struggle with this new challenge, indicating potential limitations or areas for improvement in current methodologies. The challenge may require further refinement to ensure it adequately captures the complexity and variability of real-world scenarios. Additionally, there is a need for more clarity on how different components of the benchmark specifically address zero-shot cooperation outcomes.", "Questions": "What are the specific aspects of the Overcooked environment that pose the greatest challenge to existing DCD algorithms? How does this new benchmark compare in terms of difficulty to traditional cooperative benchmarks, and are there plans to extend it to other environments?"}}
{"paper_id": "tePFpDgyqg", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces LongPack, an innovative method for constructing high-quality long context data essential for training large language models. By focusing on referral relationships, especially in web pages using hyperlinks, the approach effectively addresses the challenge of data shortage for long-context training. The scalability of LongPack is impressive, achieving a corpus size equivalent to a full pretraining dataset with minimal input. The substantial performance improvement over previous methods further underscores its efficacy.", "Weaknesses": "The approach relies heavily on the quality and existence of hyperlinks, which might not be present in all data types. The assumption that hyperlinks naturally reflect the desired long-distance referrals might not hold universally, potentially limiting the applicability of the method to certain types of documents. Additionally, while the results are promising, more details on the exact implementation and potential limitations would provide a clearer understanding of the method's robustness.", "Questions": "How does LongPack handle documents or contexts without natural hyperlinks or clear referral relationships? Is there a fallback mechanism in place for such scenarios? Could the approach be extended or adapted to other types of referral signals in non-web datasets?"}}
{"paper_id": "gx3LMRB15C", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The proposal of an augmentation-free SSL method specifically for tabular data is novel and addresses a significant gap in the handling of structured data. The approach seems to offer impressive performance benefits over conventional methods in both classification and regression tasks. The introduction of regularization tokens adds a new dimension to training JEPA models with structured data.", "Weaknesses": "The effectiveness of the method is yet to be validated across a broader range of datasets and conditions. The dependence on JEPA might limit the generalization to other models. Furthermore, the novelty could be questioned in comparison to existing self-supervised methods for unstructured data, leaving room for detailed comparisons to be desired.", "Questions": "How does T-JEPA compare specifically with other SSL methods for tabular data? Are there other comparable models beyond Gradient Boosted Decision Trees that could challenge the claim of superior performance? Could the authors provide more clarity on the role and impact of regularization tokens in long-term training stability and performance?"}}
{"paper_id": "kTH3bEH6hW", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses a critical challenge for tabular data analysis in the context of few-shot learning, which is increasingly relevant in practical situations where labeled data is scarce. \n2. It proposes a novel data augmentation method tailored specifically for tabular data, allowing contrastive learning to be more effective in these domains.\n3. The introduction of a benchmark (FeSTa) comprising a wide range of datasets and algorithms provides a valuable resource for the research community, potentially fostering more robust evaluations and comparisons.", "Weaknesses": "1. The paper relies heavily on an augmentation technique that may not be broadly applicable beyond tabular data, thus limiting its generality to other forms of data input.\n2. The novelty of the augmentation strategy, while beneficial for tabular domains, could be perceived as incremental given its basis on established contrastive learning frameworks.\n3. There might be an over-reliance on benchmark performance rather than exploring the theoretical justifications and frameworks that underlie the proposed method.", "Questions": "1. How does the proposed augmentation maintain a balance between introducing variety within samples and preserving label consistency, especially when shuffling values?\n2. The benchmark results demonstrate robustness in 1-shot learning scenarios, but how do these methods perform as the number of shots increases?\n3. What are the implications for transfer learning scenarios, where tabular data might need adaptation from previously learned representations across different domains?"}}
{"paper_id": "EqcLAU6gyU", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper proposes a novel framework for agent-oriented planning in multi-agent systems by identifying critical design principles of solvability, completeness, and non-redundancy. It effectively addresses the task decomposition and allocation process, providing a robust solution for real-world problems and is complemented by a feedback loop to enhance system performance. Comprehensive experiments validate the framework's advantage over existing systems.", "Weaknesses": "The abstract lacks specific details on the implementation or evaluation metrics used in the experiments, which may make it difficult to assess the full impact of the proposed framework. Additionally, while the principles and framework are clearly stated, the adaptation of these principles in diverse multi-agent environments is not thoroughly discussed.", "Questions": "1. What specific metrics were used to show the advancement of the proposed framework over existing systems?\n2. How does the feedback loop specifically contribute to enhancing the effectiveness and robustness of the multi-agent systems?\n3. Can the framework be easily adapted for different types of multi-agent environments beyond those tested in the experiments?"}}
{"paper_id": "qrTrnrEi9d", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The TransFusion framework introduces a novel use of English translations to enhance IE tasks in low-resource languages. The approach shows significant improvements in cross-lingual transfer and achieves notable performance gains in NER tasks across multiple languages and model architectures.", "Weaknesses": "The reliance on English translations could be a limitation if high-quality translations are not available. The approach may also face challenges with ambiguous or contextually complex source texts. Further clarity on the underlying architecture and method specifics would strengthen understanding.", "Questions": "What are the potential limitations when applying TransFusion to languages with significantly different syntax from English? How dependent are the improvements on the quality of the translations used in the experiments? Are there plans to extend this framework to encompass more low-resource languages beyond those currently tested?"}}
{"paper_id": "Bp0HBaMNRl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper presents a novel differentiable causal discovery algorithm that addresses scalability and nonlinearity in latent causal models. The theoretical results contribute significantly to the causality literature by relaxing strong assumptions about latent variables and noise. The algorithm's superior performance in both accuracy and scalability demonstrates its potential in practical applications, particularly with high-dimensional image data.", "Weaknesses": "The paper may not provide sufficient details on the implementation and specific steps of the proposed algorithm. Additionally, the experiments might lack a diverse set of datasets to fully prove the algorithm's robustness across different domains.", "Questions": "How does the proposed method deal with different types of noise that may be present in observational data? Are there any specific limitations in terms of the types of datasets the method can handle effectively?"}}
{"paper_id": "JzLcKWtGnl", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel approach by enhancing spatial perception and reasoning using a progressive spatial awareness scheme. This is achieved by enriching spatial embeddings and introducing new tasks such as 3D object distance measurement, which could inspire further advancements in 3D spatial perception capabilities.", "Weaknesses": "The paper lacks clarity in methodology and experimental setup, which could affect reproducibility. The contribution of the new MODEL dataset in achieving the results is not clearly delineated. There is also a lack of benchmarking against existing leading methods addressing similar problems, which raises questions about the claimed state-of-the-art performance.", "Questions": "1. How does the proposed progressive spatial awareness scheme quantitatively compare to other spatial embedding methods?\n2. Can you provide benchmarks comparing Spatial 3D-LLM with other state-of-the-art methods on the same tasks?\n3. How significant is the impact of the MODEL dataset on the model's performance compared to traditional datasets?\n4. Could more details be provided on the implementation details or model architecture used in the Spatial 3D-LLM?"}}
{"paper_id": "hXJrQWIoR3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper identifies a gap in the explainability of graph representation learning and addresses this by introducing a novel framework that integrates graph pattern analysis. The method effectively bridges the gap between graph kernels and explainable representations, which can enhance comprehension of graph-based models significantly. The theoretical analysis provided for robustness and generalization strengthens the credibility of the proposed method. The experimental results showing the effectiveness of the approach in both supervised and unsupervised tasks further support its applicability.", "Weaknesses": "The approach may inherit the limitations of graph kernels, such as neglecting higher-level semantic features and potentially being computationally expensive for large graphs. The abstraction level for node features may not be thorough enough to capture all relevant information, which could affect the model's utility in certain contexts. Also, while theoretical analyses are beneficial, the presentation lacks detailed proofs or intuitions about some of the claims made, which could be critical for a deeper understanding of the framework's limitations.", "Questions": "1. How does the framework scale with very large graphs in real-world scenarios? Are there any optimization strategies for efficiency?\n2. What are the potential downsides regarding interpretability when using your approach compared to model-level or instance-level interpretability methods?\n3. Can the method be adapted or expanded for heterogeneous graphs, where nodes and edges have different types or attributes?"}}
{"paper_id": "67sSPPAZiG", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper provides a comprehensive exploration and contribution towards egocentric video understanding by creating a large-scale QA dataset, which could significantly advance the field.\n2. The proposed architecture with the novel 'Memory Pointer Prompting' mechanism offers a unique method to improve model comprehension over extensive video content, showcasing innovation in handling multimodal data.\n3. The new benchmark facilitates a clear way to evaluate and improve multimodal models, potentially offering a standard that others in the field can build upon.", "Weaknesses": "1. While the dataset and benchmark are significant, the paper may rely heavily on these contributions without thoroughly exploring potential limitations or challenges faced during their compilation and integration.\n2. The evaluation method's de-biasing approach's details are not thoroughly explained, which could leave room for ambiguity regarding its effectiveness and implementation.\n3. Clarity and discussion on how the proposed model performs across different types and lengths of videos, beyond aggregate metrics, would be beneficial.", "Questions": "1. How does the model perform specifically on videos of varying lengths in terms of accuracy and processing time?\n2. Can the dataset and methodology be easily adapted to other forms of video content, such as non-egocentric videos?\n3. What specific steps were made to ensure high quality in the automatically generated QA samples?"}}
{"paper_id": "kjmLabjSE2", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper provides a novel analysis of Differential Privacy (DP) in the context of non-convex and non-smooth loss functions, filling an important gap in the existing literature. The approach demonstrates a clear understanding of the challenges in extending DP guarantees beyond simple convex functions to more complex settings encountered in practical machine learning. The introduction of bounds based on H\\\"older gradient continuity is innovative and has the potential for high impact.", "Weaknesses": "While the theoretical development is strong, the paper may lack practical examples or experiments that clearly showcase the real-world utility of the newly derived privacy bounds. The mathematical rigor could also make the paper less accessible to those not specialized in privacy or optimization theory.", "Questions": "1. Are there specific machine learning applications or scenarios where the new DP bounds could be directly applied, and how do they compare empirically to traditional methods?\n2. Can the authors provide intuition or visualization for the implications of using Holder gradient continuity in DP settings?\n3. How might the techniques introduced in this paper influence future research in Differential Privacy, specifically for training machine learning models on sensitive data?"}}
{"paper_id": "I18MA5DjoP", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces a novel analytical framework, Neuron Predictability Lens (NPL), to understand neuron behavior in transformer-based LLMs.\n2. Provides both global and local analyses about neuron predictability, contributing to insights on model behavior and function.\n3. Empirical results on well-known models, LLaMA-2 and GPT-J, could offer practical applicability and validation of the proposed framework.", "Weaknesses": "1. Clarity issues in presentation, especially regarding key technical details and explanation of novel concepts like Neuron Predictability.\n2. The results might not be compelling enough to strongly establish the necessity or advantage of NPL.\n3. Some explanations around the methodology, especially in local analysis and neuron interpretation, could be deepened to better highlight the significance of findings.", "Questions": "1. How do you ensure the robustness and generalizability of your neuron predictability findings across different transformer model architectures?\n2. Could NPL be applied to other neural network frameworks beyond transformers, or is it highly specialized for this specific architecture?"}}
{"paper_id": "qpDqO7qa3R", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel approach for zero-shot video restoration utilizing pre-trained image restoration diffusion models. The method displays strong quantitative results and visual comparisons across various challenging datasets, demonstrating superior generalization across different degradations compared to traditional methods. Additionally, it offers versatility by being compatible with any 2D restoration diffusion model.", "Weaknesses": "The proposed method may lack theoretical rigor in the mathematical framework underpinning the hierarchical latent warping strategy and token merging process. The paper may also benefit from more detailed experimental analyses or a broader range of datasets to further substantiate the claims of generalization and robustness.", "Questions": "How does the hierarchical latent warping strategy specifically contribute to zero-shot video restoration performance? What are the computational overheads associated with the proposed approach compared to traditional methods? Is there any limitation to the types of video degradations the method can handle effectively?"}}
{"paper_id": "dSQtMx6dPE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important and timely problem of privacy in graph prompt learning. Two novel algorithms, DP-GPL and DP-GPL+W, are proposed for differentially private graph prompt learning using the PATE framework, showcasing innovative solutions to privacy leakage issues. The evaluation indicates that these methods maintain high utility while addressing privacy concerns effectively.", "Weaknesses": "The paper's soundness is hindered by insufficient experimental validation. The lack of demonstrations showing how effectively DP-GPL and DP-GPL+W can resist membership inference attacks after implementation raises concerns. Furthermore, the theoretical section lacks clarity on whether the proposed methods specifically target node-level or edge-level differential privacy, potentially leading to complications in practical applications.", "Questions": "1. Can the authors provide experimental results for membership inference attacks on models protected by DP-GPL and DP-GPL+W in comparison to unprotected models?\n2. How does the centrality-based teacher voting in DP-GPL+W contribute to its improved performance over DP-GPL? Is there any experimental or theoretical evidence demonstrating this advantage?\n3. Is the proposed method applicable to heterogeneous graphs, or is it limited to homogeneous graphs?\n4. Why do the experiments focus only on the k-shot scenario with k=5? Could the authors provide insights into performance across varying k-shot scenarios, especially with smaller k-values?\n5. How do the proposed methods ensure label privacy, given the lack of explicit mention in the theoretical section?"}}
{"paper_id": "GbgCRJedQ7", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper proposes a novel Sparse Matrix Tuning (SMT) method that effectively reduces computational and memory costs while narrowing the performance gap between parameter-efficient fine-tuning (PEFT) and full fine-tuning (FT).\n2. SMT demonstrates significant improvements over existing PEFT methods like LoRA and DoRA while maintaining lower GPU memory usage.\n3. The approach is validated across a broad spectrum of tasks and is evaluated on popular large language models such as LLaMA.", "Weaknesses": "1. The method's dependency on identifying the 'most significant' sub-matrices may limit its application to other model architectures or data distributions.\n2. The paper lacks comparisons with other recent advancements in delta-tuning or related pruning approaches.\n3. Some practical concerns such as the choice of block size and generalization of the mask selection process are not thoroughly discussed.", "Questions": "1. What criteria are used to determine the significance of sub-matrices in gradient updates, and how consistent are these criteria across different datasets and model architectures?\n2. Is the method applicable to models beyond those evaluated, and how does it handle variations in model size and layer configurations?\n3. How does the SMT method perform against other state-of-the-art techniques in delta tuning or pruning, and why were these not included in the evaluation?\n4. Can the approach be extended to domains beyond large language models, such as vision or multi-modal tasks, while retaining its benefits?"}}
{"paper_id": "ZZVOrId3yN", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "This work presents a novel architecture for multimodal medical image segmentation called CrossModalNet, offering rigorous mathematical analysis and introduction of the CMIF metric. The paper emphasizes a robust theoretical foundation and applicability to joint adversarial domain adaptation techniques, showcasing potential improvements in segmentation performance.", "Weaknesses": "While the paper introduces interesting concepts, the presentation of mathematical details and proofs needs to coincide with experimental validation across more datasets for generalizability. A detailed comparison with existing architectures using comprehensive state-of-the-art models, besides the MM-WHS dataset, may further solidify the claimed advancements.", "Questions": "How well does CrossModalNet perform on other datasets or medical imaging modalities? What specific advantages does the CMIF metric offer compared to existing fusion metrics?"}}
{"paper_id": "gLHuAYGs6a", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel approach to multi-view clustering by leveraging a heterogeneous graph and random walks, which is a promising direction. The use of a multi-step random walk strategy to capture high-order structures is innovative and could potentially improve clustering performance. The experimental section is comprehensive, involving five real-world datasets, indicating the method's applicability and robustness.", "Weaknesses": "The paper lacks detail on the computational complexity of the proposed method, which is crucial for understanding its practical applicability. While the concept of using heterogeneous graphs is interesting, there is limited discussion on why this approach is superior to other existing methods. Additionally, the theoretical foundation or proof of the algorithm's convergence and performance is missing, which might affect the reliability of the results.", "Questions": "What is the computational complexity of the proposed method, and how does it compare to existing multi-view clustering methods? Is there any theoretical guarantee or convergence proof for the algorithm? How does the method handle scalability with large datasets and numerous views?"}}
{"paper_id": "x5l5PRtvul", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper extends the theoretical understanding of the hybrid kernel variant of Stein Variational Gradient Descent (h-SVGD), providing insights into its ability to alleviate variance collapse without additional computational cost or target density factorization. The theoretical advancements, including the existence proof for the hybrid Stein PDE and the characterisation of a special case as a kernelised Wasserstein gradient flow, significantly contribute to the field.", "Weaknesses": "The presentation of the theoretical results might be challenging for practitioners who are less familiar with the advanced mathematical framework employed. Additionally, some assumptions and results might lack intuitive explanations or broader implications for how they influence practical applications.", "Questions": "1. The paper mentions a descent lemma and a large particle limit result. Can you provide more intuition or implications of these results for practical implementations?\n2. Has there been a consideration of how the bias in the mean field limiting distribution might be addressed or mitigated in practical applications?\n3. Could additional empirical comparisons with a broader range of SVGD variants strengthen the findings? Are there plans for such comparisons in the future work?"}}
{"paper_id": "pOUAVXnOQP", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The introduction of Sinusoidal Trainable Activation Functions (STAF) addresses a critical limitation in INRs related to spectral bias, enabling superior performance in modeling high-frequency details. The authors convincingly demonstrate how STAF surpasses existing methods like KAN, WIRE, and SIREN in both accuracy and training efficiency, making a strong case for its significance in improving the quality of continuous signal reconstruction. Comprehensive evaluations further underscore its robust performance.", "Weaknesses": "While the paper provides solid theoretical and empirical backing, a potential limitation might be the increased complexity in implementation compared to simpler activation functions. The extent of computational overhead should also be carefully considered, especially in resource-constrained environments.", "Questions": "How significant is the computational cost increase when using STAF compared to traditional ReLU or even fixed sine activation functions? Can STAF be easily integrated into existing INRs without substantial modification?"}}
{"paper_id": "iN64nSYt0z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a critical area in the field of LLMs, focusing on enhancing their ability to follow user instructions more accurately, which is a valuable contribution. The introduction of the new metric, Influence, to measure how instructions propagate through the transformer layers can provide useful insights. The proposed GUIDE method shows a significant improvement in instruction adherence, as demonstrated by quantitative results.", "Weaknesses": "The paper lacks a comprehensive comparison with existing methods that address similar problems, which would help contextualize its novelty and effectiveness. The absence of detailed discussions on potential trade-offs, such as effects on output fluency or coherence when altering attention mechanisms, is a concern. Additionally, the paper could benefit from more thorough experimental validation across diverse datasets and instruction types.", "Questions": "How does GUIDE perform in comparison to state-of-the-art methods focused on instruction alignment in LLMs? Can the proposed method be generalized across different LLM architectures without specific adaptations?"}}
{"paper_id": "gRuZkEy49k", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The methodology leverages the known connections between GFlowNets and maximum-entropy reinforcement learning for adapting MCTS, providing a new avenue for GFlowNet training. The theoretical framework is well-grounded, with proofs provided for the MCTS tree construction modification. The approach addresses practical strategies for applying MCTS within GFNs, demonstrated through comprehensive experiments across different parameterizations and objectives. The study shows improvement in training performance on discrete domains, indicating the practical relevance and applicability of the proposed technique.", "Weaknesses": "The presentation might benefit from clearer and more detailed explanations in certain sections, particularly for readers less familiar with certain background concepts. The experimental results, while extensive, could be strengthened by more detailed analysis and interpretation. Some aspects of how MCTS specifically enhances the sampling process compared to existing methods might need further elaboration to strengthen the case for its adoption. Additionally, insights into limitations and potential challenges of the proposed approach would strengthen the discussion.", "Questions": "1. Can you elaborate on any potential limitations or challenges when using MCTS for GFlowNet training in specific domains or under certain conditions?\n2. How does the proposed adaptation of MCTS handle higher-dimensional or more complex environments, and what modifications, if any, might be needed to ensure scalability?\n3. Could the methodology be applied effectively in continuous spaces, or is it uniquely suited for discrete spaces?"}}
{"paper_id": "d23EVDRJ6g", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "The paper presents a novel localized masked modeling paradigm which could address the challenge of overfitting when working with limited datasets in the animation domain. The introduction of a sliding window local attention in a masked transformer and the use of a distribution regularization method for quantization are appreciated contributions. The results promise advances in animation generation from a single motion reference, outperforming existing GAN and Diffusion-based methods.", "Weaknesses": "The work relies on existing concepts in the field, such as generative masked modeling and quantization, which may limit the overall originality. While the results are promising, the practical differences or improvements over existing methods like GANs or diffusion models, beyond the context of single reference motion, are not entirely clear from the abstract. Additionally, the extent of experiments and benchmarks against a broader range of state-of-the-art methods would enhance the validity of the claims. The abstract does not provide enough insight into the methodology for dealing with varying durations and topologies of input motion, which might affect general applicability.", "Questions": "1. How does MotionDreamer handle varying topologies and durations beyond what is presented in the single reference example? Are there inherent limitations in these contexts?\n2. Can MotionDreamer be adapted or scaled for motion synthesis in different media or applications beyond what is mentioned?\n3. Are there specific aspects where MotionDreamer does not perform as well compared to GAN or diffusion models? If so, what are the identified limitations?"}}
{"paper_id": "kQ5s9Yh0WI", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The approach is novel as it highlights an underexplored aspect of long-context large language models: the intrinsic limitation posed by training data. By focusing on expanding dataset availability to include longer outputs, the paper provides an incremental but effective way of improving LLM capabilities. Additionally, AgentWrite offers a practical solution to break down complex generation tasks into manageable subtasks, which is a valuable contribution to make existing LLMs more versatile.", "Weaknesses": "There could be more extensive experimentation to convincingly demonstrate the effectiveness of AgentWrite across different tasks and domains. The evaluation mainly focuses on the performance within new benchmarks constructed for this specific purpose, which may not fully capture real-world applicability. Furthermore, the paper could benefit from more in-depth discussions on the potential trade-offs involved in substantially increasing output lengths, such as computational cost or potential hallucinations in outputs.", "Questions": "How does AgentWrite handle coherence and consistency between subtasks, especially when attempting to ensure that the overall generated text maintains a seamless narrative or argument? What measures are in place to ensure that increasing output length does not lead to an increased likelihood of hallucinations or factual inaccuracies in the model's output?"}}
{"paper_id": "9CqkpQExe2", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The introduction of a dynamic Ada-K routing strategy for MoE-based LLMs is well-motivated and represents a significant innovation over static Top-K routing. The paper demonstrates clear advances in computational efficiency and model performance, supported by robust empirical evaluations. The approach is versatile, applicable across a range of MoE-based LLMs, and shows impressive metrics in terms of reduction in FLOPs and inference speedup. The inclusion of policy optimization for end-to-end learning of the routing decision adds a layer of sophistication absent in previous models.", "Weaknesses": "While the empirical results are strong, the paper could further explore the theoretical implications and limitations of the Ada-K strategy. The adaptability of the routing strategy under varying conditions and its potential impact on different types of contextual tasks requires more extensive exploration. Additionally, while the allocator modules are described as lightweight, there is no detailed analysis on the overhead introduced by these modules themselves compared to static Top-K routing.", "Questions": "How does the Ada-K routing strategy adapt when dealing with tokens that carry ambiguous contextual significance and how does it ensure that critical tokens are accurately identified without bias? Furthermore, are there specific types of tasks or datasets where the Ada-K strategy does not perform as expected, and how might this be addressed in future research? Finally, given the significant improvements shown by Ada-K, what are the implications of this routing strategy for real-time applications where quick decision-making is crucial?"}}
{"paper_id": "PpYy0dR3Qw", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces an innovative algorithm, LoCoDL, which combines local training and compression to efficiently address communication challenges in distributed and federated learning settings. This approach not only theoretically ensures reduced communication complexity but also outperforms existing methods in practical experiments. The use of a large class of unbiased compressors adds to its robustness across different applications.", "Weaknesses": "The paper primarily focuses on strongly convex functions, which might limit the generalizability to non-convex settings often encountered in practice. Additionally, while the algorithm outperforms existing methods, further empirical validation across diverse datasets and conditions would strengthen the claims.", "Questions": "1. How does the performance of LoCoDL compare to other algorithms in non-convex settings?\n2. Are there any specific limitations or scenarios where LoCoDL might not perform as expected?\n3. How does the choice of unbiased compressors affect the overall performance, and are there recommendations for selecting them?"}}
{"paper_id": "UatDdAlr2x", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper highlights interesting insights into how minimal architectural changes can significantly affect transformer performance, particularly in simple tasks. It systematically dissects the intricacies of transformer design choices and their effects on the solutions that emerge.", "Weaknesses": "While insightful, the focus on a simple synthetic task like histogram counting may limit the perceived applicability of the findings to more complex, real-world scenarios. The practical implications or applications of these insights are not thoroughly discussed.", "Questions": "Do you intend to extend this study to more complex tasks or real-world scenarios to observe if the same architectural sensitivities persist? What additional insights can be drawn about token-mixing and capacity when considering multi-layer transformers?"}}
{"paper_id": "IwgmgidYPS", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The paper presents a novel dataset, MedTrinity-25M, which significantly enhances the scope and scale for medical imaging tasks by providing multigranular annotations without the need for text-image pairs.", "Weaknesses": "1. The automated pipeline for creating the dataset might not capture the nuances of expert human annotation, which could affect the quality of the dataset.\n2. There is a potential risk of bias introduced by the reliance on pre-existing domain-specific models for region of interest identification.", "Questions": "1. How does the automated annotation process compare in quality and accuracy to manual expert annotation? Is there validation performed?\n2. Are there any measures in place to address potential biases introduced by the domain-specific models used in the pipeline?\n3. What are the plans for updating and maintaining the alignment of this dataset with evolving medical standards and new imaging modalities?"}}
{"paper_id": "WG7GzGx3G9", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel method called Rotated Runtime Smooth (RRS) for activation smoothing in large language models, which effectively addresses the challenge of INT4 quantization by reducing outliers in activations. The proposed solution demonstrates a significant improvement in WikiText-2 perplexity, outperforming existing state-of-the-art methods.", "Weaknesses": "The paper could provide more comprehensive details on the computational overhead and practical implications of integrating the Rotation operation and Runtime Smooth into existing systems. Additionally, the runtime performance improvements over existing methods should be explicitly demonstrated and quantified.", "Questions": "How does the proposed RRS method impact the computational cost and latency compared to existing methods? Can the approach be generalized to other tasks or models beyond the ones evaluated in the paper?"}}
{"paper_id": "BhECSDSkAE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses a significant challenge in medical imaging by proposing a novel task and methodology for one-shot video object segmentation using static image datasets.\n2. The inclusion of a comprehensive dataset (OS-I2V-Seg) to advance research in medical video segmentation is commendable.\n3. The use of a memory mechanism and self-distillation for test-time training to adapt the model from images to video frames reflects an innovative approach.", "Weaknesses": "1. The paper could benefit from more detailed explanations on the construction and effectiveness of the test-time training strategy employed, as well as more insights on the memory mechanism used.\n2. There is a lack of clarity on how well the proposed method can generalize to different medical imaging modalities or be adapted to non-medical videos.\n3. The presentation could be improved by providing more detailed descriptions of the experimental setup and discussing the limitations of the approach more comprehensively.", "Questions": "1. Could the authors provide more clarity on the implementation aspects of the memory mechanism and how it scales with different video lengths or complexities?\n2. Is the proposed OS-I2V-Seg dataset specific to certain types of medical videos, and how diverse is it in representing different medical conditions or imaging techniques?\n3. How does the performance of this approach compare to traditional fully-annotated video segmentation methods in terms of accuracy and computational efficiency?"}}
{"paper_id": "mnB4hDTIDr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel perspective to evaluate diversity in LLM-generated datasets by treating it as a classification task. This approach potentially provides a more structured framework and makes diversity evaluation more computationally efficient.", "Weaknesses": "The practical significance and potential real-world applications of the proposed diversity evaluation method are not clearly articulated. Additionally, it is unclear how the proposed method compares in performance and accuracy to existing methods. The correlation with diversity pseudo-truths mentioned lacks clarity on how it is measured and its impact on the outcomes.", "Questions": "What specific applications could benefit from assessing the diversity of LLM-generated datasets with the proposed method? How does DCScore compare with more traditional diversity evaluation methods in terms of precision and scalability? What is the underlying logic behind the correlation with diversity pseudo-truths, and how is it determined?"}}
{"paper_id": "Y0qmwm6tgy", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a pertinent issue in model pruning literature regarding the instability caused by weight perturbations, especially pertinent for large language models. The MoreauPruner algorithm offers a novel approach that leverages the Moreau envelope, providing robustness against such perturbations. The experimental results on several well-known LLMs, including extensive evaluation, add to the strengths of this work.", "Weaknesses": "The paper lacks clarity on the practical implications of pruning instability due to weight perturbations, leaving the reader questioning the real-world impact of the observed differences. Additionally, the performance improvements demonstrated by MoreauPruner, though present, are not dramatically better than existing methods, notably in certain contexts. Another limitation is the slightly restricted choice of target models, which primarily centers on the LLaMA family. The paper would benefit from improved descriptions regarding computational costs and resource demands of the proposed method.", "Questions": "How critical is the difference in pruning results to the overall performance of the models in real-world applications? Could additional sources of weight perturbation be considered in future studies to further generalize the findings presented here?"}}
{"paper_id": "aAcOaJYbUg", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses an important gap in the evaluation of OOD robustness for modern vision models in the context of web-scale datasets. \n2. The introduction of LAION-C with novel distortion types provides a fresh and challenging benchmark for contemporary models, specifically designed to be OOD relative to data scraped from the web. \n3. The psychophysical experiment that compares model performance with human observers is a valuable addition, as it allows for a deeper understanding of how models perform relative to human perception under these new conditions.", "Weaknesses": "1. While the novel distortions are a major contribution, the paper could benefit from a deeper exploration of the specific properties of these distortions and why they were chosen over potential alternatives. \n2. There is limited discussion on the potential bias introduced by the class distribution differences between LAION-C and traditional benchmarks like ImageNet-C.", "Questions": "1. Could you elaborate on how the distortions were selected and whether there was any consideration of other types of potential distortions that might also be OOD for web-scale datasets?\n2. How do you anticipate that the smaller number of classes in LAION-C compared to benchmarks like ImageNet-C might affect the generalizability of your results?"}}
{"paper_id": "duGygkA3QR", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel integration of Koopman theory, specifically Dynamic Mode Decomposition (DMD), with Graph Neural Networks (GNNs) to improve modeling and prediction tasks on graph-structured data. The connection between DMD and GNNs for estimating a low-rank linear operator provides a fresh perspective and potentially more efficient approach to capturing complex dynamics. The paper is well-written and structured, facilitating a clear understanding of the concepts presented.", "Weaknesses": "While the paper presents a strong novel contribution, there may be concerns regarding the practicality and computational efficiency of implementing DMD in various real-world graph applications. Additionally, the presentation of results could further benefit from including comparisons with more diverse baseline methods beyond current state-of-the-art GNNs to demonstrate broader applicability and improvement in real-world scenarios.", "Questions": "What are the computational costs associated with implementing DMD in GNNs when compared to other state-of-the-art models, particularly in large-scale graph applications? How does the proposed method handle the challenges of scaling to different types of graph structures (e.g., highly irregular or very large graphs)?"}}
{"paper_id": "s6nYndMwG7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper proposes a method that refines LiSSA by selecting hyperparameters based on spectral properties, improving its practicality for large models. It provides a good balance between theoretical insights and empirical validation. The use of random sketching for evaluating spectral properties is a beneficial approach for handling large-scale data. Empirical comparisons with PBRF offer evidence of the proposed method's efficacy.", "Weaknesses": "The reliance on specific spectral properties might limit the applicability of the method to scenarios where these properties can be efficiently estimated. The novelty of the contribution is somewhat limited as it can be seen as an improvement on an existing method. The potential impact is not fully demonstrated across diverse datasets or real-world scenarios.", "Questions": "1. How does the efficiency of random sketching compare to other methods for spectral property evaluation, especially in very large models?\n2. Are there specific model architectures or sizes where this method might not be applicable or efficient?\n3. Can this approach be extended to other settings where the inverse Hessian is used, beyond influence functions?"}}
{"paper_id": "9BVMD3keG8", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 5, "Strengths": "The paper presents a structured approach to understanding the role of contextual information in online learning within a brokerage setting, providing clear algorithms and matching bounds for both feedback models. The constraints and assumptions are clearly defined, showcasing the paper's rigor.", "Weaknesses": "The abstraction and assumptions around traders' valuations being zero-mean perturbations and the dependency on bounded density may limit the generalizability of the results. Additionally, while the results are promising, the paper could have expanded more on the practical implications and the conceptual differences compared to existing works.", "Questions": "1. How do the assumptions about the bounded densities and zero-mean perturbations affect the applicability of these algorithms in real-world scenarios?\n2. Can the methodology be adapted or extended to account for more realistic market behavior where biases in trader valuations exist?"}}
{"paper_id": "mnna9LUg7P", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper presents a novel method for quantizing SSMs, addressing the unique challenges posed by their sensitive feature maps and outlier activations. The proposed static 8-bit quantization technique is both innovative and practical, demonstrating significant improvements in latency with minimal accuracy loss. The approach is well-suited for large language models, enabling their deployment on resource-constrained platforms. The work is supported by comprehensive experiments on two substantial SSMs, Mamba 2.8B and Jamba 52B, showcasing the scalability and effectiveness of the method.", "Weaknesses": "While the paper demonstrates practical value, there are concerns regarding the clarity of certain methodological details, particularly the specifics of the quantization process and the handling of outliers. Additionally, the experiments are limited to specific hardware setups, and broader testing across diverse platforms could provide stronger evidence of generalizability. Some of the theoretical motivations for using the Hadamard transform in this context could also be elaborated more thoroughly.", "Questions": "1. Can the authors provide more details on how the Hadamard transform specifically aids in achieving an outlier-free quantization space? A deeper understanding of this mechanism in relation to SSMs would be beneficial.\n2. The paper mentions experiments on specific hardware but lacks broader hardware variability. Are there plans to validate the approach on other edge and cloud devices to ensure wider applicability?\n3. The paper primarily focuses on improvements for SSMs. Could the proposed techniques be adapted or extended to quantize other model architectures, such as those involving traditional attention mechanisms, and if so, how would they be modified?"}}
{"paper_id": "3X3LuwzZrl", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel approach to tackle multi-label node classification (MLNC) on graphs by considering influences between labels, which is a relatively unexplored area in existing literature. LIP model constructs a label influence graph for high-order influence propagation, showing promising results on benchmark datasets.", "Weaknesses": "The paper lacks a clear comparison with existing label propagation methods and does not adequately elaborate on the difference from previous approaches. The influence of crucial hyperparameters on model performance is not thoroughly analyzed, and there is a need for more comprehensive experimentation across various settings to establish generalization claims.", "Questions": "How do the proposed method and previous label propagation techniques differ in handling label influences? Could the authors provide more visualizations or examples to illustrate how the LIP model adjusts learning based on label influence? How sensitive is the model performance to the choice of hyperparameters such as \\alpha and \\beta?"}}
{"paper_id": "IqaQZ1Jdky", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces an innovative approach by replacing fixed activation functions in neural networks with learnable ones, which enhances both accuracy and interpretability. The use of Bernstein polynomials as a function basis within KANs is a novel application that is theoretically grounded, leveraging the Weierstrass approximation theorem. The approach has been validated through comprehensive experiments across several domains, indicating its broad applicability. The paper is well-organized, and the experiments are carefully designed to demonstrate the proposed method's strengths over conventional models.", "Weaknesses": "The practical impact of the theoretical claims, such as the benefit of the uniform approximation property and error minimization, is not thoroughly evidenced in the experiments. The claims regarding flexibility and robustness are not strongly supported with empirical results. The performance improvements, although present, are not groundbreaking and may not justify the additional complexity introduced by the proposed framework. The comparison to state-of-the-art models in each domain is limited, making it difficult to evaluate the proposed method's true contribution.", "Questions": "Can the authors clarify how the selection of Bernstein polynomials (and the variation of the function basis) benefits the model's performance in different datasets? What are the practical implications of the theoretical findings, such as Theorem 3.3, on real-world datasets? Is the computational cost of implementing VBn-KAN significantly higher than standard KANs or other neural networks due to the learnable activation functions, and how does this affect scalability?"}}
{"paper_id": "OdMqKszKSd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "1. The method allows the generation of detailed 3D articulated objects from a single image, addressing scalability and practicality concerns in the field.\n2. The use of a diffusion model to capture plausible variations in object geometry and kinematics is innovative and effective.\n3. The coarse-to-fine generation pipeline, leveraging part connectivity and abstraction, offers a structured approach to object generation.", "Weaknesses": "1. The method relies heavily on existing data for training, which could limit its effectiveness and applicability when such data is scarce or unavailable.\n2. There might be challenges in generalizing the model to broader categories of objects, beyond those specifically tested in the study.\n3. Details about the integration of high-level structure and geometric details could be further elaborated to fully understand the process.", "Questions": "1. How does the model handle objects with more complex structures and articulations that were not part of the initial training set?\n2. Are there specific limitations in terms of object categories or types that the current model may struggle with?\n3. How does the performance of the model compare when tested on unseen object categories, outside the training data?"}}
{"paper_id": "LOBhVTtVnc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "- The introduction of Geometric Spatiotemporal Transformers (GSTs) is a novel and significant contribution to the field, as it addresses the limitations of existing Graph Neural Networks (GNNs) by handling input sequences of arbitrary lengths and capturing long-term contexts more effectively.\n- The use of Temporal Difference Graph (TDG) is innovative, effectively capturing global dynamic patterns and mitigating cumulative errors in long-term prediction tasks.\n- The paper demonstrates the method's superiority through empirical evaluation across multiple challenging physical systems, enhancing its credibility and applicability across different scales.\n- The approach leverages Transformers in an autoregressive framework, which is a promising direction given recent advances in the field.\n", "Weaknesses": "- The paper might lack comprehensive runtime analyses to compare the efficiency of the proposed technique relative to existing methods. This is necessary to assess the practicality of GSTs in resource-constrained applications.\n- Details on model scalability, especially for large-scale problems or extended timeframes, are not thoroughly discussed. This might raise questions about how the method performs in more complex real-world scenarios.\n- The abstract does not specify the size or complexity of the datasets used for evaluation, which is crucial to validate the claims about the method's performance.\n- More extensive comparisons with state-of-the-art models, especially in terms of computational costs and model complexity, could provide a clearer understanding of the trade-offs involved.\n", "Questions": "- How does the GST method scale with larger datasets or systems with more complex interactions? Are there any known limitations or bottlenecks?\n- What are the specific computational requirements for training and implementing GSTs compared to traditional GNNs? How does the introduction of TDG affect these requirements?\n- The paper presents strong empirical results, but can you provide more insights into any scenarios or types of systems where GSTs may underperform compared to existing methods?\n- What specific measures were taken to ensure the alignment and preservation of E(3) symmetries within the proposed spatiotemporal blocks? Are there instances where this becomes particularly challenging?"}}
{"paper_id": "oa5UeyUVMm", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel application of diffusion probabilistic models (DPMs) for graph representation learning, which is innovative and shows strong results. The theoretical analysis is solid, proving that the denoising objective maximizes conditional mutual information, and the empirical results are impressive, achieving state-of-the-art performance on multiple datasets.", "Weaknesses": "While the theoretical contributions are significant, the practical implications and potential limitations of using DPMs for graph representation learning are less discussed. There's also a need for more details on the experimental setup and comparison with a broader set of baseline methods.", "Questions": "What are the limitations of the proposed approach in terms of scalability to larger graphs? How sensitive is the model to the choice of hyperparameters, especially concerning the diffusion process? Can the approach be generalized to other types of data representations beyond graphs?"}}
{"paper_id": "5swfKRkCx7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach to augmenting large language models with external knowledge attention for question answering, introducing an extra head for better knowledge integration. The memory-based mechanism controlling the degree of knowledge mixing is innovative and promises to address existing challenges in retrieval-augmented generation, potentially optimizing performance in both general and specific-domain QA tasks.", "Weaknesses": "While the proposed model framework seems promising, the explanation of the methodology seems somewhat abstract, lacking detailed clarity. The novelty of integrating external knowledge as an extra attention head, while interesting, would benefit from deeper theoretical grounding and experimental comparison with existing baseline strategies. More evidence supporting the purported memory-based dynamic integration's effectiveness over traditional methods would strengthen the contribution.", "Questions": "How does the proposed method compare with existing RAG approaches in terms of computational efficiency and inference speed?\nWhat metrics are used to evaluate the performance improvements in QA tasks, and could there be potential trade-offs in using the extra attention head?\nAre there any limitations concerning the types or formats of external knowledge that this approach can integrate effectively?"}}
{"paper_id": "5s1qpjrNvZ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed method introduces a novel way to dynamically calculate the guide sampling rate to ensure that the mean return of the learning policy meets a user-defined performance threshold. This is a significant contribution to guided reinforcement learning, providing a strong theoretical basis for the method.\nThe paper addresses a key issue in reinforcement learning regarding sample efficiency and effectively proposes a methodology to mitigate it by leveraging pre-existing knowledge through guide policies.", "Weaknesses": "The assumptions required to derive the sampling rate are strong and may not hold in many practical scenarios. This potentially limits the applicability of the theoretical guarantees.\nThe empirical evaluation, although sufficient for illustration, lacks depth and diversity in terms of testing environments and could have included more complex and diverse tasks to better demonstrate the method's robustness and applicability in real-world scenarios.\nSome claims, especially related to performance guarantees, could be overstated given the assumptions and limitations outlined.\nThe presentation of results could be clearer, particularly with respect to Figures and their explanatory details.", "Questions": "How robust is the proposed method when the assumptions required for the sampling rate derivation do not hold? Is the method performant in such cases?\nAre there specific scenarios or conditions where the roll-back feature is particularly advantageous or detrimental?\nCould more complex environments, possibly with a combination of dense and sparse rewards, provide deeper insights into the method's practical utility?\nHow sensitive is the method to hyperparameters that affect the guide sampling rate and the roll-back mechanism?"}}
{"paper_id": "WNb4P8aG66", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces DoD, a novel method for improving image generation by incorporating a multi-stage generation framework. By extracting visual priors and leveraging them for guidance, the method addresses the common challenge of lacking texture details in diffusion models. The approach reduces training costs significantly while still improving performance metrics such as the FID-50K score. The proposed latent embedding module for compressing and retaining semantic information is a unique contribution that enhances the efficacy of diffusion models.", "Weaknesses": "The paper lacks detailed explanation and comparison with existing state-of-the-art process-based diffusion methods and other frameworks that integrate image features for conditioning, which could further solidify the novelty and benefits of the proposed method. The results section, particularly the main results, provides limited analysis and direct comparison with newer methodologies in the same domain. There is a missed opportunity to showcase more significant qualitative improvements compared to the baseline methods, making it difficult to visually assess the tangible impact of the proposed techniques.", "Questions": "1) How does DoD compare directly with other multi-stage or process-based diffusion methods, particularly regarding efficiency and output quality?\n2) What specific use cases or applications do the authors envision for DoD that highlight its strengths over existing diffusion or image generation models?\n3) Are there any considerations or challenges noted while integrating the latent embedding module, especially in diverse or larger datasets?\n\nImproving upon these aspects could potentially raise the quality of the study and fortify its contributions to the field."}}
{"paper_id": "UfczlMudN6", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a unified framework for addressing both in-distribution and out-of-distribution generalization in deep reinforcement learning (DRL), which is a necessary advancement for real-world deployments. The combination of a robust adaptation module and joint training pipeline is a methodical approach to the problem, and the strong performance of the GRAM algorithm on realistic tasks is noteworthy.", "Weaknesses": "The paper appears to focus solely on locomotion tasks, which may not fully demonstrate the generalizability of the proposed method across different types of environments. It is also unclear how the proposed framework compares against established baselines beyond the mention of strong generalization performance, suggesting that additional comparative analysis may be beneficial. Furthermore, the framework's dependency on a joint training pipeline might introduce complexities in practical deployment scenarios.", "Questions": "How does the robust adaptation module handle scenarios where in-distribution and out-of-distribution dynamics are not clearly distinguishable? Can the GRAM framework be easily adapted for tasks beyond locomotion, such as manipulation or human-robot interaction tasks? What metrics are used to evaluate the generalization performance, and how is significance tested across different trials or environments?"}}
{"paper_id": "60Vd7QOXlM", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel and more effective way of generating canaries for privacy auditing of large language models, providing a significant improvement over previous methods. Extensive experiments are conducted on various LLMs, demonstrating the effectiveness of the proposed approach compared to the state-of-the-art.", "Weaknesses": "The paper does not fully explore the limitations of the proposed method and how it may be affected by various factors, such as the size and architecture of the LLMs or the type of data used. Additionally, it may not provide enough practical guidelines on how to implement these methods in real-world scenarios.", "Questions": "1. How does the effectiveness of the new canary generation method vary with different architectures or scales of language models? \n2. What are the potential limitations of the proposed approach in more restricted or less ideal environments?\n3. Could the proposed method be effective if integrated into existing privacy frameworks currently employed in industry?"}}
{"paper_id": "GqhvJ1o8m5", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel methodology, ImmersePro, for stereo video generation, which is a notable advancement in terms of reducing dependency on explicit disparity maps that are prone to errors. It also introduces a large-scale dataset, YouTube-SBS, which could be a valuable resource for the community.", "Weaknesses": "The method description lacks clarity on how the implicit disparity guidance is effectively replacing the traditional explicit maps, and the potential limitations of this approach are not adequately addressed. Additionally, more details on the dataset's diversity and representativeness would strengthen the paper.", "Questions": "How does ImmersePro handle diverse scene complexities and occlusions without explicit disparity maps? What are the limitations of using implicit disparity guidance in different video genres or lighting conditions?"}}
{"paper_id": "ua5MHdsbck", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The paper presents a novel approach for protein design focused on extrapolating fitness beyond the training range, which addresses a significant challenge in the field.\n2. The proposed method effectively employs triplet relations, incorporating preference alignment models to better capture fitness gradients, which enhances search capabilities.\n3. Experimental results on designing AAV and GFP proteins show substantial improvements in extrapolative tasks, suggesting the practical applicability and impact of the method.", "Weaknesses": "1. The reliance on a scorer function for defining triplets can introduce robustness issues, especially in real-world settings where scorers may not generalize well out of distribution.\n2. Some aspects of the methodology, particularly the explanation of triplet formation and scorer distillation, are complex and could be more clearly articulated to enhance reader comprehension.", "Questions": "1. How does the preference optimization method handle noisy labels in the triplet definitions, particularly in situations where scorer functions might introduce inaccuracies?\n2. Can the authors elaborate on the process and importance of mask infilling? Specifically, how prevalent are these masked samples in triplets where fitness ordering challenges arise due to editor limitations?"}}
{"paper_id": "56Zn3halhq", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces an innovative approach to data augmentation in the context of time series forecasting using reinforcement learning, which is a novel application of the method. \n2. It addresses the challenge of identifying and transforming marginal samples to improve model robustness, which is a significant contribution to the forecasting field.\n3. The presentation is clear, structured, and the empirical analysis is well-detailed to support the claims.", "Weaknesses": "1. The impact of this contribution might be limited by the specificity of the forecasting models used in the study, hence the general applicability is not entirely demonstrated.\n2. While the approach appears promising, there may be issues with scaling and complexity when applied to larger datasets or more complicated forecasting models, which are not addressed in the abstract.\n3. The novelty may be somewhat constrained by the fact that it builds on existing techniques such as autoencoders and reinforcement learning, thus the level of innovation might be considered incremental rather than breakthrough.", "Questions": "1. What datasets were used in the empirical analysis, and do they represent a wide range of forecasting challenges?\n2. How does the computational cost of the proposed method compare with traditional fixed-size training set approaches?\n3. Can the model effectively generalize and improve performance on out-of-sample datasets not seen during the training phase?"}}
{"paper_id": "aPTGvFqile", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses an important problem in the alignment of modalities in CLIP, promising significant enhancements in downstream tasks.\n2. AlignCLIP method shows improvements across various evaluations, suggesting robust applicability.\n3. Presentation is clear, with a defined problem statement and structured approach.", "Weaknesses": "1. The novelty of the contribution, particularly regarding shared parameters and semantically-regularized separation, might not be significant without more detailed explanation and distinction from existing methods.\n2. Lack of detailed ablation studies or error analysis could leave questions regarding why specific improvements are observed.\n3. There are no explicit comparisons with the very latest methods, which might leave room for speculation on its standing among state-of-the-art.", "Questions": "1. How significant is the contribution of each component, like shared parameters and the separation objective, to the overall performance improvements?\n2. What are the implications of the modality gap specifically in zero-shot settings, and how is this addressed effectively in varied datasets?\n3. How does AlignCLIP perform in tasks that have not been specifically mentioned, such as retrieval tasks or complex reasoning tasks, where cross-modal synthesis is crucial?"}}
{"paper_id": "GOwNImvCWf", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents an innovative approach by introducing a behavioral loss to complement structural reconstruction, aimed at better capturing the features critical for reconstructing high-performing models. The evaluation across multiple model zoos demonstrates the practical benefits of the proposed method. The study effectively highlights the synergy between structural and behavioral features, showing improved performance across evaluated downstream tasks.", "Weaknesses": "The novelty of the proposed behavioral loss might be limited as similar ideas may exist in previous literature. The paper could benefit from a more extensive experimental section, considering a broader range of architectures and application domains to validate the generalizability of the approach. Additionally, the increased training cost and complexity introduced by the behavioral loss are acknowledged but not quantitatively assessed in the paper.", "Questions": "What specific features or metrics does the behavioral loss target that structural reconstruction fails to capture, and how are these aligned with improvement in model performance?\nAre there potential limitations or challenges in scaling the proposed method to larger, more complex models or different types of neural networks?\nCould the authors clarify the intuition for choosing the particular model zoos used for evaluation and whether these selections might bias the reported outcomes?"}}
{"paper_id": "C9BA0T3xhq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces Extended Implicit Q-Learning (EIQL), a novel algorithm designed to extend the utility of the Bellman update in offline Reinforcement Learning by incorporating actions beyond dataset constraints. It strategically enhances the maximization capability of the Bellman update and mitigates error extrapolation risks. The empirical results demonstrate improved performance over traditional offline RL algorithms, especially in environments with sparse rewards and incomplete trajectories.", "Weaknesses": "The paper could benefit from a more detailed explanation of the proposed approach, particularly regarding its implementation and theoretical basis. Additionally, the novelty might be questioned, as the approach seems to be a variation of existing Implicit Q-Learning methods. The presentation of the results could be clearer and more comprehensive, with comparisons to a broader range of baseline methods. More details on hyperparameter choices and training setups would enhance reproducibility.", "Questions": "1. How does the proposed approach compare to other recent advances in offline RL, such as those focusing on robust policy learning without reliance on specific assumptions about the static dataset?\n2. Could the authors elaborate on the theoretical foundations behind EIQL, particularly how it addresses error extrapolation without increasing computational complexity?\n3. In environments with very sparse rewards, how does the proposed method ensure the learned policy does not overly focus on suboptimal action spaces?"}}
{"paper_id": "EeqlkPpaV8", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper provides a novel theoretical analysis of adaptive complexity for sampling, which is crucial for improving the understanding and development of parallelizable sampling algorithms. The characterization of hardness potentials and the application of classical smoothing techniques are particularly noteworthy.", "Weaknesses": "The abstract and overall presentation are somewhat dense, making it challenging to follow without a strong background in mathematical optimization and sampling theory. The paper might also benefit from clearer explanations and justifications for the assumptions used in the analysis. There is a lack of empirical validation or practical examples to illustrate the applicability of the theoretical results.", "Questions": "How do the theoretical findings translate to practical applications in high-dimensional data contexts? Are there specific instances or problems where this approach has shown significant advantages over existing methods? Could there be potential extensions or modifications to address non-convex distributions?"}}
{"paper_id": "oK1zJCWBqf", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "SPO offers a novel approach by aligning generative models with human preferences without relying on reward models, thus simplifying the alignment process. The method's theoretical foundation is solidly explained, showing convergence properties under the Bradley-Terry model.", "Weaknesses": "There is a lack of clear empirical evidence demonstrating the practical effectiveness of SPO compared to existing alignment techniques, such as those using reward models. Additionally, the presentation could benefit from clearer explanations and visual aids to enhance understanding.", "Questions": "How does SPO compare in terms of computational cost and performance with existing reward model-based alignment techniques? Are there specific scenarios or datasets where SPO significantly outperforms traditional methods?"}}
{"paper_id": "Dq9VrVuLzV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 6, "Confidence": 4, "Strengths": "1. The proposed method, SyntheOcc, offers a novel approach to synthesizing photorealistic and controlled images for the task of 3D occupancy prediction, addressing a significant challenge in data annotation for autonomous driving.\n2. Incorporating 3D semantic multi-plane images (MPIs) is innovative, providing a comprehensive and spatial understanding of the scene, which improves the conditioning input for diffusion models.\n3. The technique demonstrates practical application potential, with extensive evaluations on the nuScenes dataset showcasing its effectiveness in data augmentation for perception models.", "Weaknesses": "1. While the paper demonstrates the effectiveness of the proposed method, the discussion around limitations and potential challenges in real-world applications is limited.\n2. The complexity of encoding 3D geometric information into a 2D diffusion model is addressed, but more technical details or comparisons with existing methodologies could strengthen the assertion of novelty.\n3. It would be beneficial to understand how the method handles the scalability of generating large datasets with diverse driving scenarios.", "Questions": "1. How does SyntheOcc ensure the diversity and variety of scenarios while maintaining photorealism during dataset generation?\n2. Are there any limitations observed in the generation of particular types of scenes or objects, such as irregularly shaped or occluded objects?\n3. What are the computational requirements for implementing SyntheOcc at scale for continuous data generation in real-time applications?"}}
{"paper_id": "73Q9U0vcja", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The integration of a generative diffusion model with sequential experimental design presents a novel approach to tackling inverse problems in imaging, particularly in the context of reduced data acquisition requirements (such as lower X-ray exposure) while maintaining or improving image reconstruction quality.", "Weaknesses": "The paper could provide a more detailed analysis of how the trade-off between computational cost and reconstruction quality is managed. Additionally, a comparison against other active learning methods in similar settings would strengthen the contribution.", "Questions": "How does the proposed method perform in more complex settings, such as fan-beam or 3D cone-beam CT? What are the potential ways to mitigate the computational cost associated with the inference process?"}}
{"paper_id": "UiEjzBRYeI", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel approach, SAM-CP, which clearly enhances the Segment Anything Model (SAM) by introducing two types of composable prompts that allow for versatile segmentation. This approach effectively broadens SAM's capability into semantic-aware segmentation tasks, achieving state-of-the-art performance in open-vocabulary segmentation. The framework provides a structured methodology for calculating the affinity between queries and SAM patches, which is a unique contribution to the field.", "Weaknesses": "While the method shows great promise, it relies heavily on SAM's initial output, which may not always be optimal for all segmentation tasks, particularly for datasets outside of the domain it was trained on. The paper lacks a deep exploration of the limitations of SAM-CP in scenarios where SAM's performance may degrade. Additionally, the complexity of the proposed affinity calculation could be computationally expensive and may not scale well with very large or complex datasets. There is limited discussion on the computational efficiency of the approach.", "Questions": "How does SAM-CP handle scenarios where SAM's initial segmentation is significantly off from the desired semantic boundaries? Are there any mechanisms to correct such deficiencies? Could the proposed affinity calculation method be applicable or adapted to scale for significantly larger datasets or real-time applications, or does it pose constraints in high-complexity environments?"}}
{"paper_id": "lessla98Wp", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach to video colorization using language descriptions, which is an innovative idea that pushes the boundaries of creative color application while maintaining temporal consistency. The introduction of cross-modality pre-fusion and temporally deformable attention modules addresses key challenges in maintaining color consistency and applying creative color schemes. The experimental results are comprehensive and demonstrate significant improvements over existing methods.", "Weaknesses": "The paper does not thoroughly discuss limitations or potential failure cases of the proposed method. There is a lack of detail on how the method performs with diverse or complex language inputs and its scalability to very long or detailed video sequences. Additionally, while the concept of using language to control color application is novel, the paper could further explore the potential limitations and challenges this presents, such as handling ambiguous or contradictory descriptions.", "Questions": "How does the performance of L-C4 scale with increasingly complex or lengthy language descriptions? Are there any specific constraints on the language input in terms of complexity or structure? Additionally, how does the model handle conflicts or ambiguities in the language descriptions provided by users? What are the specific limitations of the cross-modality pre-fusion module in terms of scalability or applicability to different video content types?"}}
{"paper_id": "UstOpZCESc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper tackles the novel problem of integrating lifelong learning with privacy-aware unlearning, which is a significant contribution to the field of AI and privacy.\n2. The proposed PALL method offers a balanced approach to both learn and unlearn information effectively within a single architecture, which addresses both current and predicted needs for AI applications.\n3. The method is empirically validated across multiple architectures, demonstrating versatility.\n4. The structured use of sparse subnetworks and parameter sharing is an innovative method that supports both learning stability and memory efficiency.\n5. The use of an episodic memory rehearsal mechanism for exact unlearning is a compelling solution given the problem's constraints.", "Weaknesses": "1. There might be concerns about the scalability and efficiency of the proposed method when applied to very large tasks or data sets, which isn't fully addressed.\n2. The complexity of the model might hinder its practical deployment in real-time applications where adaptability and quick responses are necessary.\n3. While empirical validations are provided, the theoretical backing could be stronger, particularly in explaining how diverse tasks impact the model's performance.\n4. It remains unclear how well the method will handle unlearning requests that are deeply intertwined with learned tasks, impacting the overall performance.\n5. The paper could benefit from clearer illustrations of the model's architecture and execution to enhance understanding.", "Questions": "1. How does the performance of PALL vary when applied to different types of tasks, such as text processing or other forms of data beyond image classification? \n2. Could you elaborate on how the episodic memory rehearsal mechanism facilitates unlearning and if it could introduce potential biases in the model?\n3. How does PALL manage multiple simultaneous unlearning requests, and is there a priority mechanism in place for handling these requests?\n4. To what extent does the choice of architecture impact the effectiveness of the PALL method, and are there certain architectures that are more suited for this model?\n5. Are there specific limitations to the types of tasks or datasets where PALL may not perform as well, and how could these be mitigated?"}}
{"paper_id": "LnRegTxsa4", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces novel modules (CVCAM and MHSAM) for improving cross-view object geo-localization, which show promising results over previous methods. The creation of a new dataset, G2D, for ground-to-drone localization tasks adds value to the field and addresses a significant gap.", "Weaknesses": "While the paper proposes new modules, the paper lacks detailed theoretical analysis or ablation studies to clearly delineate the specific impact of each component. The novelty in comparison to existing attention mechanisms is not well-highlighted, and it is unclear how new the proposed attention methods are given existing techniques like Transformer's cross-attention. The dataset details need further expansion to understand its construction, biases, and how it compares to existing ones.", "Questions": "How do the proposed CVCAM and MHSAM specifically differ from existing cross-attention mechanisms, and what unique enhancements do they provide? Can the authors provide more details about the G2D dataset in terms of its construction process, scale, and potential biases?"}}
{"paper_id": "2ErS9Bkc3O", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a matrix-theoretic explanation for adversarial vulnerability in deep neural networks, offering insights that align with prior information-theoretic analyses.", "Weaknesses": "The theoretical analysis may reflect an extension of existing understanding of adversarial fragility rather than entirely new results. Additionally, the paper lacks experimental validation which is usually needed to support theoretical findings. The presentation can be improved with more detailed exposition and contextual positioning among current literature.", "Questions": "How does the matrix-theoretic explanation practically aid in improving network design or training processes to enhance adversarial robustness? Are there specific classes of neural networks where the degradation effect is less pronounced?"}}
{"paper_id": "pK3oe2bubc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper tackles an interesting problem by introducing new training strategies for vision transformers that allow them to maintain functionality under unconventional operation conditions, such as shuffled or missing layers. This adaptability is a significant step forward for distributed neural network architectures.", "Weaknesses": "While the paper proposes innovative methods, the 20% drop in accuracy is quite significant, and further exploration is needed to mitigate this loss. Additionally, the paper lacks comprehensive evaluations across different transformer architectures and datasets, which would strengthen its claims and applicability.", "Questions": "How generalizable are these methods to other types of neural networks beyond vision transformers? What steps can be taken to reduce the accuracy loss experienced with the proposed adaptations during layer shuffling or pruning?"}}
{"paper_id": "sec09tLQUl", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper makes a relevant contribution to the field of model generalization by addressing reliance on spurious correlations through a novel dropout technique. The use of example-tied dropout targeting specific neurons and the extensive empirical evaluation across various tasks provide evidence of the approach's effectiveness.", "Weaknesses": "The paper does not clearly explain how the neurons responsible for memorization are identified. Furthermore, while the dropout method is innovative, it builds upon existing knowledge about memorization without significantly extending theoretical understanding. The methodology's computational complexity may be a concern in large datasets, but this issue has not been fully addressed.", "Questions": "How are the neurons responsible for memorization identified without group annotations during training? What is the computational overhead of the proposed FairDropout method in practice, particularly for large networks?"}}
{"paper_id": "96jZFqM5E0", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel framework for 3D hand pose estimation by pre-training on a large dataset of in-the-wild hand images. The approach uses an innovative contrastive learning method that adapts weights based on inter-sample distances, leading to performance improvements over state-of-the-art methods. The paper includes extensive experiments demonstrating effectiveness across multiple datasets.", "Weaknesses": "The method relies heavily on the quality and diversity of the collected datasets, which might not be broadly generalizable. The potential limitations or constraints of the proposed approach in more varied or real-time applications are not fully explored. Additionally, while the paper presents a novel approach, it builds on existing contrastive learning frameworks without a significant breakthrough in fundamental understanding or technique.", "Questions": "Could the adaptive weighting mechanism be further generalized to other types of image or pose estimation tasks, and how does it perform in real-time application scenarios?"}}
{"paper_id": "uNd289HjLi", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel self-supervised denoising framework C2S that does not require high-SNR labels, which alleviates a major limitation of supervised methods. The GADSM loss and detail refinement extension ensure that the model effectively balances denoising and spatial feature preservation. The multi-contrast denoising capability also adds a new dimension to the proposed method.", "Weaknesses": "The paper might not provide sufficient empirical evidence demonstrating substantial performance gains over existing self-supervised and especially supervised methods. Also, it would benefit from clearer descriptions of the potential caveats or limitations when the proposed C2S is applied to varied real-world settings. The presentation of results could also be enhanced with more visual examples before and after denoising.", "Questions": "1. What measures are taken to ensure that the detail refinement extension truly preserves fine spatial features across all tested datasets and conditions?\n2. Could you illustrate a practical scenario involving multi-contrast denoising and how significantly it improves from the standard single-contrast approach?\n3. Are there specific limitations or edge cases where C2S might perform suboptimally compared to existing methods?"}}
{"paper_id": "e2ONKX6qzJ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel modification to classifier-free guidance by decomposing the update term into parallel and orthogonal components, effectively addressing the issue of oversaturation in diffusion models while improving generation quality. The proposed approach, Adaptive Projected Guidance (APG), requires minimal computational overhead, making it a practical enhancement for existing models. Extensive experiments demonstrate the superior performance of APG across various metrics.", "Weaknesses": "While the modifications to the CFG update rule are a clever approach to improving image generation without additional complexity, the paper could benefit from a more in-depth theoretical analysis of why these changes lead to the observed improvements. Additionally, the presentation could be more refined to enhance clarity and understanding of the technical details.", "Questions": "How does the proposed down-weighting of the parallel component in CFG compare to alternative methods for managing oversaturation, and are there scenarios where APG may be less effective? Additionally, could the insights into the connection between CFG and gradient ascent potentially be applied to other model guidance techniques beyond diffusion models?"}}
{"paper_id": "a2eBgp4sjH", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper systematically studies the problem of nearest neighbor search with filter constraints (MultiFilterANN), showcasing both theoretical and practical insights. It provides new algorithms with strong space-time tradeoffs and proposes new datasets for MultiFilterANN, which fill a gap in current literature.", "Weaknesses": "The proposed algorithms, though promising, may lack clarity in their direct connection between theoretical and practical contributions. The separation between theoretical results and empirical evaluations is blurry, making it somewhat challenging to follow how the theory directly supports the empirical methodology. The use of existing graph indices in empirical work suggests limited novelty on that front.", "Questions": "How do the theoretical insights from the large filter regime translate into practical performance enhancements for varying filter sizes in your empirical studies?\nWhat are the limitations of your proposed method compared to existing approaches, particularly in terms of scalability and adaptability to different types of datasets?"}}
{"paper_id": "EIXZXPz7jU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel approach by integrating diffusion models with PINNs for better handling of singularities in PDE solutions. The use of flow matching for sample generation is innovative and helps address inherent problems in existing methods, promising more efficient computation and potentially higher accuracy.", "Weaknesses": "The proposal lacks clarity in explaining certain technical aspects, such as the precise mechanism of sample generation and how diffusion models are specifically employed. The experimental results, while promising, are not extensively varied, raising questions about the robustness and general applicability of the method. More diverse PDE examples would further substantiate claims.", "Questions": "1. Can the method be generalized beyond the addressed scenarios, and how does the performance scale with more complex PDE systems?\n2. What are the computational overheads introduced by the diffusion models, and how do they compare to gains in PINN accuracy?\n3. Are there specific limitations or failure cases observed when applying this method to PDEs with different kinds of singularities?"}}
{"paper_id": "QO4bF6MHza", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The introduction of MathHay provides a much-needed benchmark specifically targeting the evaluation of long-context mathematical reasoning abilities of LLMs, which is a novel and important contribution. The paper's experiments with top-performing LLMs provide valuable insights into the current performance gaps and challenges in this area.", "Weaknesses": "The paper does not provide detailed insights into the exact types of mathematical reasoning tasks included in MathHay, which could aid in better analyzing model performances. Given the novelty of the benchmark, there may also be a need for further validation of its comprehensiveness and effectiveness.", "Questions": "How was MathHay designed to ensure that it adequately tests both information-seeking and complex mathematical reasoning? Are there plans to extend this benchmark to cover additional types of mathematical tasks?"}}
{"paper_id": "GULx8rzzjC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "The paper addresses a pertinent issue in data sharing for machine learning, introducing a method to efficiently evaluate and utilize data sources obtained with constrained access. Mycroft's ability to converge rapidly to the performance of the full-information baseline and robustly rank data sources is a significant advantage.", "Weaknesses": "The explanation of the key methods and algorithms, such as the RetrieveTopK function in Algorithm 3 and the application of gradient matching, could benefit from more clarity and detail. Additionally, there is a lack of experimental validation concerning privacy protection, a crucial aspect of this field.", "Questions": "How does RetrieveTopK ensure coverage of key data subsets? Could the approach be tested with explicit privacy-preserving techniques to offer more insights into its practical applications in contexts with strict data-sharing limitations?"}}
{"paper_id": "PnZ2lbQaao", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed DICF framework effectively combines adversarial Bayesian modeling and domain indexing to tackle the cold-start problem in cross-domain recommendation, offering an interpretable mechanism for knowledge transfer and improving recommendation accuracy.", "Weaknesses": "There might be limited novelty, as the concept of domain indexing is relatively new but not unprecedented. Further clarity and detail are required to understand exactly how the domain indices are inferred and used in the proposed framework.", "Questions": "How does DICF handle the scalability issue when applied to large-scale datasets, considering the potential computational complexity involved in adversarial Bayesian frameworks?"}}
{"paper_id": "AT64R0ivUO", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel method, SteerPrompt, which explicitly highlights key context by steering an LLM's attention scores without requiring parameter changes. This approach demonstrates a significant improvement in performance, showing an average improvement of 7.95% for LLAMA3-70B-Instruct. The method is applicable at inference time, making it a flexible and efficient option for enhancing LLM comprehension.", "Weaknesses": "The approach may be limited to specific use-cases such as open-book QA, and its applicability to other tasks is not explored. Additionally, while SteerPrompt does not require changing model parameters, its effectiveness may depend on the accuracy of identifying key contextual information, which could be a limiting factor. The paper does not address how this method affects models with different architectures or how it scales with model size.", "Questions": "How does SteerPrompt compare to other state-of-the-art methods in terms of computational resources and efficiency? Are there scenarios where the method may not perform as well, and how can these be addressed? Could this approach be integrated into real-time or streaming applications, and what would be the potential challenges?"}}
{"paper_id": "nGiGXLnKhl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "1. The paper introduces Vision-RWKV, a model tailored for vision tasks by modifying the RWKV architecture, highlighting its potential in efficiently processing high-resolution images. \n2. Its unique features include reduced spatial aggregation complexity, showcasing superior performance over Vision Transformer (ViT) in both speed and memory usage across various tasks. \n3. The availability of the model and code promotes reproducibility and further exploration.", "Weaknesses": "1. While the model showcases performance enhancements, the paper could benefit from more detailed technical explanations of the modifications made and their impacts on architecture.\n2. The comparison with other architectures, especially more recent or lesser-known models, could help place the contribution in a broader context.", "Questions": "1. Can the modifications in VRWKV be applied to other architectures beyond RWKV, potentially broadening its applicability in vision tasks?\n2. Are there any particular limitations or scenarios where VRWKV might not perform as well as window-based models?"}}
{"paper_id": "TBJCtWTvXJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel optimizer, SignSoftSGD (S3), which addresses significant gradient variations and reduces the risk of loss spikes, improving the robustness and convergence speed in training deep neural networks. The integration of Nesterov's accelerated gradient technique without additional memory overhead is a key strength. Theoretical analysis and extensive experimentation on various tasks validate the benefits of the proposed method.", "Weaknesses": "Although the paper provides solid theoretical and empirical support, the practical implementation details and impact on different network architectures could be further elaborated. More discussion on potential limitations of the S3 optimizer, especially in specific use cases, would be valuable.", "Questions": "How does S3 compare in terms of computational efficiency with other state-of-the-art optimizers? Are there any specific scenarios where S3 might not perform as expected compared to Adam or AdamW?"}}
{"paper_id": "8gSrJOL2oc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel framework called TRIDENT for compositional zero-shot learning (CZSL), addressing limitations of previous methods by using multimodal large language model embeddings and introducing attribute smoothing. The approach seems well-grounded theoretically and demonstrates state-of-the-art performance on three challenging datasets. The innovative use of MLLM embeddings and adaptive feature aggregation shows promise in improving generalization to novel compositions.", "Weaknesses": "The paper's presentation could benefit from clearer explanations and diagrams, as some portions, especially regarding Figure 2 and mathematical notations, may be unclear to readers. Details on the training process of key components like the LLM and the image embedder are somewhat ambiguous. Additionally, a deeper discussion on the integration and role of attribute-object disentanglement in the final inference process is needed. The lack of comparisons with the latest works limits an understanding of relative advancements over very recent methods.", "Questions": "1. Can the authors provide more insight into how the modules used during training, such as attribute-object disentanglement, contribute to the model's final inference performance?\n2. How does the proposed method compare with the most recent methodologies, like Troika, in terms of performance and generalization capabilities?\n3. Could the authors clarify the specific training and freezing strategy employed for different modules within Figure 2, especially concerning the MLLM and visual backbone components?"}}
{"paper_id": "yheQRc5xWB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach by leveraging state-space models (SSMs) in the domain of time-varying counterfactual prediction (TCP), specifically addressing both effectiveness and running efficiency.\n2. The proposed method, Mamba-CDSP, presents an innovative solution to mitigate confounding bias by de-correlating between current treatments and historical covariates, treatments, and outcomes.\n3. The experimental results show significant improvement over existing baselines, demonstrating the potential impact of the approach on both synthetic and real-world datasets.\n4. The paper provides a clear motivation for combining SSMs with TCP, offering a compelling argument for the approach's benefits in long-sequence modeling.", "Weaknesses": "1. The reliance on synthetic datasets may limit the ability to generalize findings to real-world scenarios, though such datasets are often necessary for initial validation.\n2. While the paper discusses running efficiency as a strength, there is limited detailed analysis or comparison of the computational performance against existing models in the results section.\n3. The explanation of Mamba-CDSP might be dense for some readers due to the complex interplay of various components (e.g., SSMs, de-correlation mechanisms) without visual flow diagrams or step-by-step examples.", "Questions": "1. Can the authors provide more detail on the computational efficiency of Mamba-CDSP, specifically how it compares in terms of model complexity and time to similar recent approaches in TCP?\n2. How does the approach handle noise and variability in real-world datasets compared to synthetic examples?\n3. Could the authors elaborate further on the impact of the de-correlation strategy on the predictive power in comparison to a standard covariate balancing approach?"}}
{"paper_id": "HxKSzulSD1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel concern with the concept of 'weak-to-strong deception' in the context of superalignment, which is not widely recognized but could have significant implications for the reliability of Large Language Models.", "Weaknesses": "While the concept of deception is intriguing, the paper might benefit from a more thorough exploration of potential solutions. The mitigation strategy discussed, involving bootstrapping with an intermediate model, appears limited in effectiveness.", "Questions": "How can the concept of 'weak-to-strong deception' be quantitatively measured in more complex real-world scenarios, beyond the controlled experimental settings discussed?"}}
{"paper_id": "wH8XXUOUZU", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel autoencoder architecture with impressive capabilities for high-resolution diffusion models, achieving significant acceleration without compromising on accuracy. The presentation of Residual Autoencoding and Decoupled High-Resolution Adaptation shows promise in effectively addressing optimization difficulties and generalization penalties associated with high spatial compression ratios. Results on ImageNet demonstrate credible improvements in speedup and Fidelity, emphasizing practical relevance.", "Weaknesses": "There is insufficient detail provided about potential limitations or scenarios where the proposed methods might not perform well. Additionally, while significant speedup is reported over SD-VAE-f8, a deeper exploration of how DC-AE compares across a broader range of architectures or datasets would enhance the evaluation. The underlying theoretical justifications or analyses for some of the architectural choices, such as space to channel residual learning, could be more robustly discussed. ", "Questions": "What are the underlying assumptions or potential limitations of the Decoupled High-Resolution Adaptation procedure? Can the approach be generalized across different domains outside of high-resolution models, and are there any anticipated challenges? Are there cases where DC-AE might still underperform against other high-compression autoencoders in practice?"}}
{"paper_id": "QWIg5e6mtT", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed approach, Heterogeneous-PSRO, addresses the limitations of existing methods by integrating heterogeneous policies to achieve lower exploitability in team games. The theoretical guarantees and empirical validation presented in this paper are solid and can potentially improve strategy exploration in cooperative-competitive settings.", "Weaknesses": "The paper claims lower exploitability and convergence without introducing additional computational complexity, yet practical computational implications are not well-addressed. Further insights into how the sequential correlation mechanism specifically operates in highly complex games would be beneficial.", "Questions": "1. How does the model ensure that integration of heterogeneous policies does not lead to a substantial increase in computational cost?\n2. How scalable is the H-PSRO framework in more complex, real-world simulations beyond the matrix games tested in the experiments?\n3. Does the proposed solution offer any particular benefits or impacts on non-zero-sum games or variants of zero-sum games where more than two teams are involved?"}}
{"paper_id": "kbSU5bwoRv", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. Innovative contribution in providing the first open-source zero-shot singing voice conversion model, capable of converting to human and non-human timbres.\n2. Effective disentanglement of the singing voice into content, timbre, and pitch features, enhancing the quality of conversion.\n3. A valuable large-scale dataset comprising 1,815 hours of singing voice data with 6,367 speakers, supporting robust model training and evaluation.", "Weaknesses": "1. The paper may benefit from clearer explanation and discussion of the method's novelty and how it significantly advances beyond existing SVC methodologies.\n2. More detail is needed on the objective and subjective evaluation metrics used to assess model performance.\n3. The reliance on multiple ASR models for feature disentanglement might introduce complexities that can affect the model's reproducibility and generalization.", "Questions": "1. The abstract mentions extreme conditions such as converting singing to animals' timbre. Could you provide more insights into how these experiments were structured and the results obtained?\n2. How does the model's performance compare to traditional methods in terms of computational efficiency and resource requirements for real-time applications?\n3. Can you elaborate on the strategies employed to ensure the elimination or minimization of timbre leakage when compressing content features?"}}
{"paper_id": "KyqtKhv6q1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The introduction of Differentiable Map Priors is an innovative and simple approach that leverages historical traversal data to improve 3D perception systems in autonomous vehicles. The framework is noted for its ease of integration into existing algorithms and its minimal computational overhead, which strengthens its applicability in real-world scenarios. The experiments conducted are comprehensive and demonstrate consistent improvements across various architectures on established datasets.", "Weaknesses": "While the approach shows promising results, the primary weakness lies in the relative novelty of the method's application. The extent of improvements could be contextual and might not be as impactful in areas where traversal history is less dense or specific. In addition, the extent of robustness in varied and unseen environments could be explored more thoroughly.", "Questions": "1. How does the use of Differentiable Map Priors handle locations with sparse historical data? Are there strategies to compensate for such areas?\n2. Could you elaborate on the scalability of this framework with respect to larger, more diverse datasets?\n3. Are there any specific limitations or trade-offs identified when integrating this method into existing 3D perception systems?"}}
{"paper_id": "lTL4t68BNc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper introduces a novel graph attention mechanism, RGA-IB, which is inspired by the Information Bottleneck principle, showing improved robustness against adversarial attacks in GNNs.\n2. Extensive experiments demonstrate the effectiveness of RGA-IB in maintaining classification accuracy under adversarial conditions.", "Weaknesses": "1. While the paper shows promising results, the explanation of how the Information Bottleneck principle is integrated into the graph attention mechanism could be more detailed.\n2. The paper does not fully address the potential limitations or scalability issues of the proposed method when applied to larger graphs.", "Questions": "1. Can the proposed RGA-IB mechanism be easily integrated into existing GNN architectures without significant computational overhead?\n2. How does RGA-IB perform on real-world large-scale graph datasets, in terms of both accuracy and computational cost?"}}
{"paper_id": "hWmwL9gizZ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 8, "Confidence": 3, "Strengths": "The paper presents a novel deep learning approach for immunogenicity prediction, which seems to outperform existing methods. It introduces a comprehensive dataset and a dual attention mechanism, providing potentially valuable benchmarks for vaccine design.", "Weaknesses": "The paper lacks clarity on some methodological details, specifically regarding the ablation studies to understand the contributions of different representations used. There are references to tables and metrics not clearly explained or missing, leading to interpretability issues. Additionally, some figures are challenging to interpret.", "Questions": "Could you clarify the rationale behind using sequence, fine, and coarse representations, and how each contributes to model performance? How does ProVaccine ensure generalizability across different types of pathogens like viruses and bacteria? Can you elaborate on the unexpectedly missing details and appendices referred to in the main text?"}}
{"paper_id": "IL85Ebjg9j", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel approach by introducing a computational trust-based discounting method that addresses conflicts in multi-view classification. The concept of using an instance-wise probability-sensitive trust discounting mechanism is unique and potentially valuable in resolving conflicts inherent in multi-view data. The extensive evaluation on six real-world datasets, including the introduction of a new metric, provides a comprehensive assessment of the method's reliability and effectiveness.", "Weaknesses": "The methodology may lack clarity due to the complexity of the computational trust-based discounting approach, which might hinder reproducibility. Additionally, while the results show improved reliability, the paper does not fully explore the limitations or potential downsides of the proposed approach, which could affect its applicability in diverse real-world scenarios.", "Questions": "What specific challenges or limitations might arise when applying the computational trust-based discounting method to domains beyond those used in the experiments?\nHow does the proposed method compare in terms of computational efficiency with existing multi-view classification techniques?\nCould you provide more details on how the new metric, Multi-View Agreement with Ground Truth, is constructed and how it specifically contributes to assessing prediction reliability?"}}
{"paper_id": "cnKhHxN3xj", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel approach to understanding neuron entanglement using the Wasserstein distance, which is an innovative measure for assessing polysemantic neurons. The proposed framework effectively leverages this distance to disentangle complex input-output relationships and improve performance under weight sparsity. The methodology and experiments provide insightful contributions to the field of interpretability in large language models.", "Weaknesses": "The paper lacks detailed explanations on some of the methodological aspects, such as how the Wasserstein distance is computed and why it is selected over other metrics. The presentation could be clearer, as some parts are difficult to follow, especially the technical details of the experimental framework. Additionally, the paper does not sufficiently address potential limitations of the proposed approach or its scalability.", "Questions": "1. How does the proposed measure of Wasserstein distance compare to other existing methods of measuring neuronal entanglement? \n2. What are the potential limitations or challenges of applying this disentanglement framework to real-world models in production settings?\n3. How does the performance of the mixture of sparse experts approach vary with different levels of sparsity? Are there scenarios where it fails to maintain accuracy?\n4. Could the framework be generalized or adapted to other model architectures beyond large language models?"}}
{"paper_id": "dgR6i4TSng", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. Introduces a novel application of quantum computations to parameter-efficient fine-tuning.\n2. Demonstrates significant parameter efficiency gains over traditional methods like LoRA.\n3. Successfully applies Quantum-PEFT to transfer learning benchmarks in both language and vision tasks, indicating broad applicability.", "Weaknesses": "1. The paper does not thoroughly explain the computational overhead or potential limitations of employing quantum computations in practical scenarios.\n2. Limited discussion on how the approach scales with different model sizes, particularly larger-scale models.\n3. Potential challenges in implementing quantum parameterization in existing classical systems are not addressed.", "Questions": "1. How do classical systems interact with Quantum-PEFT in terms of computational requirements and implementation complexity?\n2. Are there any specific types of tasks or models where Quantum-PEFT might not be the optimal approach?\n3. How does the proposed method compare with the latest advancements in PEFT methodologies, such as those that have been recently introduced or optimized?"}}
{"paper_id": "ye1mxb79lw", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses the complex problem of bilevel optimization by introducing a novel algorithm, BILBO, which is notable for its sample-efficient approach and its capability to handle decoupled settings. The theoretical foundation is solid, offering a sublinear regret bound, which is a significant achievement in this context. Empirical results bolster the theoretical claims, demonstrating the algorithm's effectiveness across both synthetic and real-world domains.", "Weaknesses": "While the presentation is robust, certain assumptions and decisions within the algorithm are not fully justified or explored, such as the specific mechanisms of function query selection. Moreover, the comparative analysis with related work could be more comprehensive, providing deeper insights into how BILBO distinguishes itself from existing methods.", "Questions": "1. How does BILBO perform relative to other leading bilevel optimization methods not outlined in the paper? A more expansive comparative analysis would be informative.\n2. What are the practical implications of the decoupled setting? Examples illustrating its necessity or benefit would be helpful.\n3. Could you elaborate on any assumptions made in the theoretical framework that might limit the algorithm's application in specific scenarios?"}}
{"paper_id": "1fwZJzGdKj", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel multi-agent collaborative framework for data selection, which effectively accelerates LLM training and improves data efficiency. The experimental results show significant improvements over state-of-the-art methods.", "Weaknesses": "The connection and justification for using a multi-agent approach are not entirely clear, and it seems that a simpler non-agent based model might also achieve similar results. The paper would benefit from a more detailed explanation of how the agent console integrates the information from various agents.", "Questions": "1. What specific benchmarks were used to evaluate the average performance gain of 10.5%? \n2. Can you provide more insight into the criteria used by the different data selection agents? \n3. How does the agent console decide the weightings of different agents during the training process?"}}
{"paper_id": "pISLZG7ktL", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a comprehensive empirical study on data scaling in imitation learning for robotic manipulation, providing insights into how data scaling can affect generalization performance. The rigorous evaluation protocol involving 40,000 demonstrations and 15,000 robot rollouts is a significant strength, highlighting the method's robustness and potential for real-world application. The proposal of an efficient data collection strategy is practical and shows promise for scalability.", "Weaknesses": "While the study is extensive, it focuses primarily on two tasks, which might limit the generality of the insights gained. The reported threshold beyond which additional demonstrations have minimal effect may not apply universally across more complex or varied tasks. The novelty of the contribution could be challenged, as it is unclear whether the revealed scaling laws are significantly different from those already understood in other domains. Additionally, more details on the nature of the tasks and environments would strengthen the claims.", "Questions": "1. How do the identified data scaling laws compare to those in other fields like NLP or vision? Are there any fundamental differences that should be highlighted?\n2. Could the data collection strategy be generalized to more complex tasks without a significant increase in cost or effort?\n3. How does the diversity of environments and objects translate to tasks beyond the ones studied? Would similar scaling apply to tasks involving more diverse object categories or more complex manipulations?\n4. What are the specific limitations of the current approach in handling tasks with drastically different dynamics or requiring fine-grained manipulation?\n5. Can the findings regarding the threshold for demonstrations be reliably applied to guide data collection efforts in other robotic manipulation tasks?"}}
{"paper_id": "dML3XGvWmy", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The approach provides a novel framework for self-evolving agents that utilize LLMs, which potentially explore a broader design space beyond manually crafted agents. The methodology could significantly impact how agents are designed and developed, particularly in fields requiring adaptive and evolving strategies.", "Weaknesses": "The concept of allowing agents to dynamically modify their own logic and behavior raises concerns regarding stability, safety, and predictability. The paper might not address possible risks, ethics, and limitations associated with uncontrolled evolutionary processes in AI systems. More experimental details and ablation studies could strengthen validation.", "Questions": "How does the Gdel Agent ensure safety and prevent destructive modifications during self-improvement? Are there specific mechanisms to guide or limit the self-evolution process to avoid undesirable outcomes?"}}
{"paper_id": "xlxGsX1pc7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. Introduction of a novel, diverse benchmark for evaluating mathematical problem-solving in LLMs, addressing current limitations in existing benchmarks.\n2. Includes a substantial number of open-ended, university-level problems, adding variety and complexity to the assessment of mathematical skills in language models.\n3. The inclusion of a multimodal component (20% visual problems) pushes the boundaries of current LLM evaluations by incorporating visual reasoning.\n4. Open-sourcing the dataset and the evaluation code promotes transparency and encourages further research and development in this area.", "Weaknesses": "1. The soundness of the methodology for using an LLM to judge the correctness of generated solutions may be questionable, as it introduces additional complexity and potential biases in evaluation.\n2. While the benchmark is diverse, the reliance on only teaching materials may not fully encapsulate novel or emergent mathematical problems seen outside academic settings.\n3. Limited maximum accuracy rates (63% text-based, 45% visual) in LLMs suggest that the dataset might be too challenging or that the capabilities of current models are still lacking significantly.\n4. The paper could provide more details on the specific challenges faced by LLMs on the visual component to suggest clear research directions.", "Questions": "1. How does the use of teaching materials ensure that the problems are representative of typical challenges faced in real-world applications of university-level mathematics?\n2. What specific steps were taken to ensure the unbiased evaluation of solutions using LLMs as judges, given the challenges in understanding the breadth of mathematical solutions?\n3. How could the benchmark be adapted to further include or emphasize real-world applications of mathematics beyond academic exercises?"}}
{"paper_id": "AjXkRZIvjB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a new benchmark, GSM-Symbolic, for evaluating LLMs on mathematical reasoning, which allows for more nuanced insights into the models' capabilities and shortcomings.", "Weaknesses": "The study focuses heavily on existing limitations of LLMs without proposing significant methodologies to address them, potentially limiting the practical impact of the findings.", "Questions": "How can the insights gained from using GSM-Symbolic be leveraged to improve the mathematical reasoning capabilities of LLMs beyond highlighting their limitations?"}}
{"paper_id": "IJiTI0fB0e", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to quantifying diversity in text-conditioned generative models by introducing the Conditional-Vendi and Information-Vendi scores. The theoretical grounding and decomposition of the Vendi diversity score offer a solid interpretation of the concept of diversity in the context of generative models. The methodology is well-structured, and the paper provides comprehensive experiments that demonstrate the effectiveness of the proposed metrics.", "Weaknesses": "The paper is quite technical, which may limit its accessibility to a broader audience unfamiliar with information-theoretic concepts. There could be more details about the experiments, such as the specific datasets used, to fully understand the implications and applicability of the proposed methods. Additionally, the practical advantages of adopting these new diversity scores over traditional ones are not thoroughly discussed, leaving the reader to infer the benefits.", "Questions": "1. How sensitive are the proposed diversity scores to the choice of kernel used in the matrix-based information measures?\n2. Can the methodology be extended to other modalities beyond text-conditioned generative tasks, such as audio or multimodal systems?\n3. What are the computational implications of using the proposed scores compared to existing diversity measures? Are there any efficiency concerns?"}}
{"paper_id": "fs2Z2z3GRx", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel approach for improving reverse-time sampling in image reconstruction tasks, specifically targeting scenarios with high measurement noise or severe ill-posedness. The presented method, Flow with Interpolant Guidance (FIG), demonstrates high competitiveness and improvement over state-of-the-art algorithms, particularly in challenging scenarios. Theoretical justification and empirical evidence are well-documented, showcasing the algorithm's effectiveness.", "Weaknesses": "The presentation of the paper could be enhanced to better convey the motivation and underlying intuition for adopting measurement interpolants. While the theoretical basis is provided, the explanation of why the proposed method is advantageous over existing methods in tackling specific limitations is not thoroughly detailed. The practical implications of the theoretical aspects could be more clearly articulated.", "Questions": "How do measurement interpolants specifically improve reverse-time sampling in cases of high noise or severe ill-posedness? Could the authors further elaborate on the practical advantages of the proposed method compared to existing approaches, especially in terms of computational complexity and scalability in various real-world applications?"}}
{"paper_id": "H4FSx06FCZ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed framework, SecureGS, provides an innovative approach to addressing the challenges of 3D Gaussian splatting steganography by enhancing security and rendering fidelity, while maintaining efficiency. The hybrid decoupled Gaussian encryption mechanism and density region-aware anchor growing and pruning strategy offer novel and practical solutions. Extensive experiments validate the superiority of SecureGS over existing methods, making it a significant contribution to the field.", "Weaknesses": "The complexity of the proposed SecureGS framework and its various components may pose challenges for practical implementation and adoption. Additionally, while the experiments are extensive, the specific computational costs associated with achieving the reported enhancements in security and rendering fidelity are not explicitly detailed.", "Questions": "1. Could the authors elaborate on the computational overhead introduced by SecureGS and how it compares to traditional GS steganography methods?\n2. How does SecureGS perform in real-time scenarios, and are there any specific limitations when deploying it in such environments?\n3. What are the potential trade-offs in terms of rendering fidelity when maximizing security features using the proposed framework?"}}
{"paper_id": "OGtUfA6Amo", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach with a hierarchical graph network to effectively handle Irregular Multivariate Time Series. The model not only handles varying scales and sampling rates across variables but also outperforms state-of-the-art methods in forecasting and classification tasks. The proposed Hi-Patch network demonstrates robust performance improvements over established baselines.", "Weaknesses": "The approach, while innovative, could be computationally intensive, given the fully connected graph networks and hierarchical architectures. The paper does not provide a detailed complexity analysis or discuss potential limitations in scalability. Presentation-wise, certain aspects of the hierarchical structure could be elucidated further for clarity.", "Questions": "How does the model handle missing data or extreme irregularities in time series? How scalable is the proposed approach when applied to very large datasets or real-time data streams? Are there specific downstream tasks where the performance benefits are more pronounced?"}}
{"paper_id": "ujpAYpFDEA", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a significant gap in current watermarking research for large language models by introducing imperceptibility as a key factor. The proposed Water-Probe algorithm effectively identifies existing watermarks and offers a novel approach, Water-Bag, to improve watermark imperceptibility, which is critical to maintaining user trust and avoiding attacks.", "Weaknesses": "The paper's discussion on the limitations of existing watermarking techniques could be expanded to include more diverse techniques beyond mainstream ones. Additionally, more details on the implementation and evaluation of the Water-Bag strategy are needed to fully understand its impact and practicality.", "Questions": "How does the Water-Bag strategy impact the computational efficiency of watermarking processes, and are there scenarios where its implementation may not be practical? Additionally, how does the effectiveness of Water-Probe correlate with the complexity of the watermarking techniques applied?"}}
{"paper_id": "7UKHNQIErp", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a robust formalism for understanding and developing direct preference alignment (DPA) loss functions, shedding light on the semantic underpinnings of these algorithms. The work offers a novel perspective on rigorously characterizing DPA losses and the relationships between recent proposals, which could lead to the creation of more effective AI alignment strategies.", "Weaknesses": "The technical depth and novelty of the framework may require readers to have a strong background in symbolic reasoning and discrete mathematics, potentially limiting accessibility to a broader audience. Additionally, specific comparative evaluations of the proposed formalism with alternative frameworks would enhance the clarity and strength of claims regarding its utility.", "Questions": "How do the derived symbolic expressions compare with traditional loss functions in practice, particularly in terms of computational efficiency and performance in aligning models with human preferences? Could the formalism be applied beyond existing DPA variants to entirely new domains or types of models?"}}
{"paper_id": "CpgWRFqxhD", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces novel modules for improving audio-driven talking video generation, such as the memory-guided temporal module for identity consistency and smooth motion, and the emotion-aware audio module to enhance audio-video interaction. It provides a robust solution to current challenges in ensuring seamless audio-lip synchronization and expressive animations. The model's ability to perform well without relying on traditional facial biases like landmarks and bounding boxes is an impressive aspect. Additionally, the paper is well-structured and includes extensive experiments demonstrating superior performance compared to state-of-the-art methods.", "Weaknesses": "The paper may not have thoroughly addressed the computational complexity and resource requirements of the proposed method, which could be crucial for real-world applications. Furthermore, the effectiveness of the approach on diverse facial structures, ethnicities, and challenging audio inputs might require further exploration to ensure generalizability. It is also important to understand the limitations of the emotion detection mechanism and how it might affect the output quality.", "Questions": "1. How does the memory-guided module scale with input length and does it have implications on memory consumption or processing time?\n2. What are the challenges in training the model, and how long does a typical training session take on available hardware?\n3. Does the model exhibit any bias or performance disparity when evaluated across different datasets not mentioned in the paper, such as those with underrepresented demographics?"}}
