{"paper_id": "B0OwtVEejJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper introduces a novel approach called TangleNAS that successfully integrates weight-entangled spaces with gradient-based NAS methods, offering the benefits of both paradigms. The experiments are comprehensive, covering various search spaces and validating the method's efficiency and effectiveness.", "Weaknesses": "The paper might have limited details on how to effectively tune the parameters in the weight-entangled space, which could be crucial for real-world applications. Additionally, the method might be complex to implement for practitioners unfamiliar with NAS.", "Questions": "How does TangleNAS specifically handle different types of operations in the architecture space, such as non-standard layers not considered in benchmarks? What are the potential limitations when applying this method to non-neural network settings?"}}
{"paper_id": "ZlEtXIxl3q", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides a novel approach by using contrastive loss functions to handle the nonlinearity in global epistasis models, offering significant improvements in accuracy and efficiency. Extensive benchmarking on both simulated and real-world datasets supports the effectiveness of the proposed method.", "Weaknesses": "While the approach is promising, the reliance on specific benchmarks may limit the generalizability of the findings. Additionally, the complexity introduced by using contrastive losses could lead to challenges in computational demands and implementation.", "Questions": "How does the computational complexity of the contrastive loss approach compare to traditional methods when scaled to larger datasets or more complex models?"}}
{"paper_id": "6yJuDK1DsK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "FEATHER introduces a novel approach to lifelong TTA with significant parameter efficiency. It achieves comparable or better performance than state-of-the-art approaches with much fewer parameters, offering improved computational efficiency. The method's ability to adapt without accessing the source data increases its applicability in real-world scenarios.", "Weaknesses": "The paper does not extensively address the potential limitations in adapting to non-CNN architectures, such as Transformers. The full benefits of adapter methods versus full fine-tuning in preventing error accumulation and forgetting are not thoroughly compared. Additionally, the paper could include more detailed empirical results on memory and computational savings compared to other methods.", "Questions": "Can FEATHER be adapted for use with Transformer architectures effectively? How do the memory and computational savings stack up against traditional fine-tuning approaches across various model sizes?"}}
{"paper_id": "lt6xKGGWov", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "Introduces an innovative use of neural estimation of mutual information for feature selection.\nCaptures complex dependencies between features and targets effectively.", "Weaknesses": "The experiments were limited to synthetic datasets, leaving the real-world applicability of the method untested.\nLack of clarity in some algorithmic descriptions.", "Questions": "How does the method perform on real-world data compared to other state-of-the-art feature selection methods?\nWhat are the computational resource requirements for running MINERVA on large datasets?"}}
{"paper_id": "Bvrc6kobWd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel gradient-based perspective on the role of margins in contrastive learning, which could lead to improved frameworks. The authors performed thorough experiments across multiple datasets to validate the proposed approach.", "Weaknesses": "The theoretical explanations provided are not very deep and might be difficult to follow for some readers. Additionally, the empirical results, while promising, do not strongly establish the superiority of the proposed method over existing approaches.", "Questions": "How does the proposed gradient-based understanding of margins differ from existing decision-boundary analyses, and what unique insights does it provide? Are the improvements observed significant enough to justify the added complexity?"}}
{"paper_id": "FJjHQS2DyE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper addresses the crucial gap in unsupervised domain adaptation by introducing CASA, which effectively handles label distribution shifts.", "Weaknesses": "The theoretical contributions might require stronger empirical validation to ascertain ubiquitous applicability beyond tested scenarios.", "Questions": "How scalable is CASA with respect to large-scale datasets with high-dimensional features?"}}
{"paper_id": "xelrLobW0n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel benchmark for evaluating the continual learning capabilities of large language models, specifically addressing gaps in existing benchmarks by considering important factors such as generalization, instruction adherence, and safety. The proposed methodology includes leveraging reasoning paths annotated by GPT-4, which contributes to mitigating catastrophic forgetting. TRACE as a benchmark is composed of a diverse suite of challenging tasks that are relevant to assessing contemporary LLMs.", "Weaknesses": "One key weakness of the paper is the reliance on pre-trained components like GPT-4 for reasoning annotation, which could limit the scalability and applicability of its approach across different domains. The presentation could be more streamlined, especially in how results are discussed across the main text and appendices. The integration of forward transfer metrics would have been beneficial for a comprehensive evaluation. Furthermore, the paper mainly focuses on extensions of prior metrics without introducing fundamentally new approaches to evaluate aligned LLMs.", "Questions": "1. How does the proposed benchmark and RCL method perform compared to existing continual learning baselines like episodic memory approaches? 2. Can you clarify the dataset sources included in TRACE and their potential for overlap with domains used in pre-training LLMs? 3. How does the safety assessment in TRACE compare in terms of constraints and evaluations to those found in safety-specialized benchmarks?"}}
{"paper_id": "bYQkOPvgDw", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. Overall, the paper presents a novel and promising approach to handling label noise in graph neural networks based on probabilistic graphical model.\n\n2. The sequentially introduced models PRGNN-V1 and PRGNN-V2 are clearly depicted and the proposed objectives are well motivated. \n\n3. Experiments are conducted on both noisy Homophilous and Heterophilous graphs. Clear improvements are observed compared to the implemented methods.", "Weaknesses": "1. Several crucial points remain unclear. First, it is still hard to understand why we need the uncertainty modeling. One might directly apply the encoder-decoder models equipped with the corresponding losses to handle the noisy labels. To be specific, what happens if we only use Q and P_\\varphi without the prior distribution, and conduct the training without the prior-related losses? The second question is related. From the experiments in Figure 3, the weighted cross-entropy loss wL_CE(P_\\varphi, Y_N) is much more important than others. So, it seems it is much more important to balance the loss between reconstruction of clean labels and that of noisy labels, other than the probabilistic modelling particularly given the label space (which is the random variable we focus on) is of small dimension. Third, the authors claim that the proposed probabilistic graphical model is able to avoid the issue of label smoothness in existing methods. However, it seems this is attributed to the application of prototype label vectors, instead of simple concatenation of labels with node embeddings for message passing. This is also observed in Figure 3. Overall, to make the proposed models effective, the authors introduce various techniques. However, it appears that the one which demonstrates consistent success is not aligned with the contributions claimed by the authors. \n\n2. About the introduction of the baselines, it is unknown which one applies label smoothness, and which one is proposed for Heterophilous graphs.\n\n3. It seems this paper is only applicable for the transductive setting not inductive setting. \n\n4. The last question is on the general research domain for learning from noisy graphs. While it is true that many data we process in practice definitely contain different kinds of noise, the proposed methods are always evaluated in the scenarios where we add artificial perturbation to the clean data. My point is we actually need some real tasks and perhaps some fixed benchmarks to evaluate the performance of different methods, such that we can clearly justify how the progress is made and how the challenges are addressed.", "Questions": "See the weaknesses above."}}
{"paper_id": "sKPzAXoylB", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper effectively addresses a critical gap in continual learning by simultaneously tackling catastrophic forgetting and plasticity loss using a novel utility-based approach. The use of Utility-based Perturbed Gradient Descent is innovative and shows promising results across various tasks, including reinforcement learning, highlighting its versatility.", "Weaknesses": "Certain aspects, such as the scalability of the approach and its computational overhead, are not thoroughly discussed. Also, the dependency on Taylor series approximation for the utility calculation might be a limitation under certain conditions, and more comparison with state-of-the-art methods could strengthen the claims.", "Questions": "How does the proposed method scale with larger, more complex datasets and models? Additionally, what are the computational overheads introduced by UPGD compared to traditional gradient descent methods?"}}
{"paper_id": "H8Qg1IIMaR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The study highlights a critical issue in the robustness of LLMs and VLLMs, focusing on their vulnerability to simple permutation attacks, which is an underexplored area.", "Weaknesses": "The paper's exploration is somewhat narrow, focusing primarily on one type of adversarial attack without proposing novel solutions or mitigations.", "Questions": "What alternative strategies could be explored to enhance the robustness of LLMs against adversarial permutations?"}}
{"paper_id": "tth2qXY7RU", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel quantization method using Super Floating-Point (SuFP) that effectively balances precision and computational efficiency. The proposed method is demonstrated to maintain or exceed the accuracy of full-precision models with significant improvements in computational capability and energy efficiency. The inclusion of multi-region quantization with scalable bias and efficient hardware implementation is a notable advancement.", "Weaknesses": "The paper may require further validation across a broader range of models and datasets to establish its general applicability. Additionally, the implementation details and hardware requirements for the proposed SuFP method might need more comprehensive exploration to fully understand its integration challenges.", "Questions": "What are the specific limitations or scenarios where SuFP might not perform as well? How does the complexity of implementing SuFP compare to existing quantization methods in real-world applications?"}}
{"paper_id": "fjf3YenThE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel variance-reduction technique to overcome limitations of current zeroth-order hard-thresholding algorithms, thus improving convergence rates and applicability in high-dimension optimization tasks.", "Weaknesses": "The paper could have provided more in-depth experimental validations across diverse datasets and real-world scenarios to further substantiate the claimed improvements. Additionally, theoretical assumptions like restricted strong smoothness may limit broader applicability.", "Questions": "How do the improvements in convergence rates of the new algorithm compare to other modern sparse optimization techniques not based on zeroth-order methods? Which practical scenarios most benefit from removing restrictions on the number of random directions?"}}
{"paper_id": "H9DYMIpz9c", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel and efficient approach for data distillation specific to autoregressive tasks, addressing a clear gap in the current literature. The method FARZI is shown to be effective, achieving 98-120% of the full-data performance with only 0.1% of the dataset, demonstrating significant potential for reducing computational and environmental costs.", "Weaknesses": "The scalability and practicality of applying FARZI to larger language models and datasets remain unexplored. Further analysis regarding the overall time efficiency compared to traditional training methods is needed to fully assess the benefits.", "Questions": "How does FARZI scale with significantly larger datasets and more complex models in real-world scenarios?"}}
{"paper_id": "4kLVvIh8cp", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents an innovative approach to offline reinforcement learning using non-linear function approximations with instance-dependent regret guarantees. It extends pessimism-based methods from linear to non-linear contexts, attempting to address a critical gap in current methodologies. The proposed PNLSVI algorithm is notable for its statistical optimality and efficiency, potentially having significant implications for applications with complex state-action spaces.", "Weaknesses": "The paper's assumptions, particularly regarding data coverage, might be too strong for practical scenarios. The lack of empirical validation or experimental results makes it difficult to assess the algorithm's practical effectiveness and competitiveness against existing methods. Additionally, the paper does not address potential limitations or scenarios where the algorithm may not perform optimally, alongside lacking a lower bound match for the general function approximations considered.", "Questions": "How does the PNLSVI algorithm perform in practical experiments or benchmarks compared to other non-linear offline RL methods? What types of practical applications or scenarios are most suitable for the PNLSVI algorithm given its assumptions and theoretical guarantees? Can the coverage assumptions be relaxed while maintaining efficiency and optimality?"}}
{"paper_id": "CSpWgKo0ID", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The study addresses an important gap in understanding the strategic interactions of LLMs in social settings. It uses a novel approach by employing behavioral game theory to evaluate LLMs in repeated games. The research design, involving a variety of 2x2 game scenarios, allows for a comprehensive assessment of LLM behavior.", "Weaknesses": "The paper could benefit from a deeper exploration of why LLMs struggle with coordination games compared to self-interest games. The robustness of findings related to varying prompts and payoff matrices needs more clarity. Additionally, the methodology for simulating human-like strategies using behavioral prompts requires further elaboration.", "Questions": "How does the use of different prompting techniques specifically enhance the coordination capabilities of LLMs? Are there particular patterns in LLM behavior that suggest specific avenues for improving their performance in coordination games? What implications do these findings have for the future deployment of LLMs in real-world social settings?"}}
{"paper_id": "gwbQ2YwLhD", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a gap in understanding the impact of scale on structure learning algorithms, providing theoretical insights and experimental validation. It highlights a crucial consideration for interpreting results in higher-dimensional and non-linear settings.", "Weaknesses": "The findings may not be entirely novel, as the influence of scale is somewhat recognized in lower dimensions. The practical significance might be limited given many real-world applications may already account for these considerations. Clarity in the presentation of theoretical proofs could be improved.", "Questions": "How do the proposed solutions, such as normalization, impact the overall complexity and computation time of structure learning algorithms? Are there specific types of datasets or scenarios where the observed scale influence becomes negligible?"}}
{"paper_id": "eIYDKNqXuV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed Village-Net Clustering method is computationally efficient and automates the determination of the number of clusters, providing a scalable solution for large datasets.", "Weaknesses": "The accuracy of Village-Net Clustering heavily depends on the initial K-Means step. The method may require further refinement to achieve higher accuracy.", "Questions": "How does the Village-Net Clustering method compare with more computationally intensive clustering methods on high-dimensional datasets beyond MNIST and Letters?"}}
{"paper_id": "bAMPOUF227", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel framework, SuperContext, which successfully integrates supervised language models (SLMs) into large language model (LLM) in-context learning. This integration leads to improved generalizability and reduced hallucinations, especially in out-of-distribution (OOD) tasks. The inclusion of comprehensive experiments across multiple datasets strengthens the findings.", "Weaknesses": "The methodology, while innovative, can be viewed as an incremental improvement over existing techniques. The use of SuperContext to combine the capabilities of SLMs and LLMs is well executed but lacks groundbreaking theoretical contributions. Furthermore, the paper could benefit from a more detailed discussion on the scalability and potential constraints in diverse application domains.", "Questions": "How does the SuperContext framework handle scalability when integrating multiple task-specific SLMs in real-world applications? Are there any benchmarks or baseline comparisons provided for computational efficiency compared to standalone LLMs?"}}
{"paper_id": "eNoiRal5xi", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper presents a novel approach that combines parameter and data space perturbations, leading to improved generalization across unseen domains. The method consistently outperforms existing SAM variants in benchmarks, demonstrating its robustness.", "Weaknesses": "The theoretical foundations for the approach are not thoroughly established. There may be overlap with existing techniques such as domain augmentation, raising questions about the uniqueness and novelty of the approach.", "Questions": "How does the proposed UDIM framework theoretically guarantee improved generalization compared to existing domain adaptation techniques?"}}
{"paper_id": "uU0Adp7Sfo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel collaborative-competitive framework for GANs, which addresses limitations in existing models by integrating both competition and collaboration aspects. This approach is theoretically grounded in game theory and demonstrated to be effective with comprehensive experiments.", "Weaknesses": "The presentation of results could be clearer, and the visual examples provided are hard to assess due to their size. The paper could strengthen its comparisons to other methods that have similar objectives.", "Questions": "How does CCGAN performance compare to leading GAN models in quantitative measures like FID? What are the implications of this model when applied to datasets beyond those with regularly shaped objects?"}}
{"paper_id": "dKju7tbe6D", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel architecture combining iterative and parallel computation, outperforming existing models on multiple benchmarks. It demonstrates strong generalization and efficiency, requiring fewer parameters and computation steps than competing approaches.", "Weaknesses": "The paper might benefit from additional clarity and examples illustrating how the parallel computation aspect is explicitly realized in the model compared to previous architectures. Some of the architectural decisions are not immediately intuitive and need further explanation.", "Questions": "How exactly does the parallel reasoning aspect differentiate itself from having merely a larger internal state, and where in the model architecture is this parallelism most crucially enforced or realized explicitly?"}}
{"paper_id": "DL7JWbdGr3", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel application of self-supervised learning in the context of epidemic forecasting, a field with significant practical implications. The use of diverse datasets and tasks like random masking and seasonal detection is innovative and shows promise, with results indicating improved accuracy over state-of-the-art models. The methodology is well-articulated, with comprehensive experiments that demonstrate the model's adaptability and robustness.", "Weaknesses": "While the results are promising, the paper lacks clarity on certain methodological choices, such as the selection of hyperparameters, and does not thoroughly address potential limitations in its approach. Moreover, there is a notable absence of a detailed comparison with existing baseline methods using the same pre-training paradigm. This omission makes it challenging to ascertain the isolated impact of the proposed self-supervised tasks versus architectural improvements or increased computational resources.", "Questions": "How does the model ensure independence in the pre-training phase when using datasets from epidemic events that might overlap temporally or geographically? Additionally, could the authors clarify the rationale behind the choice of specific parameters for tasks like last segment masking and how these settings influence model performance?"}}
{"paper_id": "1PaDPHDhwe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper proposes a novel approach to address the trade-off between robust and average accuracies using a simple yet effective post-processing technique. It introduces a new evaluation metric, robust coverage, which provides a comprehensive measure of the trade-off and debiasing algorithm performance.", "Weaknesses": "The paper's approach may be seen as a straightforward application of scaling, which might limit its perceived novelty. There could also be concerns regarding the generalizability and scalability of the instance-wise adaptive scaling method across different datasets and settings.", "Questions": "How does the proposed method compare to other state-of-the-art solutions in terms of computational complexity and scalability? Is there a theoretical explanation for why class-specific scaling effectively manages the trade-off between robust and average accuracies?"}}
{"paper_id": "iI7hZSczxE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a notable limitation in disentangled representation learning for time series data by introducing a novel method, DISOC, that leverages contrastive learning. This approach shows significant improvement in performance, especially in handling correlated attributes, and introduces a new metric, the Time Disentangling Score (TDS), which enriches the field.", "Weaknesses": "The paper could benefit from clearer explanations of the method's components, specifically the integration of l-variational inference and contrastive learning. Additionally, there may be a lack of comprehensive evaluations across various datasets to thoroughly demonstrate generality and robust performance.", "Questions": "What datasets were used to validate the effectiveness of DIOSC? How does the Time Disentangling Score compare with other metrics in measuring disentanglement quality?"}}
{"paper_id": "qrGjFJVl3m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 5, "Strengths": "The paper presents a significant advancement in the field of large vision-language models by integrating fine-grained visual understanding and multilingual capabilities. The Qwen-VL models outperform existing benchmarks, showcasing superior performance in various visual-language tasks. The comprehensive multi-stage training pipeline on a diverse multilingual and multimodal corpus is a notable strength, providing the models with enhanced generalization and understanding abilities.", "Weaknesses": "While the models demonstrate excellent performance, the paper could further elaborate on the scalability of the training process and potential challenges in integrating additional modalities in future work. Additionally, more comparison and differentiation with proprietary models would be beneficial to understand the unique contributions of Qwen-VL more clearly.", "Questions": "What specific challenges are anticipated in integrating further modalities into Qwen-VL models, and how might these be addressed? How does the model's performance compare with that of leading proprietary models specifically in the realm of multilingual visual-language dialogs?"}}
{"paper_id": "qup9xD8mW4", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "+ The introduction of behaviour distillation can enrich the literature and direction of Dataset Distillation. Different from standard DD, behaviour distillation does not require access to the expert datasets. This is quite close to a standard /basic RL setting and could be a good starting point.\n\n+ The author's writing is easy to follow and pretty clear on the technical details\n\n+ The experimental section show promising results using the proposed algorithm HADES.", "Weaknesses": "- Although it is interesting, the proposed behaviour distillation seems to not have a clear motivation on why it can be useful (what's the motivation for proposing this problem and its potential application, besides DD hasn't been applied to RL), and why not directly formulating the problem on an expert dataset. Distilling directly from scratch can make the problem a lot harder.\n- It would be great if the authors can discuss the behaviour difference between a standard RL algorithm and a synthetic data-driven RL algorithm (HADES). What's the potential benefit?\n- One missing citation on BPTT [2]. It would be nice to add some discussion on optimization algorithm in DD, for example BPTT (the original one[1] and momentum-based[2]).\n\nOverall the paper is quite interesting. Looking forward to authors' response to the above questions.\n\n[1] Dataset Distillation\n\n[2] Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks", "Questions": "See above."}}
{"paper_id": "Zw8YxUWL4R", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper successfully introduces the P+ conditioning space, demonstrating improved expressiveness and control in image synthesis. The experiments conducted on a Stable Diffusion model support the claims made about improved control over image synthesis. The proposed Extended Textual Inversion (XTI) technique shows significant improvements over existing methods like Textual Inversion (TI).", "Weaknesses": "While the P+ space shows promise, the novelty of the contribution could be considered incremental as it builds upon existing diffusion models and techniques. The explanation regarding the disentanglement of shape and appearance in the P+ space could be clearer and backed by more illustrative examples. Some technical details on how per-layer tokens are managed in the U-Net architecture would help solidify the understanding of the method.", "Questions": "Could you provide more technical details or possible mathematical formulations explaining how per-layer tokens are incorporated into the U-Net architecture to achieve the desired effects? How does the computational cost of using P+ in cross-attention layers compare to traditional methods?"}}
{"paper_id": "3NXhwkZGjz", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach in the field of source-free unsupervised domain adaptation by transforming the task into a semi-supervised learning problem. This innovative perspective helps address privacy concerns and provides a flexible method that can be integrated into existing systems.", "Weaknesses": "The motivation for using multiple hypotheses is not clearly justified, and the practical utility of accessing multiple high-quality hypotheses in common scenarios was not thoroughly discussed. Also, the proposed method does not achieve state-of-the-art results in all evaluated tasks, suggesting room for improvement.", "Questions": "How does the proposed method handle scenarios where multiple hypotheses are impractical or unavailable, and how does it perform in such cases compared to others with single hypotheses?"}}
{"paper_id": "Fn655mJ4bv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces one of the first approaches to post-hoc interpretation specifically for structured output predictions, addressing a gap in current methods. It presents an energy-based approach that considers correlations between output variables, leading to potentially improved explanation quality.", "Weaknesses": "The paper primarily focuses on tabular data input, which may not fully explore interpretability for other data types like images where regional importance is crucial. The documentation of the learning algorithm lacks sufficient detail, raising questions about its robustness to hyperparameter choices like tau and k.", "Questions": "1) How does the learning algorithm react to different values of tau and k? Is it robust to such changes?\n2) Can the method be adapted to better handle datasets like images where correlated features might not be straightforward?\n3) How does SOInter handle noise in test images?\n4) Can you elaborate more on the \"relative F1\" metric mentioned in the text multi-label classification section, as there seems to be a confusion?"}}
{"paper_id": "VyMW4YZfw7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper challenges the complexity of modern GNN architectures, proposing simpler methods that achieve competitive performance. It offers a fresh perspective through the use of classical nonparametric techniques in the spectral domain, which is an innovative approach in the context of graph-based learning.", "Weaknesses": "While the proposed methods seem promising, the paper may lack comprehensive experiments across diverse datasets and settings to fully validate the claims. Additionally, the understanding of the exact conditions under which these simpler models outperform complex ones is still not entirely clear.", "Questions": "How do the proposed spectral methods perform on larger, more diverse datasets beyond the common benchmarks mentioned? Are there specific types of graphs or conditions where these methods may not perform as well? How sensitive are the results to the chosen kernel functions and approximations?"}}
{"paper_id": "09xFexjhqE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed method, AutoLoRa, addresses the gradient divergence problem in Robust Fine-Tuning by efficiently partitioning the optimization process. The paper's empirical validation demonstrates significant improvements over existing methods, particularly in enhancing adversarial robustness while reducing sensitivity to hyperparameters.", "Weaknesses": "The explanation of hyperparameter scheduling automation lacks clarity, and the provided ablation studies do not sufficiently highlight the benefits of heuristic strategies. Some sections in the presentation, like the explanation of constant factors in formulas, could also be improved for clarity.", "Questions": "How does AutoLoRa perform with different network architectures beyond ResNet? What are the specific advantages of the introduced heuristic strategies for hyperparameter scheduling?"}}
{"paper_id": "nmBjBZoySX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "The AdaGLT framework effectively addresses limitations in existing GLT methods by automating, adapting, and dynamically integrating the process of identifying sparse structures within GNNs. It exhibits improved scalability and efficiency demonstrated through extensive theoretical and experimental validation.", "Weaknesses": "The paper could explore more comprehensive comparisons with a broader range of baseline methods, and the scope of datasets used in experiments may not cover wider real-world applications sufficiently.", "Questions": "How does the AdaGLT framework handle extremely large-scale graphs in real-world applications, and what are the computational trade-offs involved when compared to conventional GNN approaches?"}}
{"paper_id": "di52zR8xgf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a significant open alternative to black-box generative models, improving the infrastructure of Stable Diffusion with enhanced architecture and novel conditioning methods. It highlights improvements in image quality and adherence to prompts while maintaining transparency.", "Weaknesses": "The model still relies on a two-stage process and requires further work for single-stage generation and text synthesis improvements. There could be additional comparisons with other state-of-the-art pixel-space diffusion models.", "Questions": "How does the performance of SDXL compare quantitatively to the latest pixel-space diffusion models? Can the two-stage training approach be effectively merged into a single-stage to enhance the model further?"}}
{"paper_id": "0b328CMwn1", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces Activation Prompts (AP) as a promising extension of visual prompting, leveraging intermediate activation layers to improve efficiency and accuracy. Extensive experimentation over 29 datasets provides thorough validation of the claimed benefits of AP over existing methodologies, indicating strong empirical evidence for its effectiveness.", "Weaknesses": "Despite the promising results, the novelty of the approach may be questioned due to similarities with existing methods such as VPT, which have already explored variations in prompt application. Furthermore, detailed comparisons with those existing methods might not be entirely consistent or fair due to varying experimental setups. Clarity on parameter counts and configurations is necessary to ensure the validity of the efficiency claims.", "Questions": "How does the approach compare against VPT and similar methodologies in a controlled, identical experimental setting? Can further insights into the theoretical underpinnings of AP be provided to differentiate it more clearly from previously established techniques?"}}
{"paper_id": "AMDKqZcZbi", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a key challenge in machine learning related to catastrophic forgetting and presents a novel solution inspired by biological systems. The proposed model, MESH, demonstrates strong performance in both rapid learning and memory retention tasks, outperforming existing baselines. The use of a biologically inspired approach is innovative and the results are promising, suggesting a potential new direction for developing robust machine learning models.", "Weaknesses": "While the proposed model shows promise, its evaluation is primarily limited to the sequential Morris Water Maze task, and its general applicability to other domains or tasks is unclear. The paper could improve by expanding its evaluation to a wider range of tasks to better validate the model's generalization capabilities. Additionally, the explanation of the model's architecture and theoretical foundations could be more detailed for full clarity.", "Questions": "Can the proposed model be applied to other types of tasks beyond those that resemble the Morris Water Maze? How does the model handle tasks that require different types of learning beyond spatial and sequential tasks? What are the limitations of the biologically inspired architecture when scaled to larger, more complex tasks or datasets?"}}
{"paper_id": "am7BPV3Cwo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a relevant and realistic problem of OOD detection in imbalanced datasets, which has significant practical implications. The proposed method, ImOOD, provides a novel combination of post-hoc normalization and training-time regularization for tackling the issue effectively. The method demonstrates consistent improvements over state-of-the-art methods on several benchmarks. The paper is well-structured, making it easier to follow the narrative from problem identification to solution and evaluation.", "Weaknesses": "The paper assumes that only the ID dataset is imbalanced and does not consider scenarios where the OOD datasets might also be imbalanced, which could limit its generality. The impact of post-hoc normalization is not clearly isolated and tested independently, raising questions about its standalone effectiveness. Additionally, explanations about learning label distributions and their specific roles in different phases of the method could be more explicit. More comprehensive empirical validation across varied OOD datasets would strengthen the results.", "Questions": "How does the ImOOD framework perform if the OOD datasets are also imbalanced? Is the learning of label distributions used consistently across all phases, and how is it updated? Could the authors provide more details on the potential limitations when both ID and OOD datasets exhibit imbalances?"}}
{"paper_id": "R1crLHQ4kf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The method shows high accuracy in detecting adversarial attacks across various ASR systems, demonstrating robustness even under adaptive attacks. The approach is generalizable and does not rely on altering input data.", "Weaknesses": "The novelty may be limited as it builds on existing concepts of analyzing output probability distributions without substantial theoretical innovation. There might be computational overhead with analyzing distributions at each time step.", "Questions": "How does the method perform when scaled to real-time applications, and what are the computational costs involved?"}}
{"paper_id": "i7P2mK3x3o", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel flow-based neural ODE model (Q-flow) that is well-motivated and addresses a significant gap in existing flow-based models by learning transportation between arbitrary distributions. The method shows promising empirical results, particularly in high-dimensional data applications, and the approach is clearly presented with applications to real-world tasks like image-to-image translation.", "Weaknesses": "Although the Q-flow model demonstrates strong empirical performance, theoretical analysis of the model's guarantees and potential limitations compared to other state-of-the-art methods is less emphasized. The computational efficiency and practical scalability of the proposed method in large-scale applications are not fully explored.", "Questions": "What are the computational costs and limitations in terms of scalability of the Q-flow model when applied to very large datasets or in real-time applications? How does the model compare to other state-of-the-art methods in terms of both accuracy and efficiency?"}}
{"paper_id": "YIls9HEa52", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to overcoming the limitations of traditional rSLDS models by introducing the irSLDS model, which provides a flexible and efficient framework for analyzing trial-varying neural data. The proposed method integrates latent geometry and flexible state cardinality, capturing recurrent dependencies, which enriches the analysis of neural dynamics. The use of a PDE framework for efficient computational inference is a technical strength.", "Weaknesses": "While the model shows promise, the paper might lack extensive comparison with other state-of-the-art models beyond rSLDS, reducing the scope of its claimed improvements. The application of the model, while validated on real and synthetic data, might benefit from further benchmarking in more diverse settings. The inference process, while described as efficient, could be expanded in terms of computational details and potential limitations in larger-scale implementations.", "Questions": "How does the performance of the irSLDS model compare with other advanced models in terms of accuracy and computational efficiency beyond the settings presented? What are the limitations of this approach in practical applications, especially regarding scalability and computational resources needed for large datasets?"}}
{"paper_id": "koYsgfEwCQ", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper proposes an innovative method for 3D scene decomposition using object-centric voxelization, offering improvements over existing methods. It effectively tackles occlusion issues and enables dynamic scene editing including manipulation of geometric shapes and paths.", "Weaknesses": "Some aspects of the method are potentially under-explained, such as the exact implementation and the concept of an 'episode'. The explanation for the warmup and dynamics stages can be confusing. It is also unclear how some modules, like the connected components and the feature graph, are integrated. Furthermore, the results are somewhat limited to a few real videos, and segmentations might need improvement.", "Questions": "Could an invertible mapping be applied instead of using two separate deformation networks to ensure cycle-consistency? This approach might simplify the method or improve results."}}
{"paper_id": "o6AN2ZoNw2", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a method for open-vocabulary semantic segmentation that achieves competitive performance without the use of large-scale pre-trained models. The approach is simple yet robust, leveraging pseudo-mask generation and language supervision effectively.", "Weaknesses": "The novelty of the approach is limited, with several similarities to existing methods. The experimental comparisons lack some crucial baseline models, and the claims of state-of-the-art performance are not fully substantiated. Some methodological details and evaluation protocols need further clarification.", "Questions": "How does the pseudo-mask generation compare to other methods like LOST or TokenCut in terms of mask quality? Why were specific models in Tables 3 and 4 presented multiple times with different results? How are the binary masks predicted by the model converted to dense masks for evaluation?"}}
{"paper_id": "HXZK1Z8tHa", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed ShareFormer architecture effectively reduces computational overhead and improves trainability, with empirical results that outperform state-of-the-art methods in various image restoration tasks such as super-resolution and denoising. The methodology offers a novel balance between efficiency and accuracy.", "Weaknesses": "The paper's explanation of the core innovations, like shared attention map mechanisms, lacks clarity and may not be as novel as claimed, given prior work in this area. Additionally, some potential unfairness in comparisons due to differing training setups raises questions.", "Questions": "Does the proposed method rely significantly on larger training patch sizes compared to baseline methods, and could this affect result fairness? Can the authors provide more clarity on the detailed operations of shared attention maps within the architecture?"}}
{"paper_id": "QO3yH7X8JJ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses a relevant gap in the field of super-resolution, providing an innovative approach to using pre-trained diffusion generative models without retraining.\n2. Strong empirical results demonstrating the effectiveness of the method across multiple datasets using standard metrics.\n3. The introduction of the Perceptual Recoverable Field (PRF) as a theoretical basis for the method is novel.", "Weaknesses": "1. The proposed method might not address the computational efficiency of noise injection and its calculation in varying scenarios.\n2. The clarity of the method's explanation could be improved for better understanding and reproducibility.\n3. Limited discussion about potential limitations or scenarios where the method might not perform optimally.", "Questions": "1. How does the noise injection process scale with image size and differing levels of detail in low-resolution images?\n2. Can the methodology be applied to domains other than images, such as video or 3D models?\n3. What are the performance benchmarks regarding computational cost compared to traditional scaling methods?"}}
{"paper_id": "sNtDKdcI1f", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a comprehensive examination of the influence of output length in RLHF models. The analysis is thorough and sheds light on a previously underexplored bias in reward modeling. The methodological approach is robust, utilizing multiple datasets to ensure generality.", "Weaknesses": "The presentation could be improved, as figures and tables are not entirely self-explanatory, requiring readers to frequently refer back to the text for clarification. Additionally, the implications of the findings on practical applications and future RLHF tasks need clearer articulation.", "Questions": "How can practitioners mitigate the length bias in RLHF without compromising on the actual quality improvements? Are there qualitative differences in output when longer responses are discouraged in RLHF? What specific steps can be recommended to refine reward models beyond addressing length bias? Are the results generalizable to other RLHF applications beyond dialogue and question answering?"}}
{"paper_id": "HFtrXBfNru", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important issue in the field of Graph Neural Networks by focusing on temporal generalization in evolving graphs. The proposed method, SMART, shows strong results in both synthetic and real-world scenarios, highlighting the efficacy of self-supervised learning for minimizing representation distortion. The combination of theoretical analysis and a practical approach adds robustness to the study.", "Weaknesses": "The lack of comparison with state-of-the-art methods limits the ability to gauge the full impact of SMART. Additionally, some assumptions made for the theoretical framework appear strong, which could limit real-world applicability. A broader evaluation of different graph types and more comprehensive benchmarks would enhance the paper.", "Questions": "How does SMART perform compared to other state-of-the-art methods in scenarios with diverse graph types? Can the assumptions in the theoretical analysis be relaxed to improve applicability?"}}
{"paper_id": "4u0ruVk749", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 4, "Strengths": "The authors introduce a novel application of diffusion models for estimating individual treatment effects, which appears to effectively handle unobserved confounders.", "Weaknesses": "The methodology and assumptions behind the proposed approach are not entirely clear, and the paper does not adequately address potential limitations of the diffusion approach compared to more conventional methods.", "Questions": "How does the proposed diffusion model specifically address stability and flexibility issues observed in previous generative models like VAEs and GANs for confounder simulation?"}}
{"paper_id": "Pa6SiS66p0", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The introduction of a multi-modal benchmark for continual learning is a significant step forward for the field. The idea of leveraging multiple modalities to reduce catastrophic forgetting is well-motivated and holds promise for real-world applications. The proposed method demonstrates improvements over baseline methods in challenging multi-modal continual learning scenarios.", "Weaknesses": "The evaluation is limited to a single dataset, which may not fully demonstrate the robustness of the method across different domains. More comprehensive comparisons with existing state-of-the-art methods in both multi-modal and unimodal continual learning would strengthen the findings. Additionally, some aspects of the methodology may require clearer explanation for reproducibility.", "Questions": "Would the performance improvements observed in the VGGSound dataset extend to other multi-modal datasets? Are there specific limitations or potential drawbacks of the proposed method when applied to other domains or modalities beyond audio and visual data?"}}
{"paper_id": "rkplYfqUr0", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper proposes a novel generative zero-shot text classification framework, which addresses miscalibration and brittleness issues seen in discriminative approaches.\nThe use of natural language descriptions and aggregating multiple paraphrase results adds robustness and improved performance.\nThis work demonstrates significant performance improvements across diverse benchmarks, supporting the framework's effectiveness.", "Weaknesses": "The method relies on manually crafted label descriptions and paraphrases generated by external models like ChatGPT, which could introduce bias or inconsistency.\nThere may be concerns regarding the scalability or feasibility of creating high-quality descriptions for diverse, larger-scale applications.\nThe evaluation focuses heavily on specific NLP tasks, raising questions about generalizability to other domains or tasks.", "Questions": "How consistent is the approach when generating paraphrases from different models or with varying levels of label description quality?\nCould the reliance on human or AI-generated descriptions introduce potential biases to the classification outcomes?\nWhat are the computational requirements and time considerations when employing GEN-Z compared to traditional approaches?"}}
{"paper_id": "pTU2X9mUBe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a comprehensive and publicly available dataset that addresses critical gaps in last-mile delivery research. Its scale and diversity make it a valuable resource for advancing algorithm development in logistics. The dataset supports various research tasks and benchmarks.", "Weaknesses": "The paper could provide more details on the exact methodologies used to collect and verify the dataset. There might be limitations in generalizing findings from this dataset to other contexts, given its focus on specific regions in China.", "Questions": "How does the dataset address privacy concerns, given the sensitive nature of logistics data? Are there any collaborations planned with other organizations to expand the dataset's geographic coverage?"}}
{"paper_id": "uhtQyRrTzY", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel and scalable method for creating large-scale multi-view datasets without the need for 3D annotations, which is a significant advantage for real-world applications. The proposed approach, MIMIC, improves the performance of dense vision tasks while reducing reliance on expensive and complex annotated datasets.", "Weaknesses": "The presentation could be improved for clarity, particularly in detailing the methodology. The paper might be seen as specialized, focusing on dense vision tasks, and may not address tasks outside this domain extensively.", "Questions": "How does the performance of MIMIC-based models compare to models trained on manually annotated datasets across a broader range of tasks beyond the dense vision tasks specified?"}}
{"paper_id": "WZfatbNdLV", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces the TrASPr model, which uses a novel combination of transformers and Bayesian Optimization to predict and manipulate RNA splicing with high accuracy. The model's ability to outperform state-of-the-art models in tissue-specific splicing and its potential utility in therapeutic applications represent significant contributions to the field. The comprehensive evaluation against other models demonstrates its strengths convincingly.", "Weaknesses": "Despite the promising methodology, the paper may suffer from limitations in addressing splicing complexity across various tissues and conditions. The dependency on specific datasets like ENCODE for validation might limit generalizability. Additionally, the paper could provide more detailed results on the limitations or failure cases of the proposed model.", "Questions": "How does the TrASPr model perform across a wider range of tissues and less-studied splicing events? Are there plans for expanding the datasets used in pretraining and fine-tuning to enhance model generalizability? Additionally, could the authors clarify how the integration with the Bayesian Optimization algorithm was validated for different splicing scenarios?"}}
{"paper_id": "YkRwadXWHd", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel multi-modal framework combining video, keypoints, and optical flow for continuous sign language recognition (CSLR), enhanced through a hierarchical knowledge distillation approach. It achieves state-of-the-art results on multiple datasets. The integration of diverse modalities addresses the challenge of dynamic hand movements and enhances feature extraction while reducing computational costs.", "Weaknesses": "The novelty may be considered incremental as it builds upon existing multi-modal and knowledge distillation techniques. There are concerns regarding the computational aspects and efficiency of the method. The claim of state-of-the-art results needs clarification regarding the experimental setup and baseline comparisons.", "Questions": "What are the specific computational resources required for training and inference? How sensitive are the model's hyperparameters and loss weightings across different datasets? Can the model maintain performance in more challenging environments beyond the provided datasets? Will the code be made available to ensure reproducibility?"}}
{"paper_id": "q4Bim1dDzb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 6, "Confidence": 3, "Strengths": "The paper proposes a novel unified voxelization framework for inverse rendering that significantly accelerates the process compared to traditional methods. The method efficiently integrates illumination modeling using spherical Gaussians, which supports varying lighting conditions and maintains high rendering quality.", "Weaknesses": "The paper lacks detailed experimental evaluation across more diverse datasets and complex scenes, which raises questions about the robustness and scalability of the approach. There is also insufficient comparison with existing state-of-the-art methods to demonstrate clear advantages.", "Questions": "How does the proposed method perform on complex scenes compared to existing state-of-the-art methods in terms of both speed and quality? Can the approach handle dynamic scenes or temporal changes in illumination effectively?"}}
{"paper_id": "39cPKijBed", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed method for mitigating dataset bias in diffusion models is novel and addresses an important issue in generative modeling. The method is well-supported by both theoretical analysis and empirical results, demonstrating improved performance over traditional methods.", "Weaknesses": "The method's reliance on time-dependent density ratio estimation might introduce complexity and potential instability in model training. Additionally, the paper could benefit from more extensive experimentation on a wider range of datasets to further validate the approach.", "Questions": "How does the introduction of time-dependent density ratios impact the computational efficiency and convergence time of the model training process?"}}
{"paper_id": "aG3EARrrd1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces an innovative model called Online Sparse Residual Tree (OSRT), which combines tree structures with radial basis functions to handle streaming data efficiently. It demonstrates improved prediction accuracy and efficiency over traditional methods on benchmark datasets.", "Weaknesses": "The paper lacks detailed experimental validation on a wider range of datasets to thoroughly assess its generalizability. The complexity of the OSRT model and its assumptions about tree structure integration may not be fully justified.", "Questions": "How does the OSRT model perform compared to other state-of-the-art methods on non-time series datasets? Can the model parameters be adapted to different types of data without substantial manual tuning?"}}
{"paper_id": "Ts95eXsPBc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces Spatially-Aware Transformers (SAT) that incorporate spatial information into episodic memory models, demonstrating improvements in spatial reasoning and memory efficiency across various tasks.", "Weaknesses": "The experiments mostly focus on small-scale environments, raising questions about the generalization to more complex scenarios. Additionally, while the concept of integrating spatial dimensions is novel, the paper does not thoroughly explore possible limitations or challenges of the proposed methods.", "Questions": "How do the proposed models perform in highly dynamic environments with frequently changing spatial configurations?"}}
{"paper_id": "LfhG5znxzR", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses the critical issue of neural network interpretability, which is highly relevant to the research community. The authors present a novel approach using sparse, discrete bottlenecks through codebook features, which significantly enhance the interpretability of neural networks. The method is well-motivated and effectively integrates vector quantization into neural network architectures.", "Weaknesses": "While the proposed method shows promise, the paper lacks comprehensive comparisons with alternative methods in terms of performance and interpretability. Additionally, the need for fine-tuning neural networks to introduce these bottlenecks could limit the method's applicability in some cases. The scalability of the proposed approach to very large networks and datasets is not thoroughly evaluated.", "Questions": "How does the performance of the neural networks with codebook features compare to those without, especially in complex datasets or tasks? Are there specific types of tasks or networks where this method may not be effective or applicable? How does the approach handle potential trade-offs between interpretability and network performance?"}}
{"paper_id": "bUGzjiUsIq", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The problem of partially annotated multi-label classification is well-motivated and relevant to real-world applications. The proposed PAPG framework offers a novel way to handle incomplete annotations by efficiently combining reinforcement learning with supervised learning approaches. The experimental results across various datasets demonstrate the robustness and generalizability of the method, showing clear improvements over existing baselines.", "Weaknesses": "The method's reliance on reinforcement learning, which can be computationally intensive, may hinder its scalability to very large datasets. Additionally, while the paper presents comprehensive experimental results, it lacks an ablation study to disentangle the contributions of the different components of the PAPG framework.", "Questions": "How does the proposed method scale with increasing numbers of labels and instances? Is there room for further optimizations to reduce computational overhead?"}}
{"paper_id": "uMAujpVi9m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to overcoming data scarcity in protein-ligand interactions by using pseudo-complexes generated from protein segments and pretrained molecule encoders. It successfully improves tasks like druggability prediction and ligand binding affinity. This method shows great promise in enhancing biomedical applications through better pocket representation.", "Weaknesses": "The approach relies heavily on the quality of pseudo-complexes, which may not fully capture real-world protein-ligand interactions. Additionally, the method's dependency on pretrained small molecule encoders might limit flexibility.", "Questions": "How generalizable is ProFSA to different types of proteins or ligands? Are there any limitations in its application to certain classes of compounds?"}}
{"paper_id": "uqxBTcWRnj", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The framework provides a novel approach to bridging neural and symbolic representations effectively. It demonstrates significant improvements over existing methods in discovering compositional patterns without relying on predefined features. The introduction of new evaluation metrics that align with human interpretations adds value to the method's interpretability.", "Weaknesses": "The framework's applicability to real-world datasets is not extensively covered. The method's reliance on certain assumptions for symbolic feature learning may not generalize across diverse domains. Additionally, the integration and efficiency of the model could be further improved.", "Questions": "How does the framework scale with more complex, real-world datasets? What are the limitations of the proposed evaluation metrics? Can the TDL framework be integrated with existing neural architectures seamlessly?"}}
{"paper_id": "o4CLLlIaaH", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel point-based approach to generalizing neural radiance fields, which addresses common issues like occlusions and view inconsistencies in existing methods. The method shows improved results on standard benchmarks and enhances scene interaction capabilities.", "Weaknesses": "The paper may not address all edge cases or be fully robust across diverse datasets. It is unclear how the method scales with highly complex scenes or different types of occlusions not covered in the current datasets.", "Questions": "How does the proposed method perform on dynamic scenes with moving objects? Can the approach be extended to handle such scenarios, or are there inherent limitations?"}}
{"paper_id": "KNzL1nglNB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The authors propose a novel approach, Label-encoding Risk Minimization (LRM), which extends the capabilities of Empirical Risk Minimization for scenarios with insufficient labeled data. The paper presents a theoretical analysis linking neural class-mean collapse and ERM, which is supported by empirical evaluations across multiple scenarios.", "Weaknesses": "The presentation could benefit from clarity and additional details, especially regarding the experimental setup and more extensive comparisons with existing methods. The contribution could be deemed incremental as it primarily builds on existing concepts rather than introducing an entirely new paradigm.", "Questions": "Could the authors provide more details on how LRM handles complex datasets with significant class imbalance? How does the method scale with larger datasets or in real-time applications?"}}
{"paper_id": "pvhyBB86Bt", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a fundamental issue in the deep learning domain, extending the understanding of Jacobian stability from initialization to the whole training process. The use of random matrix theory to analyze MLPs throughout training offers a novel perspective.", "Weaknesses": "The paper relies heavily on theoretical assumptions that might be too strong without supplementary theoretical justification. The purported theoretical contributions could benefit from more rigorous empirical validation. Additionally, the examples provided might be overly simplistic, not covering more complex network scenarios.", "Questions": "Could the authors explore more complex network architectures or real-world scenarios to enhance the applicability of their findings? Also, how could this work be extended to encompass other neural network types, such as convolutional neural networks?"}}
{"paper_id": "Mylk5iamJC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "Proposes a unified approach using generative models for both CSR and OSR, potentially simplifying the handling of unknown classes. Demonstrates superior performance to existing methods on benchmark tasks.", "Weaknesses": "The novelty might be challenged due to similarities with existing work in using Gaussian Mixture Models for recognition tasks. Limited discussion on the distinction and comparison with prior work reduces clarity on the contribution and its impact.", "Questions": "How does the proposed method specifically compare with previous generative approaches for OSR in terms of computational complexity and scalability? Are there any limitations observed in specific domains or types of data?"}}
{"paper_id": "MsOcVFzv8D", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 5, "Strengths": "The paper develops a novel method for multi-domain text classification that is theoretically justified, addressing a significant gap in the field. It applies margin discrepancy and adversarial training to achieve state-of-the-art results, and its efficacy is empirically validated across multiple datasets.", "Weaknesses": "While the papers method outperforms existing approaches, the direct application of existing theoretical frameworks might limit the perceived novelty of the contribution. The experimental settings and comparisons, though thorough, could be expanded to more diverse domains to strengthen the claims.", "Questions": "How does the choice of domains in the experimental evaluation affect the generalizability of the method? Would the approach still perform well in less related domains with more significant discrepancies?"}}
{"paper_id": "fBlHaSGKNg", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a significant gap in semi-supervised learning by focusing on efficient sample selection for annotation under budget constraints. It introduces a novel criterion, -Maximum Mean Discrepancy (-MMD), and an innovative sample selection methodology, Unleashing the Power of Annotation (UPA), which improves SSL performance in constrained environments. Experimental results demonstrate the effectiveness of UPA across various SSL frameworks and datasets.", "Weaknesses": "The paper lacks sufficient details on the implementation and theoretical justification for the -MMD criterion and the UPA framework. The experimental validation could be extended to more diverse datasets and compared against a broader range of existing active learning techniques. The paper could improve on clarity regarding the methodology and the specific setup of experiments, such as the choice of hyperparameters and their impact.", "Questions": "How does the -MMD criterion specifically contribute to the improved performance of SSL compared to other diversity-focused metrics? What factors influenced the choice of datasets, and could the method be applied to real-world datasets beyond CIFAR and SVHN? Is there any specific rationale behind the choice of Gaussian kernel and CLIP extracted features?"}}
{"paper_id": "HwQ8NVvdmm", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel method, Hadamard Domain Quantized Training (HDQT), which effectively addresses the challenges of quantized continual learning. The approach shows that it is possible to maintain accuracy close to full precision models while significantly reducing resource consumption, making it highly suitable for edge applications. The integration of Hadamard transforms and stochastic rounding is innovative and well-justified, offering a clear advancement in the field. The experimental results clearly demonstrate the efficacy of the proposed method across different datasets.", "Weaknesses": "The paper could benefit from a more comprehensive evaluation on a wider variety of neural network architectures and real-world applications beyond CIFAR100 and HAR datasets. Additionally, details regarding the implementation complexity, especially concerning the integration of Hadamard transforms in existing hardware or software frameworks, are not sufficiently explored. More insights into potential limitations or challenges of deploying HDQT in practical settings would be valuable.", "Questions": "Can the authors provide more details on how the Hadamard transform and stochastic rounding can be integrated into existing deep learning frameworks? What are the potential challenges in deploying HDQT in real-world edge applications, and how might they be addressed?"}}
{"paper_id": "cZo6pDtDZr", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces innovative LDP algorithms for collision probability estimation, significantly improving sample efficiency and enabling sequential testing. The algorithms are rigorously evaluated, demonstrating near-optimal sample complexity and adaptability.", "Weaknesses": "The focus on collision probability against heavy hitters is not entirely contextualized, leaving questions about its relative ease or broader significance unresolved.", "Questions": "How does the introduced approach compare in terms of sample complexity with known results for locally private heavy hitters? Can the problem of collision probability estimation be shown to be unequivocally easier or more applicable than those?"}}
{"paper_id": "F7QnIKlC1N", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper proposes a novel self-attention mechanism that enhances the ability to predict 3D ground-state conformations from 2D molecular graphs. \n2. There is a clear and well-motivated gap identified in existing methodologies, which the paper aims to address.\n3. Demonstrates improvement over state-of-the-art models on benchmark datasets.", "Weaknesses": "1. The novelty of the model components may be questioned as similar methods might already exist in literature.\n2. Results, while improved, may not sufficiently justify the complexity of the model, especially in comparison with simpler models.\n3. The explanation for choosing certain components of the model, such as MoleBERT and Laplacian Positional Encoding, could be more detailed.", "Questions": "1. How does the model size compare to baseline models, and does this contribute to improvement in results?\n2. Can the MSRSA mechanism be applied to other domains or tasks beyond molecular conformation prediction?\n3. What is the computational efficiency of the proposed method compared to other methods in terms of time and resources?"}}
{"paper_id": "8JCn0kmS8W", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach integrating LLMs in audio storytelling, achieving state-of-the-art results on benchmarks. It demonstrates potential in creating coherent audio from text descriptions. The compositional and interpretable framework without additional training is commendable.", "Weaknesses": "The paper has limited methodological innovation, relying on existing models combined in a novel way rather than introducing new algorithms. Potential limitations include efficiency and artificial composition. There is also a lack of system-level ablation studies.", "Questions": "How does the system's performance vary with different LLMs? Are there plans to address efficiency and artificial composition limitations in future work?"}}
{"paper_id": "XbLffB0T2z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. Addressing a relevant security issue in machine learning with increased attack transferability across models.\n2. Innovative use of contrastive learning paradigms in poisoning attacks.\n3. Significant improvement in decreasing test accuracy, over 27% reduction.", "Weaknesses": "1. Limited novelty as the method primarily combines existing algorithms.\n2. Potential reproducibility issues regarding evaluation methods, especially the application of data augmentations in contrastive learning.\n3. Unclear why high-frequency perturbations ensure better transferability across learning paradigms. Existing work descriptions might be inaccurately represented.", "Questions": "1. How do high-frequency perturbations specifically contribute to improved transferability?\n2. How generalizable are the findings to larger datasets beyond those tested?\n3. Could a more detailed comparison with existing methods improve understanding of the contributions?"}}
{"paper_id": "4y3GDTFv70", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel hypothesis that links the emergent abilities of large language models (LLMs) to Bayesian inference, providing a potential theoretical framework for understanding these phenomena. The use of synthetic data for validation is a strength, as it provides a way to test and illustrate the proposed theory.", "Weaknesses": "The paper lacks a comprehensive empirical validation with real-world data, which could strengthen the claims. The connection between the theoretical model and the observed emergent abilities in actual LLMs remains somewhat abstract and could benefit from more explicit demonstrations or examples. Some technical aspects may be challenging for readers without a strong background in Bayesian inference and probability theory.", "Questions": "How does the proposed theory account for discrepancies in emergent behaviors observed across different LLM architectures or training datasets? Could more empirical evidence from real-world LLMs be provided to support the proposed theoretical framework?"}}
{"paper_id": "ODSgo2m8aE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper effectively identifies a gap in the current research related to the biases in GNNs and proposes a novel method to address this gap. The use of Lipschitz bounds specifically for GNNs seems to be innovative and potentially impactful.", "Weaknesses": "The paper might lack clarity in presenting complex mathematical formulations in an accessible manner to a broad audience. There may also be a need for further empirical validation with more diverse datasets.", "Questions": "How does the computation of Lipschitz bounds specifically scale with very large graphs, and are there any performance bottlenecks noted in JacoLip under such conditions?"}}
{"paper_id": "RlfD5cE1ep", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a significant gap in the understanding of non-contrastive learning by incorporating feature normalization into theoretical frameworks, which aligns well with practical settings. The use of high-dimensional limit dynamics and eigenmode analysis adds a robust mathematical foundation. The numerical simulations appear to effectively verify the theoretical claims.", "Weaknesses": "The theoretical model's reliance on assumptions specific to synthetic data may limit the immediate applicability to more complex and real-world datasets beyond CIFAR-10. Additionally, the paper lacks a comprehensive exploration of how its findings integrate with or extend upon existing analyses that have similarly considered feature normalization.", "Questions": "Can the authors provide insight into how the derived sixth-order dynamics compare with empirical observations across a broader range of datasets beyond CIFAR-10? Also, how do these findings influence the choice of loss functions in the design of new self-supervised learning systems?"}}
{"paper_id": "18TfucMNTr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel approach to tackle the non-convex optimization challenges in training deep neural networks using Gaussian continuation with a regularization term. The proposed method shows promising results in enhancing training stability and convergence speed.", "Weaknesses": "The numerical experiments, particularly on the MNIST dataset, seem to have issues with baseline comparisons. The baseline model accuracy reported is lower than expected, possibly indicating suboptimal implementation or configuration. This raises questions about the effectiveness of the proposed method over a more robust baseline.", "Questions": "How does the proposed Gaussian continuation method interact with traditional deep learning optimizers like SGD or Adam, particularly with their inherent noise and annealing effects? Is there a theoretical explanation or ablation study for the choice of the regularization term in the continuation optimizer?"}}
{"paper_id": "JGTC6WgO4T", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a significant gap in the existing literature regarding multi-source domain adaptation, providing both a theoretical foundation and empirical evidence of the effectiveness of their method. The proposed approach efficiently refines pseudo labels which aids in better domain adaptation.", "Weaknesses": "The method's complexity might pose challenges for practical implementation. Additionally, while empirical results are promising, more diverse datasets could further validate the framework's effectiveness.", "Questions": "Can the framework be extended to handle scenarios where only limited source domains are available with low correlation to the target domain?"}}
{"paper_id": "fQHb1uZzl7", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel and effective approach by unifying feature and cost aggregation using a Transformer-based architecture. The method has shown significant improvements on various benchmarks for dense matching tasks. The use of self- and cross-attention mechanisms allows for robust correspondence estimation.", "Weaknesses": "The paper could benefit from more discussion on potential limitations and failure cases. It is also not entirely clear if all compared methods in the experiments are trained under the same conditions, and more details on performance in specific use cases like optical flow on KITTI or Sintel are needed.", "Questions": "What are the limitations of the proposed method in scenarios with small overlapping ratios between images? How does the method perform on challenging image pairs from datasets like MegaDepth?"}}
{"paper_id": "YjG29CIOP7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper makes significant strides in advancing DFML by addressing the vulnerabilities of task-distribution shift and task-distribution corruption. The proposed methods, TEAPOT and ROSY, provide robust solutions to these issues. The work is also backed by extensive experimental validation across multiple datasets.", "Weaknesses": "The complexity introduced by the novel solutions might be seen as a potential downside. The dependence on a proactive model selection policy and reinforcement learning adds layers of complexity that may limit the broader applicability of the solution. Additionally, while the experiments are comprehensive, future evaluations might benefit from broader and more diverse benchmarks to further support the applicability of the approach.", "Questions": "How does the computational overhead introduced by ROSY compare to the potential time savings in model selection and increased performance in real-world scenarios? Additionally, how well does TEAPOT's memory-based strategy scale with larger, more complex tasks?"}}
{"paper_id": "DmDpu3r8iU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses the important topic of value understanding in large language models, which is pertinent to ensuring AI alignment with human values. The development of the Value Understanding Measurement (VUM) framework is a novel contribution that offers a structured approach to evaluate both 'know what' and 'know why' capabilities of LLMs.", "Weaknesses": "The methodology could benefit from more clarity and illustrative examples to help readers understand the application of the VUM framework and the discriminator-critique gap (DCG) approach. Additionally, the analysis might not fully account for variations in model training data and configurations which could influence value understanding.", "Questions": "How does the VUM framework account for differences in the training data and configurations of the LLMs being evaluated? Could future work include a more detailed breakdown of the models' training influences on value understanding?"}}
{"paper_id": "kXHEBK9uAY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "Combining hierarchical planning with diffusion models improves efficiency and generalization in long-horizon tasks when compared with flat diffusion-based models.\n\nThe hierarchical approach models subgoals, which improves the sampling efficiency and enhances the model's capability to generalize to unseen task combinations.\n\nEmpirical results show superior performance over existing methods, hinting at a strong application potential.", "Weaknesses": "The methodological novelty is somewhat limited, as it builds upon the existing diffuser approach by integrating hierarchical planning. While effective, the leap in innovation might not be deemed very groundbreaking.\n\nThere could be potential unexplored issues with enforcing the sequence of subgoals to ensure coherence, especially when low-level plans lack feedback.\n\nThe paper may benefit from additional ablations justifying the hierarchical modeling's benefits over state-only diffusion methods.", "Questions": "How is coherence between subgoals and low-level plans maintained when the system lacks feedback loops?\n\nAre there any specific tasks or domains where this hierarchical diffusion approach might notably struggle compared to other methods?\n\nHave alternative methods to subgoal definition been considered, such as learned instead of predefined segment splits?"}}
{"paper_id": "i7LCsDMcZ4", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 2, "Rating": 6, "Confidence": 3, "Strengths": "- The derivation of the CAM and saliency map of SNNs itself is a clear contribution. The results in Table 1 prove the correctness of this\n- EventRPG is able to consistently improve performance across event datasets\n- The time cost of EventRPG is comparable to similar augmentations in conventional vision", "Weaknesses": "My main concern is regarding the experiments:\n- The paper motivates the need for event data augmentation with the statement that \"the lack of huge event-based datasets similar to Imagenet prevents us from improving the model performance on relatively small datasets using a Pretrain-Finetune paradigm\". However, there is an event camera version of ImageNet available [1], and its paper shows that pre-training on N-ImageNet can greatly improve the accuracy on other datasets via transfer learning. Therefore, this statement in the Introduction is wrong\n- Since N-ImageNet is available, I would like to see results on this dataset. This is similar to conventional vision research on data augmentation, where ImageNet is the best testbed. If the authors are not able to train on N-ImageNet, the mini subset can be considered, though I do not think that is a comprehensive benchmark\n- The authors compare EventRPG with conventional vision methods such as Grad-CAM in Table 1. Thus, I wonder if it is possible to compare with Puzzle Mix and SaliencyMix in Table 3?\n- NDA applies the method to unsupervised contrastive learning and shows promising results. Is it possible to conduct such experiments using EventRPG?\n\n[1] Kim, Junho, et al. \"N-ImageNet: Towards robust, fine-grained object recognition with event cameras.\" ICCV. 2021.", "Questions": "See Weaknesses"}}
{"paper_id": "jHdz0CIS2y", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper combines entangled speech representations from self-supervised learning and speaker verification to improve zero-shot voice conversion. It introduces a novel training strategy with self-synthesized examples for iterative improvement, demonstrated through comprehensive experiments.", "Weaknesses": "The approach builds on existing techniques, such as using SSL features and speaker embeddings. The marginal improvement from self-synthesized examples questions the impact of the novel training strategy. Further comparative analyses could enhance understanding.", "Questions": "What unique aspects of the framework, beyond using SSL features, contribute to scalability? How does this method differ from other SSL-based models? Can the authors provide more evidence on the generalizability of the self-synthesized data augmentation?"}}
{"paper_id": "NkYCuGM7E2", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents an innovative approach by integrating Large Language Models (LLMs) for high-level decision-making in autonomous driving, which could potentially enhance interpretability and adaptiveness. The cognitive pathway framework with a chain-of-thought process is a novel application of LLMs in this domain. The presented experiments cover both single and multi-vehicle scenarios, providing a comprehensive evaluation of the system's capabilities.", "Weaknesses": "The paper does not adequately address the potential limitations of using LLMs in terms of processing speed and real-time decision-making, which are crucial in autonomous driving. The trustworthiness and consistency of LLM outputs, especially given their inherent randomness, are not thoroughly discussed. The experiments, while comprehensive, are somewhat limited in demonstrating the method's ability to handle a wide range of rare and complex driving situations.", "Questions": "How does the system ensure real-time performance given the computational demands of LLMs? Are there any specific constraints or measures implemented to handle the randomness and potential inconsistencies in LLM-generated decisions? Could you provide more details on how the interpretability of LLM decisions is evaluated? Are there any benchmarks or metrics used to assess this aspect?"}}
{"paper_id": "RtDok9eS3s", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper successfully demonstrates a significant simplification of transformer blocks, achieving faster training with fewer parameters while maintaining performance. It addresses a critical gap in the field by providing a practical solution to make transformers more resource-efficient. The methods are based on solid theoretical backgrounds, and the experiments are well-designed.", "Weaknesses": "The study is limited to specific datasets and model sizes, raising questions about the generalizability of the results to larger models or different tasks. The complexity reduction might not apply to other architectural variations or complex use cases without further modification.", "Questions": "How do the proposed simplifications affect the scalability of transformers to larger models and more complex datasets? Could there be potential drawbacks in terms of model generalization capabilities with the removal of certain components?"}}
{"paper_id": "YCPDFfmkFr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a novel approach to handling infeasible QP layers in neural networks, which is crucial for tasks in control and robotics where feasibility is not always guaranteed. The introduction of the Extended Conservative Jacobian framework shows a strong theoretical contribution that enhances the model's expressiveness and predictive performance.", "Weaknesses": "The method's reliance on primal-dual augmented Lagrangian techniques might introduce additional computational overhead, which the paper does not thoroughly address. Additionally, the presentation could be improved to make the complex mathematical concepts more accessible to a broader audience.", "Questions": "How does the computational complexity of the proposed method compare with existing approaches, especially in real-world applications? Are there specific tasks where the method performs significantly better or worse?"}}
{"paper_id": "ueTdErd5Ib", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a significant gap in contextual stochastic optimization by integrating robustness into the decision process. It provides theoretical guarantees on regret and demonstrates improved worst-case performance through experiments.", "Weaknesses": "The method may rely on assumptions about discretization that limit its general applicability. The presentation could be clearer regarding how the discretization specifically impacts performance and applicability to various problem domains.", "Questions": "How does the method scale with the complexity and dimensionality of the feasible region, and are there specific scenarios where this approach may not perform as expected?"}}
{"paper_id": "sDmjlpphdB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed Mixture-of-Prompts framework effectively divides the prompt space into more manageable sub-spaces that significantly enhance performance across NLP tasks compared to existing methods. The approach of assigning demo clusters using semantic similarity and complementing them with tailored instructions is innovative.", "Weaknesses": "There are concerns about the representativeness and effectiveness of the demo clustering, including how clusters cover the diversity of tasks and how clustering impacts the overall system performance. The simplicity of incorporating independent and joint searches might overlook the need for more heterogeneous or diverse instructions, which could be crucial for real-world application.", "Questions": "How does the choice of clustering method impact the overall system performance? Are the clusters dynamic and adaptive to new data or tasks?"}}
{"paper_id": "Mdk7YP52V3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses a critical gap in understanding the theoretical underpinnings of overparameterized heteroskedastic models using a novel approach.\n2. It introduces a nonparametric free energy framework which provides valuable insights into model behavior under different regularization settings.\n3. Empirical validations support the proposed theoretical framework and suggest practical applications in model regularization tuning.", "Weaknesses": "1. The theoretical models and derived equations lack comprehensive empirical tests across diverse scenarios beyond the specificity of neural networks, limiting their generalizability.\n2. The clarity in presenting connections between statistical physics techniques and neural networks could be improved to aid comprehension for a broader audience.\n3. The paper does not fully explore alternative explanations or hypotheses for model pathologies, potentially oversimplifying a complex issue.", "Questions": "1. Can the proposed framework be easily adapted or generalized to other types of probabilistic models beyond the conditional Gaussian likelihood?\n2. How does the proposed regularization strategy compare in performance with state-of-the-art regularization methods across different real-world datasets and model architectures?\n3. Are there any practical guidelines or heuristics derived from the study that can assist practitioners in regularization tuning for overparameterized models?"}}
{"paper_id": "YGTSLDAPqb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses the important problem of domain adaptation in machine learning, particularly in scenarios with limited labeled data in target domains. It proposes a novel two-phase framework that improves out-of-distribution performance, supported by superior results across several benchmarks. The study includes various datasets and tasks, enhancing the robustness of the findings.", "Weaknesses": "The study assumes that targeted augmentations significantly improve OOD performance, but it lacks a thorough exploration of why self-supervised pretraining doesn't always enhance OOD performance. It could benefit from a more in-depth analysis of the core problem and broader testing on varied datasets to generalize findings. The absence of a baseline comparing targeted versus general augmentations during fine-tuning also weakens the contribution.", "Questions": "How do targeted augmentations specifically improve the alignment between source and target domains compared to general augmentations? Could the framework's performance improvement in the astronomical datasets be attributed to regularization benefits rather than addressing domain shifts?"}}
{"paper_id": "JTcaziw7G1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents an innovative approach to privacy-preserving information retrieval in distributed settings using PRAG. The methodology effectively combines secret sharing with MPC to ensure secure queries and has been proven both efficient and effective through comprehensive experiments on synthetic and real data.", "Weaknesses": "The computational cost remains a constraint, suggesting that while PRAG is effective, further optimization might be needed for widespread practicality. Additionally, the complexity of the method might affect its ease of implementation and adoption.", "Questions": "How can the proposed method be optimized further to reduce computational costs? Are there any trade-offs in retrieval accuracy when optimizing for performance or scalability?"}}
{"paper_id": "lEkFq4RUCX", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The proposed DDF metric is conceptually sound and addresses key limitations of existing metrics like EMD and CD. It offers improved efficiency and accuracy in 3D point cloud processing across various tasks. The extensive experiments and results support its effectiveness.", "Weaknesses": "The paper lacks detailed explanation on the generation of reference points and the impact of their selection on the overall performance of the DDF metric. Some concepts may require further clarity for readers less familiar with the subject.", "Questions": "How does the choice of reference points affect the robustness and accuracy of the DDF metric? Are there any limitations in scenarios with highly sparse or irregularly sampled point clouds?"}}
{"paper_id": "Fq8tKtjACC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper introduces a novel approach to improving large language model performance by utilizing high-quality data, showing that smaller models with quality data can achieve state-of-the-art results. The proposed phi-1 model outperforms larger models in code generation tasks, demonstrating the effectiveness of the methodology. The use of a classifier for data filtering and synthetic data generation to boost diversity and instructiveness are noteworthy innovations.", "Weaknesses": "The specialization of the model to Python and the sensitivity to prompt variations are noted limitations. Another potential weakness is the reliance on synthetic data and GPT-3.5 generated tokens, which may not always reflect real-world or naturally occurring data distributions.", "Questions": "How does the model's performance vary with different programming languages other than Python? Are there plans to generalize beyond code generation tasks?"}}
{"paper_id": "8fQlGQkj0S", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel theoretical framework that clearly distinguishes between task learning and retrieval modes in in-context learning. It uses a rigorous mathematical model to derive insights that are empirically validated. The study addresses an important gap in understanding how pretraining distributions affect in-context learning.", "Weaknesses": "The assumptions made, such as the use of Gaussian mixtures, might limit the generalizability of the findings. The paper could benefit from a more comprehensive discussion of related work and a clearer explanation of how its approach improves upon or differs significantly from existing models. Some aspects of the theoretical framework could be better explained for broader accessibility.", "Questions": "How does the proposed framework generalize to non-Gaussian or more complex distributions? Can the findings be extended to other types of models or tasks beyond what was tested?"}}
{"paper_id": "hRos9WldRK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses the important challenge of label noise in deep learning and proposes a novel meta-learning based method that dynamically adjusts weights to improve robustness without the need for a clean validation set. The method is validated on multiple datasets and shows promising improvements over existing approaches, indicating strong empirical results. The integration with existing techniques highlights the method's versatility.", "Weaknesses": "The reliance on a validation set for feedback may still impose constraints, and the performance on certain real-world noisy datasets beyond the tested ones remains unclear. Assumptions about the meta-learning mechanism and its adaptability across diverse noise distributions could have been better addressed. Additional real-world cases could strengthen the evidence of generalization.", "Questions": "How well does the method scale with larger, more diverse datasets with varying degrees of noise? Are there specific noise patterns or distributions where the proposed method struggles? Can this approach be effectively integrated into industrial applications where datasets are extremely large and noisy?"}}
{"paper_id": "ttRSBZiCiu", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The WaveFluid model efficiently synthesizes high-fidelity audio with competitive performance compared to diffusion and GAN-based vocoders, requiring only one inference step and reducing computational overhead.", "Weaknesses": "The paper lacks an extensive evaluation of the proposed model across various datasets and conditions. The scalability and generalization of the model to other audio synthesis tasks are not thoroughly explored.", "Questions": "How does the model perform on different speech synthesis tasks beyond the ones discussed, and what are the limitations regarding the types of audio it can generate?"}}
{"paper_id": "WYsLU5TEEo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed framework successfully integrates interpretability and robustness in neural networks. It leverages GANs for generating counterfactual instances, which not only illuminate the decision-making process (interpretability) but also enhance the classifier's resilience against adversarial examples. This dual achievement addresses a significant gap in existing literature where such attributes are often treated separately.", "Weaknesses": "The paper primarily evaluates binary classification tasks, limiting its immediate applicability to broader, more complex multi-class scenarios. It also lacks detailed clarity in the presentation of experiments, potentially hindering reproducibility. Additionally, the discussion around model evaluation metrics can be enhanced to provide more insight into the robustness of results.", "Questions": "What specific steps can be taken to extend the proposed approach to multi-class classification problems? Can the framework be integrated with other forms of adversarial training to further enhance robustness?"}}
{"paper_id": "KKZaj2QS3G", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces an interesting combination of noise-resilient sampling and a scalable encoder for time series data. The application of hierarchical triplet loss and the integration of dilated convolutions with Inception blocks are innovative approaches that enhance the learning of time series representations.", "Weaknesses": "The paper does not adequately compare the proposed method with existing methods regarding noise handling in time series. There is insufficient exploration of the limitations of the method and potential situations where it might not perform well. Additionally, some aspects of the methodology and the experimental setup are not described in enough detail, which may affect reproducibility.", "Questions": "How does the CoInception framework perform under varying levels of noise in time series data? Are there any specific domains or types of time series data where the approach might not be effective? Could the authors provide more detailed ablation studies to isolate the impact of different components of their method?"}}
{"paper_id": "LWDRiFzbHQ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces an innovative method, Vicinal Risk Proxy, which enhances the existing model accuracy assessments by incorporating the model responses of neighboring test samples. This approach mitigates the issue of spurious model responses to individual test samples. The methodology is well-supported by experiments demonstrating its effectiveness across multiple datasets and test scenarios.", "Weaknesses": "While the paper provides a compelling new approach to address spurious responses in out-of-distribution generalization assessment, it could further investigate potential drawbacks of incorporating neighboring test samples, such as the computational overhead or the risk of introducing bias from nearest neighbor selection.", "Questions": "How does the computational complexity of calculating the Vicinal Risk Proxy compare to existing risk proxy calculations? Additionally, could there be cases where the selection of neighboring samples might inadvertently introduce bias into the risk proxy measurements?"}}
{"paper_id": "Yp01vcQSNl", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a significant gap in graph transformers by explicitly tackling edge directionality in directed graphs, which is often overlooked. The proposed DiGT model introduces innovative techniques such as dual encodings and a dynamic attention mechanism that enhances performance on directed graph tasks. The model demonstrates superior results compared to state-of-the-art models on datasets where directionality plays a crucial role.", "Weaknesses": "While the method is promising, the paper's presentation could be improved for clarity, particularly in the explanation of the dynamic attention mechanism and the dual encoding process. Additionally, the reliance on specific datasets that emphasize directionality may limit the generalizability of the results.", "Questions": "How does DiGT handle large-scale graphs computationally, and what are the trade-offs in terms of efficiency when compared to traditional graph transformers? Are there specific applications or domains where DiGT's approach is particularly advantageous?"}}
{"paper_id": "LndMyiBl3n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. Novel approach to assess and guide adversarial attacks using a modified silhouette score in graph neural networks. 2. Effective treatment across both homophilic and heterophilic graphs without needing detailed prior knowledge. 3. Efficient and scalable algorithm suitable for various graph settings, enhancing applicability in real-world scenarios.", "Weaknesses": "1. Dependence on heuristic choices for hyper-parameters could limit performance robustness and may lack theoretical justification. 2. While the modified silhouette score intuitively extends applicability across graph types, detailed empirical analysis of its sensitivity is lacking. 3. Presentation and clarity, specifically details surrounding methodological explanations, benefit from further polish.", "Questions": "1. How does the method handle varying levels of adversarial attack intensityare there thresholds where performance degrades? 2. What are the impacts on performance when applied to actual real-world graph datasets distinct from those tested? 3. Would SheAttack retain competitive effectiveness when applied to graphs with extreme heterophily levels?"}}
