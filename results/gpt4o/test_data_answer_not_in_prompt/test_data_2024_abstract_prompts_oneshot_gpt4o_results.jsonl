{"paper_id": "B0OwtVEejJ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel method by combining weight-entangled spaces with gradient-based methods, which addresses a notable gap in the NAS community. \n2. The proposed scheme effectively leverages the advantages of both paradigms, enhancing performance and memory efficiency.\n3. The open accessibility of the code promotes transparency and reproducibility.", "Weaknesses": "1. The presentation could be improved to make the integration of the two approaches clearer to readers who might not be familiar with both paradigms.\n2. The complexity arising from entangled weight-sharing might not be fully addressed, requiring further exploration of potential drawbacks.", "Questions": "1. How does the proposed method scale with larger and more complex search spaces?\n2. What are the potential limitations in terms of scalability and implementation in practical scenarios?"}}
{"paper_id": "ZlEtXIxl3q", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel application of contrastive loss functions in the context of global epistasis models, offering a promising approach to improve the efficiency of inferring fitness functions from biological sequences. The argument for an uncertainty principle in fitness-epistasis provides a strong theoretical basis for the use of contrastive losses over traditional MSE. The validation on benchmark tasks demonstrates the practical utility of the method for protein engineering applications.", "Weaknesses": "While the paper proposes an innovative approach, the presentation could be clearer, particularly in explaining the technical details and underlying assumptions of the method. More comprehensive comparisons with existing methods would bolster the claim of improved performance. Additionally, further empirical evidence would strengthen the argument for the broad applicability of the method.", "Questions": "How does the proposed contrastive loss approach perform across a wider range of biological datasets beyond the benchmarks used? Are there any limitations of the contrastive loss technique that need to be considered when applying it to more complex or diverse fitness landscapes?"}}
{"paper_id": "6yJuDK1DsK", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed method, FEATHER, introduces a significant advancement in lifelong test-time adaptation by enabling efficient adaptation with a minimal increase in parameter complexity. This is particularly useful for edge devices where computational resources are limited. The approach effectively decouples the source and target knowledge, reducing error accumulation over time, and operates without the need for source data at test time, which broadens its applicability. The empirical results on multiple benchmarks show its superiority compared to existing state-of-the-art methods, ensuring both efficiency and performance.", "Weaknesses": "While the approach is innovative, the details on how the lightweight adapters perform in various scenarios could be further elaborated. Additionally, the robustness of FEATHER might be heavily reliant on the specifics of how the adapters are designed and trained, which requires further exploration and documentation. There is also a lack of theoretical analysis on the conditions under which FEATHER guarantees robustness against error accumulation.", "Questions": "How does FEATHER manage potential catastrophic forgetting while disentangling source and target domain knowledge? Are there specific scenarios or types of domain shifts for which FEATHER is less effective? Can the approach be extended to handle other types of non-stationary domains beyond image classification?"}}
{"paper_id": "lt6xKGGWov", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The approach leverages neural estimation of mutual information, allowing the method to consider an ensemble of features rather than individual features in isolation, potentially capturing more complex interactions.", "Weaknesses": "The paper lacks detailed empirical evidence in the provided abstract, and it's unclear how this approach compares to existing methods on various benchmarks. Moreover, novel aspects beyond the use of mutual information and ensemble evaluation are not clearly articulated.", "Questions": "How does this method perform relative to existing feature selection techniques across diverse datasets? Are there limitations in terms of scalability or computation when considering large feature sets?"}}
{"paper_id": "Bvrc6kobWd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper provides a novel perspective by introducing gradient analysis to understand the impact of margins in contrastive learning, which has not been extensively explored before. The experimental results demonstrate improvements in generalization performance by emphasizing positive samples and scaling gradients based on specific criteria.", "Weaknesses": "The paper may lack in-depth theoretical justification for why margins have not been previously adopted in contrastive learning despite similar underlying mechanisms. The improvement margins may be considered marginal, and the explanations, while new, may not radically change existing understandings of the method.", "Questions": "How does the introduction of gradient-based analysis compare to existing decision-boundary-based explanations in terms of improving contrastive learning? What are the potential trade-offs or limitations introduced by emphasizing positive samples and scaling gradients according to the defined metrics? Can the proposed methods be generalized beyond the scope of the datasets tested?"}}
{"paper_id": "FJjHQS2DyE", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel method called conditional adversarial support alignment (CASA), which effectively handles label distribution shifts in unsupervised domain adaptation tasks.\n2. It provides a theoretical target risk bound, offering a strong justification for their approach and explaining why it outperforms marginal support alignment methods.\n3. Extensive empirical results demonstrate that CASA consistently outperforms state-of-the-art methods across various UDA benchmarks.", "Weaknesses": "1. While the theoretical underpinnings of the proposed method are well-explained, the practical engineering aspects and the complexity of implementing CASA could be further clarified.\n2. The paper could include more detailed analysis on the computational overhead and scalability of the proposed method, especially when applied to large datasets.", "Questions": "1. How does the proposed CASA method perform when the domain shifts are present in only certain feature dimensions? \n2. Can the authors provide more insight into how the proposed theoretical target risk bound influences the optimization procedure in practical scenarios?"}}
{"paper_id": "xelrLobW0n", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces TRACE, a comprehensive benchmark for evaluating continual learning in large language models, which addresses the oversight in current benchmarks. The introduction of the Reasoning-augmented Continual Learning (RCL) approach offers a novel solution to mitigate catastrophic forgetting while maintaining efficiency.", "Weaknesses": "The paper lacks a detailed analysis of the potential long-term impacts on general task performance of LLMs beyond the initial findings. There might also be challenges in scalability and applicability of the RCL approach to other forms of LLMs not covered by TRACE.", "Questions": "How does the TRACE benchmark compare with existing benchmarks in terms of scalability across various LLM architectures? Are there plans to extend RCL to encompass additional types of tasks or domain-specific challenges beyond what TRACE covers?"}}
{"paper_id": "bYQkOPvgDw", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses an under-explored area of robust GNNs under label noise, proposing a novel probabilistic graphical model framework, PRGNN.\n2. It introduces versions PRGNN-v1 and PRGNN-v2 to tackle the tasks effectively, showing robustness across noise types and heterophilies.\n3. The evaluations are extensive, and the implementation is available publicly, promoting reproducibility.", "Weaknesses": "1. The reliance on the Bayesian network approach might limit the model's flexibility and applicability to diverse graph structures and large datasets.\n2. The paper might not sufficiently address potential computational costs associated with the probabilistic approach, especially at scale.", "Questions": "1. How does the model performance scale with extremely large graphs in practical scenarios? Are there any computational constraints?\n2. Can the methods developed be adapted to slightly different problem domains, such as dynamic graphs or temporal label noise issues?"}}
{"paper_id": "sKPzAXoylB", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach, Utility-based Perturbed Gradient Descent (UPGD), that simultaneously addresses both catastrophic forgetting and loss of plasticity in continual learning. The method is well-justified and demonstrated through both challenging streaming learning setups and extended reinforcement learning experiments, showing significant improvements over existing methods.", "Weaknesses": "The paper might lack a detailed theoretical analysis of the UPGD mechanism which could enrich understanding of its workings and limitations. Additionally, the presentation could be improved by providing more detailed visualizations or examples of typical learning scenarios.", "Questions": "How does UPGD manage the trade-off between protecting useful units and rejuvenating unused ones in different problem domains? Also, how sensitive is UPGD to the choice of perturbation parameters?"}}
{"paper_id": "H8Qg1IIMaR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses an important and timely issue concerning the robustness of large language and vision-language models.\n2. It highlights the empirical observation of permutation sensitivity in multiple-choice question answering, which is relevant for stakeholders relying on these models.\n3. The problem identified is significant as it exposes a fundamental vulnerability in current state-of-the-art models.", "Weaknesses": "1. The paper might lack depth in providing solutions or mitigations for the identified vulnerability of permutation sensitivity.\n2. It is unclear how broadly applicable the results are across different domains and contexts beyond multiple-choice question answering.", "Questions": "1. Are there any proposed methods within the study to counteract or mitigate the permutation sensitivity observed?\n2. How does the permutation sensitivity affect the practical use of these models in real-world applications?\n3. What are the potential implications for model design to reduce such sensitivity in the future?"}}
{"paper_id": "tth2qXY7RU", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel quantization method, Super Floating-Point (SuFP), that significantly improves memory and computational efficiency without sacrificing model accuracy.\n2. SuFP demonstrates excellent adaptability to diverse data distributions through its scalable bias, which only requires a single addition operation.\n3. The proposed method achieves impressive experimental results, surpassing state-of-the-art quantization techniques in both computational capability and energy efficiency.", "Weaknesses": "1. The paper could provide more detailed information on the implementation of SuFP, especially in scenarios involving hardware constraints or specific data distribution challenges.\n2. Potential limitations of the SuFP method in scenarios not covered in the experimental benchmarks need more elaboration.", "Questions": "1. How does SuFP compare to existing quantization techniques in terms of implementation complexity and ease of integration into existing pipelines?\n2. Does the SuFP method encounter any scalability issues when applied to larger or more complex models?\n3. Can SuFP be adapted for use with non-standard neural network architectures or edge devices with limited computational resources?"}}
{"paper_id": "fjf3YenThE", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a significant limitation in the current SZOHT algorithm by introducing a generalized variance reduced ZO hard-thresholding algorithm. The theoretical analysis demonstrating improved convergence rates and broader applicability is a strong point. The practical utility of the method is showcased through applications in portfolio optimization and black-box adversarial attacks.", "Weaknesses": "The presentation could be improved for clarity, particularly in terms of how variance reduction specifically resolves the inherent conflicts in ZO settings, which might be complex for readers unfamiliar with the topic.", "Questions": "How does the proposed variance reduction technique compare in terms of computational efficiency with traditional techniques? Are there specific scenarios or types of data where this method particularly excels or struggles?"}}
{"paper_id": "H9DYMIpz9c", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces an innovative approach to data distillation for auto-regressive tasks that maintains or improves model performance with substantially less data. \n2. The empirical results demonstrate impressive performance retention using only 0.1% of the original data, which has significant implications for model efficiency and scalability.\n3. The method leverages novel techniques such as reverse-mode differentiation of the Adam optimizer and latent-space factorization, showcasing technical depth.", "Weaknesses": "1. The approach's dependency on specific techniques like Hessian-Vector Products might limit generalizability to all model types or tasks. \n2. The paper does not elaborate on potential limitations or failure modes of Farzi when not achieving the claimed performance benchmarks.", "Questions": "1. How does Farzi perform with tasks or models where the auto-regressive assumption is not strictly met? \n2. Are there specific characteristics of the datasets used in experiments where Farzi shows more significant performance improvements compared to others?"}}
{"paper_id": "4kLVvIh8cp", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel algorithm, PNLSVI, which integrates advanced techniques to tackle offline RL with non-linear function approximation, offering substantial innovations such as the variance-based weighted regression scheme and a pessimistic value iteration approach.\n2. The algorithm provides a tight regret bound and achieves minimax optimal instance-dependent regret for linear function approximation, extending instance-dependent results to non-linear function classes.\n3. The proposed work is the first to achieve statistical optimality in the realm of nonlinear offline RL, showcasing a significant advancement in the field.", "Weaknesses": "The abstract does not provide clear insight into how the performance of the proposed algorithm compares empirically against existing methods on standardized benchmarks, which could be crucial to evaluate the practical effectiveness and efficiency in real-world scenarios.", "Questions": "1. How does the proposed PNLSVI algorithm perform in empirical evaluations compared to existing state-of-the-art algorithms on standard offline RL benchmarks?\n2. What are the computational costs associated with the variance-based weighted regression scheme and variance estimation subroutine?\n3. Could the methodology of PNLSVI be extended to handle other types of RL challenges, such as those involving continuous action spaces or multi-agent systems?"}}
{"paper_id": "CSpWgKo0ID", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses an important area of research by exploring the behavior of large language models in social interactions through the lens of behavioral game theory. The study reveals insightful findings about LLM behavior in cooperative and coordinative settings, adding depth to our understanding of AI in interactive scenarios.", "Weaknesses": "The study may have limitations in generalizability, as it focuses primarily on specific game scenarios and a single LLM (GPT-4). The shortcomings in coordination games might reflect the constraints of current LLMs rather than a fundamental inability to perform well in such settings.", "Questions": "How robust are the findings across different LLMs beyond GPT-4? Could the strategies be influenced by specific training data or architectural biases? Could experiments in multi-player settings provide further insights beyond two-player games?"}}
{"paper_id": "gwbQ2YwLhD", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper tackles a fundamental issue in the field of structure learning by analyzing the influence of variable scale on the performance of structure learning algorithms. 2. It provides theoretical insights and conditions under which square-based losses can lead to incorrect DAGs, which is significant for the design of more robust algorithms. 3. The paper includes extensive experimental validation on both synthetic and real-world data, which supports the theoretical findings.", "Weaknesses": "1. The paper appears to focus primarily on scale issues without proposing new methods or solutions to mitigate these problems, which limits its practical impact. 2. The presentation of theoretical results lacks clarity in places, which could make it difficult for readers to grasp the full implications of the findings. 3. The experiments, while extensive, might not cover all potential edge cases or real-world scenarios where these findings could be applied.", "Questions": "1. Did the authors consider any potential methods for correcting or mitigating the scale issues identified in their study?\n2. Are there specific applications or domains where the identified scale sensitivity is particularly problematic? If so, could this be addressed in future work?"}}
{"paper_id": "eIYDKNqXuV", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. Village-Net Clustering introduces a novel approach to clustering by combining K-Means and a network-based clustering method, allowing it to handle complex manifold data effectively.\n2. The algorithm efficiently determines the optimal number of clusters autonomously, which is a common challenge in unsupervised learning.\n3. The paper demonstrates strong empirical performance on real datasets, indicating its competitive edge over state-of-the-art methods.\n4. Its low time complexity and scalability make it suitable for large-scale datasets.", "Weaknesses": "1. The paper relies heavily on the Walk-likelihood Community Finder (WLCF), which may not be well-known or universally accepted in the community.\n2. Benchmarks are conducted only on datasets with known ground truths; it is unclear how well the method generalizes to more complex or unknown data distributions.\n3. There might be limited insight provided into the decision-making process of how villages are determined and parameter sensitivity of the algorithm.", "Questions": "1. How would the performance of Village-Net Clustering compare on datasets without clear ground-truth clusters?\n2. Are there any limitations or specific data characteristics where this method might not perform well?\n3. Can the effectiveness of Walk-likelihood Community Finder be quantified separately to justify its integration into the algorithm?"}}
{"paper_id": "bAMPOUF227", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel framework that effectively enhances the generalizability and factuality of Large Language Models (LLMs) using task-specific fine-tuned Language Models (SLMs).\n2. The proposed method minimizes hallucinations in generative tasks and generalizes out-of-distribution data, demonstrating significant improvements in performance. \n3. Comprehensive resources such as curated datasets, prompts, and model checkpoints are provided, adding significant value to the research community.", "Weaknesses": "1. The paper primarily focuses on the inference stage improvements; further details on how the proposed method integrates with the overall lifecycle of LLM deployment would strengthen the work.\n2. There is limited discussion on potential limitations or trade-offs associated with the reliance on discriminative models for enhancing LLMs.", "Questions": "1. How scalable is the proposed framework when applied to more diverse LLM architectures beyond Llama 2 and ChatGPT?\n2. Are there specific criteria or guidelines provided for selecting task-specific SLMs to maximize the benefits for a given LLM?"}}
{"paper_id": "eNoiRal5xi", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach, Unknown Domain Inconsistency Minimization (UDIM), which effectively merges SAM optimization and parameter perturbations to improve domain generalization. The method is theoretically grounded by establishing an upper bound for the true objective of DG tasks and demonstrates consistent empirical superiority over existing SAM variants across various datasets. The statistically significant improvements in challenging scenarios further affirm UDIM's robustness.", "Weaknesses": "The paper's reliance on creating synthetic perturbed domains may introduce variability in results depending on the nature of perturbations applied. The presentation, although adequate, could be enhanced with clearer visualizations and more detailed explanations of the complex theoretical validations.", "Questions": "How sensitive is UDIM to different types and degrees of perturbations applied to the source domain data? Are there specific scenarios where UDIM might not perform as expected or encounter limitations, particularly when highly novel unseen domain structures are introduced?"}}
{"paper_id": "uU0Adp7Sfo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces CCGAN, which improves upon traditional GANs by incorporating a collaborative mechanism alongside the competitive game structure. This hybrid approach allows for enhanced data generation capabilities, particularly in complex problems like multi-modal generation and image super resolution. The theoretical framework proposed is solid with a well-defined equilibrium point, contributing to more stable and high-quality generated samples.", "Weaknesses": "The collaborative-competitive approach, while promising, may add complexity to the GAN architecture, making the model harder to implement and potentially less efficient. It is also unclear if the new mechanism can be applied seamlessly to a wide range of other generative tasks without significant modifications. There might be concerns about the scalability of the approach as model complexity increases.", "Questions": "How does the collaborative element of CCGAN specifically improve upon or differ from existing methodologies in terms of computational efficiency and model robustness? Are there particular types of datasets or conditions where CCGAN does not perform well?"}}
{"paper_id": "dKju7tbe6D", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed IPRM mechanism is novel in combining iterative and parallel reasoning, providing a unique approach to enhancing reasoning capabilities and generalization in multi-step reasoning tasks.\n2. The method requires fewer parameters and computation steps, indicating efficiency in design.\n3. Achieves state-of-the-art zero-shot performance on challenging benchmarks, highlighting its effectiveness.\n4. The visual interpretability of IPRM computation adds to its diagnostic utility, making it more transparent.", "Weaknesses": "1. The abstract does not specify how the combination of iterative and parallel mechanisms is implemented, leaving some questions about the technical execution and integration.\n2. The contribution, while novel, may need further validation across a broader range of tasks beyond the mentioned benchmarks to establish general applicability.", "Questions": "1. How does IPRM specifically implement the parallel and iterative reasoning aspects, and how are conflicts between simultaneous operations managed?\n2. Are there any limitations observed in tasks outside the benchmarks provided, especially those that might not fit typical visual question answering scenarios?"}}
{"paper_id": "DL7JWbdGr3", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel application of self-supervised learning to pre-train epidemic time-series models, effectively addressing the challenges of leveraging past epidemic data for better predictive performance. The framework demonstrates significant improvements over state-of-the-art methods, showing versatility across various datasets with different seasonal patterns and geographic distributions.", "Weaknesses": "The paper may face challenges in real-world applicability due to variations in data quality and availability across different regions and time periods. Furthermore, the complexity of designing self-supervised tasks for diverse disease dynamics might require further exploration to streamline adoption.", "Questions": "How does the pre-training approach handle varying data quality and completeness across different epidemic datasets? Are there specific diseases or data scenarios where the pre-trained models do not perform well, and if so, what are the common factors in those cases?"}}
{"paper_id": "1PaDPHDhwe", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel class-specific scaling strategy and instance-wise adaptive scaling technique that can enhance both average and robust accuracies in group distributionally robust optimization without the need for additional training.\n2. An innovative unified metric is introduced to evaluate the trade-off between average and robust accuracies, offering a more comprehensive assessment of existing debiasing algorithms.\n3. The approach is validated through extensive experiments in both computer vision and natural language processing domains, demonstrating its effectiveness.", "Weaknesses": "1. The paper could provide more in-depth theoretical analysis of the proposed scaling strategies to further validate their robustness and generalizability.\n2. The assumption that a na\"ive ERM baseline could outperform recent debiasing methods might require additional scrutiny or justification, as it can be surprising given established results in the literature.", "Questions": "1. How does the class-specific scaling strategy interact with dataset properties such as class imbalance, and are there mechanisms in place to prevent potential overfitting or underfitting?\n2. Does the proposed unified metric consider potential biases across different datasets and tasks within each domain, and can it be applied universally across any domain beyond computer vision and natural language processing?"}}
{"paper_id": "iI7hZSczxE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper addresses an important issue in the field of time series analysis by focusing on disentangled representations, which can lead to better generalization and interpretability.\n2. It introduces innovative techniques, such as the TDS metric and the l-variational inference layers with self-attention, specifically tailored for time series data.\n3. The method acknowledges the real-world attribute correlations, which is a vital aspect often overlooked.", "Weaknesses": "1. The abstract does not provide detailed empirical results or comparisons with existing methods, which makes it difficult to assess the practical impact of the proposed approach.\n2. Clarity might be improved in explaining the role of contrastive learning within the proposed model, as this aspect is complex and central to understanding its novelty.", "Questions": "1. How does the proposed TDS metric compare to existing metrics in terms of effectively evaluating disentanglement in time series?\n2. Can the method be applied to other types of time series data beyond electricity usage, and are there any domain-specific limitations?"}}
{"paper_id": "qrGjFJVl3m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. The introduction of the Qwen-VL series provides new models that excel across a wide range of visual-centric benchmarks and settings, setting new records for models of similar scale.\n2. The models are designed with detailed innovations such as a visual receptor and a multilingual multimodal corpus, enhancing the capacity of the base Qwen-LM model to include visual perception.\n3. Expansion of capabilities to include grounding and text-reading using image-caption-box alignment, and the instruction-tuned Qwen-VL-Chat demonstrates superiority in real-world dialog benchmarks.", "Weaknesses": "1. While the models demonstrate superior performance, the paper does not clearly detail any specific novel methodologies or algorithms that are fundamentally different from existing frameworks.\n2. Details on the training methods and the evaluation metrics used to validate the claims of benchmark superiority could be more comprehensive.", "Questions": "1. Can you provide more details on the specific implementation of the visual receptor and its impact on performance compared to existing models?\n2. How does the multilingual multimodal corpus specifically contribute to the grounding and text-reading capabilities of Qwen-VLs?\n3. What are the potential limitations or challenges foreseen in deploying Qwen-VL and Qwen-VL-Chat in various applications?"}}
{"paper_id": "qup9xD8mW4", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel concept of behaviour distillation for reinforcement learning, successfully extending dataset distillation methodology to RL. The proposed HaDES method can effectively condense essential information into a minimal set of examples, achieving high performance in continuous control tasks. The approach demonstrates robustness across various architectures and offers significant improvements in RL neuroevolution. Visualizations provide interpretable insights.", "Weaknesses": "While the paper demonstrates strong empirical results, the question of scalability to more complex or extensive tasks remains. Additionally, the assumption of achieving competitive performance with only four state-action pairs may not generalize to all RL tasks.", "Questions": "How does HaDES perform on more complex RL tasks beyond continuous control? Can the methodology be extended or adapted for environments with significantly larger state and action spaces?"}}
{"paper_id": "Zw8YxUWL4R", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The introduction of the Extended Textual Conditioning space (P+) and Extended Textual Inversion (XTI) provide significant advancements in controlling and personalizing text-to-image diffusion models. 2. The paper demonstrates compelling empirical evidence of XTI's superior performance in terms of expressiveness, precision, and convergence speed compared to previous methods. 3. The work has clear applications in improving the personalization and customization of generative models, specifically in text-to-image generation tasks.", "Weaknesses": "1. While the proposed methods show impressive results, the paper could provide more insights into the limitations and possible failure cases of XTI in handling complex scenes or highly abstract concepts. 2. The impact of the extended textual conditioning space on compute resources and training efficiency was not discussed in detail.", "Questions": "1. How does the introduction of per-layer textual conditioning affect the computational overhead and does it require significant changes to the existing pipeline of text-to-image diffusion models? 2. Could the proposed P+ space and XTI approaches be applied to other types of diffusion models or are they tailored specifically for text-to-image models? 3. Are there any specific challenges you anticipate when scaling this approach to handle more diverse datasets or real-world scenarios?"}}
{"paper_id": "3NXhwkZGjz", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The proposed method provides a novel approach to addressing the SFUDA task by using multiple prediction hypotheses, which helps in reducing the impact of inaccurate model predictions. The three-step adaptation process is well-structured and demonstrates state-of-the-art performance in experimental results. The approach can be easily integrated into existing methods, potentially improving their results as well.", "Weaknesses": "The methodology relies heavily on the effectiveness of hypothesis consolidation, which may not always identify the most accurate pseudo-labels. The paper could benefit from a deeper analysis of scenarios where the proposed approach might fail or underperform.", "Questions": "How sensitive is the method to the choice of initial model predictions, and how does it impact subsequent hypothesis consolidation steps? Are there specific domains or types of data where the three-step adaptation process is less effective?"}}
{"paper_id": "Fn655mJ4bv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper presents a novel interpretation technique for structured output models, which is an important area in the field of machine learning.\n2. The method takes into account correlations among output variables, potentially leading to improved explanation quality.\n3. The proposed technique is validated with both simulated and real datasets, showcasing its effectiveness.", "Weaknesses": "1. The abstract does not provide detailed information on the specific datasets used or the quantitative improvements achieved by the method, making it difficult to assess the novelty and impact fully.\n2. The explanation of the energy-based training process could be made clearer, as the abstract is somewhat vague on the mechanics of the method.\n3. It is not evident how the method compares to existing interpretation techniques in terms of computational efficiency or interpretability.", "Questions": "1. How does the proposed method compare to other state-of-the-art interpretation techniques in terms of computational efficiency and clarity of interpretation?\n2. Can the energy-based training process be generalized to other types of models or is it specific to structured output models?\n3. What are the limitations of the proposed approach, especially in terms of scalability or applicability to different domains?"}}
{"paper_id": "VyMW4YZfw7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper challenges the complexity of current GNN architectures, proposing that simpler, traditional methods can achieve comparable performance, which is an interesting perspective.\n2. Provides ablative studies on hyperparameters, which is useful for understanding the influence of these parameters on the performance of spectral GNN techniques.", "Weaknesses": "1. The novelty of applying traditional nonparametric estimation techniques in the spectral domain might be limited, as the concept is an adaptation rather than a novel algorithmic innovation.\n2. The claim regarding shifts in evaluation conventions affecting performance improvements requires further evidence and detailed analysis.", "Questions": "1. How do the performance results of the simpler methods compare quantitatively against leading GNN architectures on SSNC benchmarks?\n2. What specific changes in evaluation conventions were identified and how do these specifically affect the performance outcomes reported in recent GNN studies?"}}
{"paper_id": "09xFexjhqE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel approach to fine-tuning for adversarial robustness, addressing key challenges in current methodologies such as gradient direction divergence and hyperparameter sensitivity. The proposed AutoLoRa mechanism simplifies the process by disentangling the optimization paths and automating hyperparameter tuning, which has practical benefits for deployment.", "Weaknesses": "The dependence on LoRa branches might limit the applicability of the approach to models that can accommodate such architectural modifications. The paper does not discuss potential limitations or scenarios where AutoLoRa might not perform well.", "Questions": "How does AutoLoRa handle different types of adversarial attacks beyond those evaluated? Are there any tasks or model architectures where this approach does not yield significant improvements?"}}
{"paper_id": "nmBjBZoySX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a critical limitation in the scalability and flexibility of current Graph Lottery Ticket (GLT) methods, which is an important step forward in making GNNs feasible for large-scale and deep network applications.\n2. The introduction of AdaGLT is compelling due to its adaptive, dynamic, and automated nature, allowing for layer-specific adaptations and eliminating the need for extensive manual tuning, which can significantly improve the efficiency of GNNs.\n3. The theoretical underpinning provided for overcoming oversmoothing and achieving improved sparse structures adds significance and credibility to the proposed method.", "Weaknesses": "1. While the abstract claims the method is extensively validated across multiple scenarios, the lack of detail leaves questions about the extent and generality of the experimental validations.\n2. The paper might benefit from further clarity on how AdaGLT specifically compares with traditional methods beyond just performance metrics, such as in terms of computational costs or training time.", "Questions": "1. How does AdaGLT adapt the topology structure differently across layers compared to conventional GLT methods?\n2. Can you provide more insights into the computational overhead or resource requirements that come with utilizing AdaGLT?\n3. Are there specific types of graphs or datasets where AdaGLT performs sub-optimally compared to traditional methods?"}}
{"paper_id": "di52zR8xgf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents Stable Diffusion XL (SDXL), a significant improvement over previous versions of Stable Diffusion. The expansion of the UNet backbone and the introduction of novel conditioning schemes show innovation in model design. The training on multiple aspect ratios and the inclusion of a refinement model to enhance visual quality highlight a comprehensive approach to improving image synthesis quality. The results demonstrate competitive performance with current state-of-the-art systems.", "Weaknesses": "The paper might lack detailed comparative analysis against competitive methods in several diverse benchmarks beyond just achieving 'results competitive with state-of-the-art'. There might be limited exploration of the underlying reasons for performance improvements, particularly in novel conditioning schemes.", "Questions": "What specific datasets and metrics were used to evaluate SDXL's performance against other state-of-the-art models? How does SDXL perform across various domains or complexities of text prompts?"}}
{"paper_id": "0b328CMwn1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel concept of activation prompt (AP) which effectively extends visual prompting by applying universal perturbations to activation maps, showing significant improvements over traditional visual prompting.\n2. Extensive experiments are conducted across 29 datasets and various model architectures, demonstrating the superior performance of AP in terms of accuracy and efficiency.\n3. The theoretical rationale provided for the preferential layer depth in prompting for CNNs and ViTs adds significant value and insight into the mechanism behind the observed performance metrics.", "Weaknesses": "1. While the paper discusses the intrinsic limitations of input-level visual prompting, it would benefit from a deeper exploration of scenarios where visual prompting might still be advantageous over activation prompting.\n2. The paper could offer more analysis on how the introduction of activation prompts affects generalization and transferability across different domains and tasks.", "Questions": "1. Can the activation prompt approach be generalized to other forms of models beyond CNNs and ViTs, such as those used in other domains like natural language processing?\n2. How does the choice of activation layers for prompting affect the stability and robustness of the model predictions under varying test conditions?"}}
{"paper_id": "AMDKqZcZbi", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces an innovative task inspired by biological systems that addresses a significant challenge in machine learning: balancing rapid learning with memory retention.\n2. The use of a content-addressable heteroassociative memory model based on biological structures is well-motivated and shows strong empirical results against ANN baselines.\n3. The proposed architecture demonstrates good generalization and addresses continual learning without forgetting, which is an important contribution to the field.", "Weaknesses": "1. While the biological inspiration is compelling, it may be challenging to apply the proposed methodology to domains outside of navigation tasks.\n2. The abstract provides limited detail on the specific implementation and evaluation metrics, making it difficult to fully assess the model's effectiveness and generalizability.", "Questions": "1. How does the proposed model handle environments that are significantly different from those it was trained on?\n2. Can the principles and structures from the model be adapted for non-spatial tasks where quick learning and memory retention are also critical?"}}
{"paper_id": "am7BPV3Cwo", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a critical issue of imbalance in OOD detection which is pertinent for real-world applications.\n2. ImOOD offers a generalized statistical framework for OOD detection in imbalanced data, providing both theoretical insights and practical solutions.\n3. The method demonstrates superior performance across multiple benchmarks compared to existing approaches, indicating strong empirical results.", "Weaknesses": "1. The paper might add complexity by introducing a new framework, which could be challenging for practitioners to adopt without thorough understanding.\n2. The abstract lacks clarity on how the proposed normalization and regularization techniques are specifically applied, requiring more detailed exposition.", "Questions": "1. How does the proposed ImOOD framework handle the dynamic nature of data distribution in live environments?\n2. Are there specific limitations or scenarios where ImOOD might not perform as well as stated, particularly in non-image domains?"}}
{"paper_id": "R1crLHQ4kf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a robust method for detecting adversarial attacks on ASR systems using straightforward yet effective statistical measures and classification techniques. Extensive evaluation demonstrates high performance with an AUROC greater than 99% for adversarial detection.", "Weaknesses": "The contribution may be limited by the reliance on known statistical features that could potentially be circumvented by more sophisticated adversarial techniques. Moreover, although the detection strategy shows impressive results, its adaptability to future or unseen types of adversarial attacks is not fully addressed.", "Questions": "How does the proposed method perform against more sophisticated adversarial attacks that specifically target the extraction of these statistical features? Could the approach be extended or adapted for use in real-time ASR system applications?"}}
{"paper_id": "i7P2mK3x3o", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel flow-based model for transporting between two arbitrary distributions, demonstrating versatility beyond normalizing flows. It effectively integrates optimal transport into neural networks for generative modeling tasks, offering benefits for downstream applications like density ratio estimation and image translation.", "Weaknesses": "The presentation is a bit dense and could benefit from clearer explanations of the technical methodologies for broader understanding. Some empirical results could be expanded or included more comprehensive comparisons to alternative methods.", "Questions": "How does the model handle cases where the distributions P and Q have significantly different supports or dimensionalities? Are there any limitations or assumptions in terms of the sample size required to reliably estimate the transport map?"}}
{"paper_id": "YIls9HEa52", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative extension to the recurrent switching linear dynamical system model by introducing a semi-Markov discrete state process with latent geometry, allowing for flexible state cardinality. The use of partial differential equations to derive a new formulation enhances the model's efficiency and applicability. Validation on both synthetic and real-world data highlights the model's capability in uncovering non-stationary processes in neural activity.", "Weaknesses": "The presentation of the model's application and results could be clearer to facilitate understanding of its performance and implications. There may be a steep learning curve for readers unfamiliar with PDE theory and its application in deriving dynamical sufficient statistics.", "Questions": "How does the performance of the irSLDS model compare to other recent models for analyzing trial-varying neural data? Can the model be adapted for use in different types of neural data beyond mice electrophysiological data?"}}
{"paper_id": "koYsgfEwCQ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces DynaVol, a novel 3D scene generative model that effectively combines geometric structures and object-centric learning within a differentiable volume rendering framework.\n2. The model's ability to perform object-centric voxelization and leverage both voxel features and global features through a compositional NeRF decoder is innovative and powerful.\n3. DynaVol significantly outperforms existing methods for unsupervised dynamic scene decomposition, demonstrating strong empirical results.", "Weaknesses": "1. The abstract does not provide specific quantitative results or comparisons with baselines, which could strengthen the claims about performance improvements.\n2. The technical details of the canonical-space deformation function and its integration into the framework are not fully described in the abstract, leaving some uncertainty.", "Questions": "1. How does the performance of DynaVol scale with increasing complexity in dynamic scenes?\n2. What are the computational requirements for training the DynaVol model, and how do they compare to other state-of-the-art methods?"}}
{"paper_id": "o6AN2ZoNw2", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents P-Seg, a simple but effective framework for open-vocabulary semantic segmentation that performs well across multiple datasets without fine-tuning. The method demonstrates state-of-the-art performance on benchmark datasets, and the approach's scalability and potential for improvement through self-training are valuable contributions.", "Weaknesses": "The abstract lacks detailed explanations of the frameworks and methodologies involved, offering only a broad overview of the approach. Without the code or demo currently available, reproducibility and detailed assessment are limited.", "Questions": "What specific challenges arise when using pseudo-masks and language in training, and how does P-Seg address them? How does P-Seg compare in terms of computational efficiency and resources needed against other state-of-the-art methods?"}}
{"paper_id": "HXZK1Z8tHa", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel Transformer architecture specifically designed for image restoration, addressing important issues of latency and trainability effectively. The proposed ShareFormer model presents an innovative approach by sharing attention maps among neighboring blocks, significantly improving inference speed while maintaining performance. The addition of residual connections to the 'Value' of self-attention aids in optimizing the network without additional convolution layers. The experimental results supporting the model's effectiveness and efficiency are convincing, and the authors' commitment to open-source code and pre-trained models is commendable.", "Weaknesses": "While the ShareFormer architecture is novel, the paper does not clearly demonstrate how it compares to existing methods across a diverse set of image restoration tasks. The focus on high-resolution image restoration, although justified, could benefit from more diverse application results. The explanation of how the local inductive bias is introduced through shallow transformer aggregation could be more detailed. It would also be beneficial to see a more detailed analysis of the trade-offs between latency reduction and network trainability.", "Questions": "How does the ShareFormer model perform on low-resolution image restoration tasks compared to high-resolution tasks? Can the approach of sharing attention maps be adapted to other applications beyond image restoration? What specific scenarios or types of noise does ShareFormer handle better than existing models?"}}
{"paper_id": "QO3yH7X8JJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel approach, Diff-SR, which leverages pre-trained diffusion-based generative models for arbitrary-scale super-resolution without requiring fine-tuning or additional training efforts. \n2. The proposed method is backed by a thorough theoretical analysis, introducing the Perceptual Recoverable Field (PRF) metric to balance noise injection effectively.\n3. Extensive experiments demonstrate the superior performance of the approach across various scenarios, indicating its flexibility and adaptability.", "Weaknesses": "1. While the paper outlines the effectiveness of Diff-SR, it may not fully address the limitations and potential scenarios where the method might not perform optimally, such as extreme scaling factors or particularly challenging image textures.\n2. The impact of computational resource savings (in terms of time and hardware) over state-of-the-art practices is not quantified or extensively discussed.", "Questions": "1. Can the approach of using pre-trained DGMs for ASSR be generalized to other domains or tasks outside of image super-resolution?\n2. How sensitive is the PRF metric to different types of images, and what are the potential limitations in its application?\n3. Are there specific characteristics or types of low-resolution images where the method performs considerably worse, and how might these be addressed?"}}
{"paper_id": "sNtDKdcI1f", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper identifies a key factor in RLHF improvements, namely the correlation between response length and reward, which adds a new perspective to understand the mechanism of RLHF. \n2. It investigates interventions to maintain improvements without increasing output length, which could lead to more efficient models.", "Weaknesses": "1. The interventions proposed to mitigate length increases are not uniformly effective across different settings, which may limit their applicability.\n2. The study's findings that length alone can drive significant improvements indicates that current reward models may not be adequately nuanced in evaluating helpfulness.", "Questions": "1. What specific interventions were the most successful in mitigating the increase in response length across various settings?\n2. Are there any implications for re-designing reward models to better capture true helpfulness rather than simply correlating longer outputs with better outcomes?"}}
{"paper_id": "HFtrXBfNru", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a theoretically sound approach to addressing representation distortion in evolving graphs, which is a persistent issue in graph neural networks.\n2. The Smart model appears to be an effective solution, showing strong performance in both synthetic and real-world datasets.\n3. The paper provides a refined lower bound on distortion, which adds valuable insights into the problem.", "Weaknesses": "1. While the method shows promise, the novelty of the approach could be questioned since adaptive feature extraction and self-supervision are established techniques.\n2. The paper could improve in conveying its results and methodologies more clearly, as some technical details might be challenging to understand.", "Questions": "1. How does the proposed Smart model perform compared to other standard baselines in GNNs addressing temporal changes?\n2. Are there scenarios or types of graphs where Smart may not perform well, and how might these limitations be addressed?"}}
{"paper_id": "4u0ruVk749", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces an innovative approach to modeling unobserved confounders using a diffusion model, which is novel in the context of estimating individualized treatment effects (ITE).\n2. The methodology is grounded in solid theoretical background by leveraging advances in latent variable modeling, making it a potentially important contribution to causal inference literature.\n3. The empirical results demonstrate consistent improvements over state-of-the-art methods, highlighting the effectiveness of the proposed approach.", "Weaknesses": "1. The paper may lack clarity in explaining the implementation details, particularly regarding how the reverse diffusion process is practically realized given the complex nature of such models.\n2. The assumption of having a priori knowledge for conditioning in the Markov chain might limit the applicability of the approach in certain practical scenarios where such knowledge is difficult to obtain.\n3. More extensive theoretical justifications or discussions regarding the limitations of the model in real-world scenarios with highly complex latent spaces might be needed.", "Questions": "1. How does the performance of the proposed model vary with different types or amounts of apriori knowledge? Are there specific scenarios where the model's assumptions strongly hold or fail?\n2. Could the approach be extended to deal with dynamic treatment regimes, and if so, how might it handle time-varying confounders?"}}
{"paper_id": "Pa6SiS66p0", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper introduces a novel benchmark for multi-modal continual learning, which is a valuable contribution to the field. \n2. The approach of leveraging multiple modalities to mitigate catastrophic forgetting is innovative and aligns well with biological learning processes. \n3. The study sets a strong baseline that could stimulate further research in the area of multimodal continual learning.", "Weaknesses": "1. The paper does not provide detailed comparative results with existing CL methods, making it difficult to gauge the improvement over state-of-the-art techniques. \n2. The integration and alignment method may require extensive computational resources, which could limit its practical application.", "Questions": "1. How does the proposed method perform on diverse datasets and in comparison with state-of-the-art continual learning techniques? \n2. Can the approach be effectively scaled to incorporate more than two modalities, and how would the computational complexity be managed?"}}
{"paper_id": "rkplYfqUr0", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces Gen-Z, a novel generative prompting framework that improves the robustness and calibration of language models for zero-shot text classification. It effectively utilizes natural language label descriptions and additional context, demonstrating consistent performance gains over existing baseline methods across various benchmarks. The approach's ability to integrate contextual information and personalize label interpretation enhances its practical applicability.", "Weaknesses": "While the approach is novel, the paper does not extensively explore the limitations or potential downsides of using natural language descriptions for labels, which may introduce new biases or depend heavily on the quality of these descriptions. The scalability of integrating complex context through label descriptions is another potential concern.", "Questions": "How does the performance of Gen-Z change with differing levels of detail and length in the label descriptions? Are there specific types of contextual information that significantly impact performance, and how can one systematically determine what additional context to include?"}}
{"paper_id": "pTU2X9mUBe", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The introduction of LaDe, the first publicly available large-scale dataset for last-mile delivery, fills a significant gap in the logistics and spatio-temporal data mining fields. 2. The dataset is comprehensive and diverse, providing rich information for a variety of research tasks and settings. 3. The paper verifies the utility of LaDe by demonstrating its application in several classical baseline models, further establishing its value for the research community.", "Weaknesses": "1. The dataset may have privacy implications or limitations in terms of data anonymization, which the paper does not discuss in detail. 2. The paper could provide more in-depth experimental comparisons or novel insights using the dataset rather than just baseline verifications.", "Questions": "1. What measures have been taken to ensure the privacy and security of the data in LaDe? 2. How are the unique characteristics of each city represented and maintained in the dataset to ensure unbiased research outputs?"}}
{"paper_id": "uhtQyRrTzY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper proposes a novel dataset-curation approach that does not rely on metadata, enabling the use of real-world videos for pretraining, which is a significant advancement in dense pixel-specific representation learning. \n2. The method demonstrates substantial improvement over existing datasets in multiple benchmarks, indicating the effectiveness and applicability of the curation process.\n3. The scalability of the approach to produce even larger datasets suggests potential for further advancements.", "Weaknesses": "1. While the paper addresses dataset curation, it may lack a detailed analysis of potential biases introduced by using unannotated real-world video data.\n2. There may be challenges in replicating the approach in different contexts or domains not covered in the experiments.", "Questions": "1. How does the approach ensure unbiased data representation when using unannotated real-world videos?\n2. Can the method be adapted easily to other domains requiring different types of data? \n3. Are there specific types of real-world videos that are required to maximize the effectiveness of the proposed curation method?"}}
{"paper_id": "WZfatbNdLV", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces TrASPr, an innovative predictive model that leverages transformer architectures to predict AS in a tissue-specific manner, achieving superior performance over existing models. It skillfully integrates Bayesian Optimization for designing RNA splicing outcomes, showcasing a novel application of generative models in the RNA field.", "Weaknesses": "The abstract does not provide sufficient detail on the dataset used or the extent of the performance improvement over existing models. Additionally, the application of a 'costume' loss function in the Bayesian Optimization process is not clearly explained, which could invite skepticism regarding its effectiveness.", "Questions": "What specific datasets were used to evaluate TrASPr's performance against existing models? Can the authors provide more detail on the 'costume' loss function and its role in the Bayesian Optimization process for RNA splicing design?"}}
{"paper_id": "YkRwadXWHd", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed multi-modal framework effectively combines video, keypoints, and optical flow, enhancing feature representation for continuous sign language recognition.\n2. The introduction of a hierarchical knowledge distillation framework addresses computational challenges, ensuring high performance with reduced costs.\n3. The paper demonstrates state-of-the-art performance on three benchmark datasets, indicating the robustness and efficacy of the approach. ", "Weaknesses": "1. The novelty of the contribution might be limited, as multi-modal approaches and knowledge distillation are well-explored methods in the field. More clarity on unique aspects compared to existing methods could enhance the contribution section.\n2. The paper could benefit from more detailed insight into the fusion techniques and their individual impact on performance, which would provide better understanding and potential for further improvement.", "Questions": "1. How does the use of hierarchical knowledge distillation compare specifically to other state-of-the-art efficiency optimization techniques in CSLR?\n2. Could the methods proposed in this paper be generalized to other sign languages or adapted for use in different language recognition tasks?"}}
{"paper_id": "q4Bim1dDzb", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed UniVoxel framework offers a significant improvement in optimization efficiency over traditional inverse rendering methods. By using a unified voxelization approach and spherical Gaussians for illumination, the method reduces training times significantly while maintaining high-quality scene reconstruction. The empirical results on multiple benchmarks further validate the effectiveness of the approach.", "Weaknesses": "While the method shows promising results, the integration of spherical Gaussians and their impact on different types of scenes could be explored further. The presentation could be improved for better clarity on complex topics like the representation of incident light radiance.", "Questions": "How does the method perform on scenes with highly dynamic lighting conditions? Are there limitations to using spherical Gaussians for illumination modeling in such scenarios?"}}
{"paper_id": "39cPKijBed", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses an important issue of dataset bias in diffusion models, which is increasingly relevant as these models advance.\n2. The proposed time-dependent importance reweighting method is innovative and demonstrated to be more precise than existing approaches.\n3. Strong theoretical foundations are provided, including a connection with traditional score-matching and a proof of convergence to an unbiased distribution.\n4. Comprehensive experimental validation showing improved performance over several baselines across different datasets.", "Weaknesses": "1. The intractability of directly applying the method to score-matching may limit its practical applicability until further optimizations are devised.\n2. While the method is shown to be effective against baseline techniques, the paper could have explored more diverse datasets or bias scenarios to further validate generalizability.", "Questions": "1. How does the method perform on larger, more complex datasets with multiple types of biases present simultaneously?\n2. Are there any identified scenarios in which time-dependent importance reweighting might introduce other unintended biases, or where its effectiveness might be diminished?"}}
{"paper_id": "aG3EARrrd1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper introduces a novel online learning model, OSRT, which efficiently handles streaming multivariate data. \n2. The model's adaptive nature in both depth expansion and RBF refinement helps maintain performance even as data evolves. \n3. Empirical evaluation on several datasets demonstrates OSRT's superior efficiency and accuracy compared to existing methods.", "Weaknesses": "1. The paper lacks detailed theoretical analysis on the convergence and stability of the proposed OSRT model. \n2. Limited information is provided on the specific datasets used and how they represent real-world scenarios. \n3. There is not enough discussion on potential limitations or cases where OSRT might not perform well.", "Questions": "1. How does OSRT handle categorical variables in the streaming data? Is any preprocessing required?\n2. What are the criteria for selecting the maximum network depth in the OSRT model?\n3. Have there been any tests on the model's robustness against concept drift or outlier data?"}}
{"paper_id": "Ts95eXsPBc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to incorporate spatial information into episodic memory models, which aligns with cognitive science perspectives. The use of Spatially-Aware Transformer models that consider both temporal and spatial dimensions is a novel idea that could significantly advance the field. Furthermore, the paper proposes an Adaptive Memory Allocator, a potentially impactful contribution to optimizing memory efficiency in AI.", "Weaknesses": "While the concept is promising, the practical implementation details and potential limitations of integrating spatial information using transformers are not extensively discussed. Additionally, it is unclear how generalizable the proposed methods are across different types of tasks and environmental settings.", "Questions": "1. How does the Spatially-Aware Transformer model specifically incorporate spatial dimensions, and what challenges arise from this integration?\n2. Can the Adaptive Memory Allocator be adapted for other memory-centric approaches outside of place-centric tasks?\n3. What types of environments or tasks may prove challenging for the proposed models, and how do the authors plan to address these challenges?"}}
{"paper_id": "LfhG5znxzR", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The approach introduces a novel method of improving neural network interpretability through codebook features, which are sparse and discrete. This is innovative and addresses a key challenge in understanding neural network behavior.\n2. The paper demonstrates the versatility of the method across different datasets, including finite state machine datasets and large Transformer models, showing practical applicability.\n3. The open-sourced codebase increases transparency and facilitates further research, allowing other researchers to verify and build upon the work.", "Weaknesses": "1. The paper might not fully explore the potential limitations and edge cases where the vector quantization bottleneck method may not perform well or could cause significant degradation in performance.\n2. The method of identifying and activating specific codes lacks detailed explanation on scalability and potential computational overheads when applied to much larger models or more diverse datasets.", "Questions": "1. How does the vector quantization bottleneck approach affect the training time and computational resource requirements compared to standard networks?\n2. Are there scenarios where activating certain codes could lead to undesirable or unexpected behaviors in the model, and how are these mitigated?"}}
{"paper_id": "bUGzjiUsIq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a relevant and challenging problem in the field of multi-label classification with partial annotations. The proposed PAPG framework effectively combines reinforcement learning and supervised learning to tackle issues of class imbalance and scarcity of positive annotations. The inclusion of local and global rewards and the iterative training strategy demonstrates an innovative approach to improving classification performance across diverse tasks.", "Weaknesses": "The paper provides limited details on the implementation specifics of the PAPG framework, which makes it difficult to assess the reproducibility of the results. Additionally, while the combination of reinforcement learning and supervised learning is well-justified, the novelty may not be as significant, as similar methodologies have been proposed in related fields. A deeper analysis of the potential limitations and scalability of the proposed method is needed.", "Questions": "How does the PAPG framework handle datasets with extremely high levels of class imbalance? Are there any specific assumptions made on the nature of the input data or the distribution of annotations? Additionally, how does the method compare to existing state-of-the-art techniques in terms of computational efficiency?"}}
{"paper_id": "uMAujpVi9m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach to pretraining pocket representations using high-resolution atomic protein structures, effectively expanding the data available for modeling interactions.\n2. ProFSA performs state-of-the-art across multiple important tasks, indicating significant advances and potential impact in the field.\n3. The paper addresses a fundamental bottleneck in protein-ligand interaction modeling by simulating ligand-receptor complexes on a large scale.", "Weaknesses": "1. The approach heavily relies on the quality of the small molecule representations, which may vary or lead to biases based on the specific pretrained encoders used.\n2. While the method is innovative, the scalability and computational cost of generating and training on over 5 million complex simulations might be an issue not fully discussed.", "Questions": "1. How well does the performance of ProFSA generalize to real-world cases beyond the created 5 million complexes?\n2. Are there specific scenarios or types of ligands where ProFSA doesn't perform as well, and if so, what might be the reasons?\n3. Is there potential for integrating this method with existing models that focus on ligand specificity or flexibility, and how might that impact predictive accuracy?"}}
{"paper_id": "uqxBTcWRnj", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The Transitional Dictionary Learning framework introduces a novel approach to learning symbolic knowledge by reconstructing compositions of parts with implicit relations, which is innovative in dealing with compositional visual data.\n2. The proposed metrics align well with human evaluations, indicating that they effectively reflect the model's performance in a human-meaningful way.\n3. The method significantly outperforms existing unsupervised part segmentation methods, demonstrating its effectiveness.", "Weaknesses": "1. The paper lacks detailed exposition in some areas, potentially hindering its understanding by readers unfamiliar with the specific methodologies like game-theoretic diffusion models.\n2. Although the experiments cover three datasets, additional validation on broader datasets could further solidify the claims of general applicability.", "Questions": "1. Can the proposed transitional dictionary learning framework be extended to more complex real-world datasets with high variability in parts and compositions?\n2. How do the clustering information gain and heuristic shape score metrics correlate quantitatively with human evaluation scores beyond qualitative consistency?"}}
{"paper_id": "o4CLLlIaaH", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel paradigm for generalizable neural radiance fields that addresses key issues in previous approaches, showcasing better geometries, view consistencies, and rendering quality. 2. The proposed Generalizable neural Point Field (GPF) offers a significant shift from image-based to point-based rendering, which is a noteworthy contribution. 3. The paper demonstrates significant performance improvements over state-of-the-art methods in various datasets and settings.", "Weaknesses": "1. The paper's technical exposition could be clearer in some sections, making it challenging for readers unfamiliar with the domain to fully grasp the proposed methodology. 2. While the results are promising, further discussion on the computational complexity and scalability of the proposed approach would enhance understanding.", "Questions": "1. Can the proposed GPF framework be extended or adapted for other applications beyond neural radiance fields? 2. How does the computational complexity of the Generalizable neural Point Field compare to traditional image-based methods in terms of resource requirements and efficiency?"}}
{"paper_id": "KNzL1nglNB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces the Label-encoding Risk Minimization (LRM) as a novel approach to handle label insufficient scenarios. The method is versatile, acting as a plugin for existing models, and shows theoretical and empirical improvements over ERM in various settings.", "Weaknesses": "The paper relies heavily on the assumption of neural collapse, which may not hold in all settings. The method's effectiveness outside the tested scenarios such as in fully supervised settings or with noisy labels is unclear.", "Questions": "How does LRM perform in scenarios where neural collapse is not observed? What are the limitations and conditions where LRM might not work as expected? Can LRM be integrated with other forms of semi-supervised methodologies?"}}
{"paper_id": "pvhyBB86Bt", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper provides a novel extension of the existing framework by analyzing Jacobian behavior beyond initialization, thus offering deeper insights into neural network training dynamics. \n2. The authors utilize recent advances in random matrix theory to derive a general theorem, which strengthens the theoretical foundation of the study.\n3. The applicability of the framework to various scenarios, including sparse networks and non-iid initialization, enhances its relevance to real-world applications.", "Weaknesses": "1. The presentation of the paper might be dense, potentially limiting its accessibility to researchers not specialized in random matrix theory or neural network initialization.\n2. While the theoretical contributions are significant, empirical validation beyond conventional scenarios could further substantiate the claims.", "Questions": "1. How does the theoretical framework handle very deep or wide networks in practice, where computational limitations might come into play?\n2. Can the results be extended to investigate the behavior of other gradients, such as those related to second-order optimization methods?\n3. Are there empirical results available to show how these initialization schemes affect training efficiency and final model performance in practical settings?"}}
{"paper_id": "Mylk5iamJC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper provides a clear formalization of open-set recognition that aligns with learning theory, offering a strong theoretical basis for the research.\n2. The proposed neural Latent Gaussian Mixture Model (L-GMM) and collaborative training algorithm present a novel approach that bridges the gap between closed-set and open-set recognition, showing superior performance without requiring modifications or prior knowledge.\n3. The experimental results demonstrate that L-GMM outperforms traditional discriminative models in both image recognition and segmentation tasks, highlighting the practical efficacy of the approach.", "Weaknesses": "1. The paper might have limited discussion on the computational efficiency of the proposed framework, which is an important aspect for practical deployment.\n2. While the approach is theoretically sound, the generalization to other domains or types of data beyond those tested has not been explored, which can limit the perceived impact.", "Questions": "1. How does the computational overhead of the L-GMM compare to traditional discriminative models, especially in large-scale applications?\n2. Can the proposed framework be extended to handle online or streaming data scenarios effectively?\n3. What are the limitations of L-GMM in terms of data types or scenarios where it might not perform as well?"}}
{"paper_id": "MsOcVFzv8D", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a significant gap in the multi-domain text classification literature by providing a theoretical foundation for MDTC algorithms, which is a major contribution. \n2. The establishment of a new generalization bound using Rademacher complexity and the formulation of a margin discrepancy metric are novel and add depth to the understanding of domain divergence in MDTC.\n3. The proposed MDAT method demonstrates superior performance in empirical evaluations, outperforming state-of-the-art baselines on established benchmarks.", "Weaknesses": "1. The presentation of the work could be improved to better communicate complex theoretical concepts for a broader audience, as the current exposition might be too dense for readers not deeply familiar with the specific mathematical tools employed.\n2. While theoretical analysis is provided, the scope of empirical validation could be expanded further, potentially covering more diverse datasets and settings.", "Questions": "1. Can the proposed MDAT approach be generalized to other types of tasks beyond text classification, and if so, how does its performance compare in different settings?\n2. How sensitive is the MDAT approach to variations in domain divergence, and what are the implications for task adaptability in high-divergence scenarios?"}}
{"paper_id": "fBlHaSGKNg", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "HwQ8NVvdmm", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel technique leveraging Hadamard transforms and tiled matrix multiplication to maintain acceptable accuracy in fully quantized training, demonstrating its effectiveness in reducing computational resources while supporting continual learning. The approach shows promising results on human activity recognition datasets and CIFAR100.", "Weaknesses": "The paper lacks a detailed analysis of the trade-offs involved regarding different quantization levels and their impact on computational efficiency and model accuracy. Furthermore, the evaluation, although effective, covers a limited number of datasets and scenarios, leaving questions about the general applicability of the method.", "Questions": "How well does the proposed technique generalize across other machine learning tasks beyond the datasets evaluated? Are there specific model architectures or learning scenarios where the proposed quantization approach might struggle?"}}
{"paper_id": "cZo6pDtDZr", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents a significant improvement in estimating collision probability under local differential privacy, reducing sample complexity by a factor of 1/alpha^2 compared to previous work.\n2. It introduces the first sequential testing algorithm for collision probability, which is a novel contribution to the field and addresses a previously unmet need.\n3. The experimental results show that the proposed algorithms require significantly fewer samples than existing methods, demonstrating practical effectiveness.", "Weaknesses": "1. The presentation of the algorithm's theoretical foundations may be complex and challenging to follow for readers not deeply familiar with the statistical methods involved.\n2. The abstract does not provide a detailed comparison with state-of-the-art methods or discuss potential limitations of the proposed approach.", "Questions": "1. How does the new algorithm perform in real-world scenarios compared to synthetic experiments?\n2. Are there specific types of discrete distributions or specific conditions under which the proposed algorithms might not perform as well as others?"}}
{"paper_id": "F7QnIKlC1N", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The introduction of GTMGC, which utilizes a Graph-Transformer for 3D molecular representation learning, is innovative and presents a significant advancement in predicting molecular conformation directly from 2D structures.\n2. The novel self-attention mechanism, MSRSA, improves model performance and is applicable to other molecular modeling tasks, indicating broader applicability.\n3. The results on the Molecule3D and QM9 datasets demonstrate the method's superior performance over state-of-the-art techniques, highlighting its effectiveness.", "Weaknesses": "1. While the method is promising and shows superior performance, the abstract does not detail the computational efficiency compared to traditional methods, which could be a critical factor for practical applications.\n2. The abstract lacks specific metrics or quantitative results from the evaluations, making it difficult to assess the degree of improvement over previous methods.", "Questions": "1. Can GTMGC be adapted for larger and more complex molecular systems beyond the datasets mentioned, such as proteins or larger compounds?\n2. How does the computational efficiency of GTMGC compare to traditional methods like DFT in terms of runtime and resource usage?"}}
{"paper_id": "8JCn0kmS8W", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. WavJourney introduces an innovative framework that integrates Large Language Models with audio generation systems to create rich storytelling audio from text descriptions. This approach is novel and expands the capabilities of audio generation beyond existing domain-specific applications.\n2. The method not only achieves state-of-the-art results on text-to-audio benchmarks but also introduces a new benchmark for multi-genre story audio, which can greatly influence future research.\n3. The framework allows for human-machine co-creation, enhancing the scope of interactive and creative audio content production.", "Weaknesses": "1. While promising, the reliance on LLMs means the success of the framework is heavily dependent on the quality and capabilities of the underlying language model prompts, which may vary.\n2. The complex integration of multiple models and the conversion of text to audio scripts may present practical challenges in performance consistency and reproducibility in different settings.\n3. Further details are needed on how the system's outputs are evaluated and the diversity of tasks it can handle.", "Questions": "1. How does WavJourney handle errors or ambiguities in the text input in terms of generating coherent and high-quality audio outputs?\n2. Are there limitations on the types of audio effects and complexities of the storytelling audio that WavJourney can generate?\n3. Can the framework be adapted to support real-time audio generation or be integrated into live applications?"}}
{"paper_id": "XbLffB0T2z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses an important issue of transferability in data poisoning attacks, which is crucial for real-world applicability. The introduction of Transferable Poisoning, which leverages different learning paradigms, is a notable advancement that could influence future research in adversarial robustness.", "Weaknesses": "The assumption that an attacker could accurately predict the victim's training paradigm is quite strong and may limit the practical applicability of the proposed method. Furthermore, while the experiments are extensive, the generalizability to more diverse datasets and scenarios is unclear.", "Questions": "How effective is the proposed method when applied to models trained with ensembles of algorithms or other advanced machine learning techniques? Are there specific types of models or datasets for which Transferable Poisoning is especially effective or ineffective?"}}
{"paper_id": "4y3GDTFv70", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper provides a novel perspective on the emergent abilities of large language models (LLMs) by framing them as consequences of Bayesian inference on a sparse joint distribution. This theoretical approach offers a meaningful explanation for LLM capabilities such as in-context learning and instruction fine-tuning.", "Weaknesses": "The paper's reliance on theoretical constructs may limit its practical applicability. Additionally, the quantitative results presented might not fully capture the complex interactions in real-world language data.", "Questions": "How does the concept of -ambiguous languages affect the practical deployment and performance of LLMs in diverse linguistic contexts? Could the study account for how this sparsity might evolve as more languages and data inputs are introduced?"}}
{"paper_id": "ODSgo2m8aE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses a significant issue of fairness in Graph Neural Networks by introducing a theoretically sound Lipschitz bound formulation. 2. The experimental validation supports the theoretical claims, showing the practical effectiveness of the approach to mitigate bias in GNNs.", "Weaknesses": "1. While the paper introduces a novel framework, the explanation might be too theoretical without enough emphasis on practical implementation details. 2. The presentation could benefit from clearer descriptions and more intuitive explanations of complex concepts to reach a broader audience.", "Questions": "1. How does the formulated Lipschitz bound compare to other fairness techniques in terms of computational overhead? 2. Can this approach be generalized to other types of neural networks, and if so, how?"}}
{"paper_id": "RlfD5cE1ep", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper extends existing analysis by considering the cosine loss with feature normalization, which provides new insights into the dynamics of non-contrastive learning frameworks.\n2. It offers a theoretical understanding of how feature normalization prevents collapse in learned representations, which is a valuable addition to the field.", "Weaknesses": "1. The paper may lack empirical validation of the proposed theoretical insights, which could limit its applicability to real-world scenarios.\n2. The presentation of the sixth-order dynamics might be complex and could be challenging for readers to digest without adequate visualization or examples.", "Questions": "1. How does the proposed sixth-order dynamics compare empirically with the third-order dynamics regarding real-world performance in non-contrastive learning models?\n2. Are there scenarios where the sixth-order dynamics might not offer any advantage, or could induce instability, particularly in large-scale data contexts?"}}
{"paper_id": "18TfucMNTr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel perspective by framing noisy training in deep neural networks as a specific case of optimization by continuation. This approach is demonstrated to effectively reduce the risk of being trapped in local minima, particularly in very deep architectures with non-convex loss functions.", "Weaknesses": "While the approach is innovative, the paper may lack comprehensive theoretical analysis or comparisons with existing methodologies for overcoming local minima in neural network training. Additionally, it is unclear how well the proposed method generalizes to a wide range of architectures and applications.", "Questions": "How does the proposed continuation optimization with regularization compare to state-of-the-art methods in terms of computational efficiency and scalability? Are there specific types of architectures or tasks where this approach is particularly beneficial or less effective?"}}
{"paper_id": "JGTC6WgO4T", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses a significant gap in Unsupervised Domain Adaptation by introducing a method that doesn't rely on access to source data, which is crucial for privacy and security. 2. The proposed approach for multi-source Black-Box Domain Adaptation is novel and shows competitive results compared to state-of-the-art methods. 3. The inclusion of theoretical support strengthens the validity of the proposed method.", "Weaknesses": "1. The presentation could be improved for better clarity, especially regarding the explanation of the Pseudo label Refinery Network and its integration into the overall framework. 2. The experimental results, while promising, were not discussed in detail, leaving readers to infer the significance of certain benchmarks.", "Questions": "1. How scalable is the proposed MSBDA framework with LPR to real-world scenarios where more complex domain shifts and larger datasets are involved? 2. Could the proposed method be adapted to handle more than two modalities, and if so, how?"}}
{"paper_id": "fQHb1uZzl7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper provides a novel Transformer-based architecture that effectively combines feature and cost aggregation for dense matching tasks, demonstrating a unification approach with strong potential benefits.\n2. The methodology employs self- and cross-attention mechanisms and provides a coarse-to-fine design that enhances the accuracy of correspondence estimation.\n3. The approach shows significant improvements on standard benchmarks for semantic matching and is validated also in geometric matching contexts.", "Weaknesses": "1. While the integration of feature and cost aggregation is well-executed, the abstract does not provide detailed insights into potential limitations or computational overheads associated with the proposed approach.\n2. The contribution in terms of novel theoretical insights or intuitive guidelines on the optimal use of the combined aggregation processes might be less pronounced narrowly focusing on empirical performance.", "Questions": "1. Does the proposed architecture introduce any specific computational efficiency challenges during training or inference compared to existing methods?\n2. How does the multi-scale prediction component adapt to different types of datasets or tasks other than the benchmarks mentioned?"}}
{"paper_id": "YjG29CIOP7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper addresses an important issue in data-free meta-learning by introducing novel concepts such as Task-Distribution Shift and Task-Distribution Corruption, and proposes the TEAPOT and ROSY frameworks to tackle these challenges. The proposed solutions are innovative and the extensive experiments demonstrate their effectiveness in improving robustness and handling TDS and TDC.", "Weaknesses": "The presentation could be improved to more clearly convey complex ideas and methodologies, as some sections may be difficult to follow for those not deeply familiar with the subject. Additionally, while the experiments are extensive, more details on the implementation and evaluation metrics would enhance clarity and reproducibility.", "Questions": "How do TEAPOT and ROSY compare with the latest state-of-the-art DFML methods in terms of computation overhead? Also, how sensitive is the performance of ROSY + TEAPOT to the choice of hyperparameters in practice?"}}
{"paper_id": "DmDpu3r8iU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel framework, Value Understanding Measurement (VUM), which examines both 'know what' and 'know why' aspects of value understanding in Large Language Models (LLMs). The use of the Schwartz Value Survey and the development of a dialogue dataset with GPT-4 are significant contributions to the evaluation of value understanding in LLMs.", "Weaknesses": "The complexity of evaluating intrinsic value comprehension in LLMs is only partially addressed, and while the framework is innovative, it might not fully capture the nuanced relationship between scaling and deep value understanding. The paper seems to rely heavily on the assumption that aligning with GPT-4 annotations is indicative of true value comprehension.", "Questions": "How adaptable is the Value Understanding Measurement framework to other value surveys beyond the Schwartz Value Survey? Additionally, can the framework assess other aspects of alignment beyond value understanding, such as ethical reasoning?"}}
{"paper_id": "kXHEBK9uAY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes an innovative combination of hierarchical and diffusion-based planning strategies, which proves to be computationally efficient. The empirical evaluations on offline reinforcement learning benchmarks demonstrate superior performance and efficiency, highlighting the significance of the proposed method.", "Weaknesses": "The paper may not sufficiently address potential limitations or specific scenarios where the proposed Hierarchical Diffuser might not perform as expected. Additionally, the abstract does not elaborate on any specific failures or challenges faced during experimentation.", "Questions": "Can the Hierarchical Diffuser be easily adapted for other types of tasks beyond reinforcement learning, and if so, what modifications would be necessary? How does the method handle failure cases or situations where the temporal abstraction fails to capture critical dependencies?"}}
{"paper_id": "i7LCsDMcZ4", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel approach to generate saliency maps for spiking neural networks, which addresses a significant gap in the field. \n2. The introduction of SLTRP and SLRP rules is innovative and extends the applicability of saliency-based augmentation methods to event-based vision sensing. \n3. Empirical results demonstrate state-of-the-art performance on benchmark datasets, showcasing the effectiveness of the method.", "Weaknesses": "1. While the paper presents impressive empirical results, the scalability of the proposed methods to larger and more complex datasets is not fully explored. \n2. The reliance on specific SNN structures might limit the generalizability of the approach to other neural network architectures.", "Questions": "1. How does the proposed method scale with increasing complexity of the input data or SNN architectures?\n2. Can the SLTRP and SLRP rules be adapted for other modalities beyond vision, such as auditory or tactile event-based data?"}}
{"paper_id": "jHdz0CIS2y", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces an innovative training strategy, SelfVC, that iteratively refines a voice conversion model using self-synthesized examples, which eliminates the need for explicit disentanglement and reduces information loss. 2. The framework is versatile and applicable to various tasks, such as zero-shot voice conversion and voice conversion across different languages, without the need for text inputs. 3. Extensive experiments demonstrate that SelfVC achieves state-of-the-art results in multiple metrics, highlighting its effectiveness and improvements over baseline models.", "Weaknesses": "1. The paper may benefit from a more detailed analysis of the limitations or potential edge cases where the SelfVC framework might not perform as well. 2. The reliance on self-supervised learning models could create dependency on the quality of these pretrained models, which might impact performance in scenarios with limited or diverse data.", "Questions": "1. How does the proposed SelfVC framework handle diverse or extremely challenging voice conversion tasks, such as those involving unique accents or diverse emotional speech? 2. What are the computational requirements of the iterative training strategy, and how does it scale with larger datasets or more diverse speech inputs?"}}
{"paper_id": "NkYCuGM7E2", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents an innovative approach by integrating Large Language Models (LLMs) with autonomous driving systems to leverage human-like commonsense reasoning. This is a novel application in complex AD scenarios that enhances decision-making capability. The extensive experiments validate the effectiveness of integrating LLMs, demonstrating superior performance compared to baseline approaches in handling both single and multi-vehicle tasks.", "Weaknesses": "The methodology relies heavily on the capabilities of the LLMs without systematically addressing potential limitations or failure points, especially in rare or unstructured driving scenarios. Additionally, there is limited discussion on the scalability and adaptability of the proposed approach to a varied set of driving conditions and environments.", "Questions": "How does the system ensure the reliability and safety of decisions made by the LLM, especially in previously unseen scenarios? Is there a feedback mechanism or an update procedure based on real-time performance assessments?"}}
{"paper_id": "RtDok9eS3s", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a simplified design for transformer blocks that maintains performance and training speed while reducing parameter count and increasing throughput.\n2. The study is grounded in both theoretical analysis and empirical validation across different types of transformer models, adding robustness to the claims.\n3. The work addresses a practical concern in deep learning models, namely the complexity and brittleness of standard transformer architectures, which could have significant implications for future model design.", "Weaknesses": "1. The paper may lack a thorough investigation of potential trade-offs in generalization or other aspects of model performance not measured by per-iteration training speed and throughput.\n2. While the simplifications are beneficial in the contexts tested, the applicability to a wide range of tasks and models may need further exploration and validation.", "Questions": "1. How do the simplified transformers perform in terms of generalization across a more diverse set of tasks and datasets?\n2. Are there any tasks or specific scenarios where the simplified architecture does not perform as well as standard transformers?\n3. What potential implications could the removal of components like skip connections or normalization layers have on tasks requiring multimodal inputs or complex reasoning?"}}
{"paper_id": "YCPDFfmkFr", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses a significant challenge in integrating optimization layers into neural networks by proposing a novel approach for handling infeasibility, which can expand the applicability of QP layers. 2. It leverages primal-dual augmented Lagrangian techniques, contributing to both theory and practice in optimization within machine learning. 3. The open-source C++ software package, QPLayer, provides a practical tool for the community to implement and experiment with these concepts.", "Weaknesses": "1. The presentation could be improved for clarity, particularly in explaining technical details to a broader audience with varying expertise levels. 2. Additionally, empirical evaluations demonstrating the practical impact of the proposed QP layers on real-world machine learning tasks could be more comprehensive.", "Questions": "1. How does the proposed approach for infeasible QP layers compare in computational cost with other existing methods for similar tasks? 2. Can the proposed QP layers be easily integrated into existing machine learning frameworks, and what is the level of complexity involved in doing so?"}}
{"paper_id": "ueTdErd5Ib", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel method that integrates learning with the downstream optimization task, addressing both expected performance and robustness to worst-case scenarios.\n2. The approach provides theoretical guarantees, which adds to the credibility and soundness of the method.\n3. Experimental results showcase substantial improvements over existing methods, especially in worst-case performance, which is emphasized with a real-world application in electricity generation.", "Weaknesses": "1. The presentation of the paper could be more polished to enhance clarity, especially in describing the discretization and secondary optimization steps.\n2. While theoretical guarantees are provided, the complexity and practical applicability of the method in various domains beyond the addressed real-world problem could be discussed in more detail.", "Questions": "1. How does the proposed method scale with an increasing number of uncertain parameters?\n2. Are there scenarios or types of optimization problems where the method might not perform as effectively?"}}
{"paper_id": "sDmjlpphdB", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces an innovative approach to improving the generalization capabilities of large language models through the Mixture-of-Expert paradigm, which effectively partitions the problem space into more manageable regions.\n2. The use of both instructions and demos in the MoP framework addresses the limitations of previous demo-free instruction-based techniques, leading to significant improvements in performance.\n3. The methodology is well-structured with a clear two-phase process for constructing specialized experts, supported by theoretical grounding in in-context learning and kernel regression.\n4. The results showing up to 43% improvement on benchmark NLP tasks highlight the effectiveness of the proposed approach.", "Weaknesses": "1. The complexity and added computational overhead of implementing the Mixture-of-Experts model could be a potential barrier to practical deployment, especially in resource-constrained environments.\n2. The approach relies heavily on semantic similarity-based clustering, which may not always capture the nuanced differences between demos needed for all types of tasks.", "Questions": "1. How does the performance of MoP compare to conventional methods in low-resource scenarios where fewer demos are available? \n2. Is there an analysis of how sensitive the model's performance is to the choice of clustering technique used in the demo assignment phase?"}}
{"paper_id": "Mdk7YP52V3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper provides a theoretical explanation for the observed instabilities in heteroskedastic neural regression models through the lens of statistical physics, which is innovative and contributes to the understanding of these models.\n2. The derivation of a nonparametric free energy that aligns with empirical observations is a significant contribution and provides a solid foundation for further research.\n3. The proposed optimization scheme for regularization is more efficient than existing methods, which can have practical implications.", "Weaknesses": "1. While the theoretical insights are strong, the paper may lack extensive empirical validation across a variety of datasets or neural network architectures.\n2. The paper assumes familiarity with concepts from statistical physics, which might make it less accessible to a broader audience.", "Questions": "1. Can the proposed regularization optimization scheme be easily integrated into existing machine learning workflows, or does it require significant modification to current practices?\n2. Are there practical guidelines or heuristics for practitioners to determine the appropriate regularization strengths using the insights from this theory?"}}
{"paper_id": "YGTSLDAPqb", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a crucial problem of domain adaptation and provides a novel approach, Connect Later, which leverages targeted augmentations to bridge the domain gap post-pretraining. The experimental results demonstrate significant improvements over standard fine-tuning, achieving state-of-the-art results on multiple real-world datasets. This shows the practical applicability and effectiveness of the approach.", "Weaknesses": "While the proposed method shows promise, the reliance on domain-specific knowledge to design targeted augmentations could be a limitation, as it may not always be feasible to obtain such expertise for all domains. Furthermore, the innovation is somewhat incremental as it builds upon established methods like pretraining and augmentation.", "Questions": "Can the methodology be generalized to domains other than those tested, or does it rely heavily on the availability of domain-specific knowledge for creating targeted augmentations? How sensitive is the performance improvement to the specific choice of targeted augmentations?"}}
{"paper_id": "JTcaziw7G1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel secure multi-party computation (MPC) protocol for privacy-preserving information retrieval, which is crucial for privacy in distributed settings. \n2. It effectively addresses the problem of data privacy in neural-based information retrieval, which is increasingly relevant as more sensitive data is generated.\n3. The approach is innovative and potentially impactful, as it allows data to remain decentralized while still being useful for augmentation in large language models.", "Weaknesses": "1. The abstract does not specify the performance trade-offs in terms of computational overhead introduced by the MPC protocol, which might affect the practicality of the solution.\n2. Further details on how the proposed protocol compares against state-of-the-art methods in terms of retrieval accuracy and efficiency metrics would be beneficial.", "Questions": "1. What are the computational and communication overheads associated with the proposed MPC protocol compared to traditional methods?\n2. How scalable is the system when dealing with very large datasets, particularly in terms of server and client resources? \n3. Can the proposed method be easily integrated with existing large language models?"}}
{"paper_id": "lEkFq4RUCX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed directional distance field (DDF) provides a novel and efficient method to quantify dissimilarity between 3D point clouds. Extensive experiments demonstrate that DDF achieves higher accuracy across multiple tasks while being memory and computationally efficient. The inclusion of source code enhances reproducibility and practical application.", "Weaknesses": "The paper may benefit from more exhaustive comparisons with a wider range of existing metrics to fully establish the advantages of DDF, especially in varied or more complex scenarios. Additionally, the impact on tasks outside the evaluated ones could be discussed more.", "Questions": "How does DDF perform when applied to point clouds with varying densities or noise levels? Are there any specific limitations when dealing with highly complex or detailed geometries?"}}
{"paper_id": "Fq8tKtjACC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The model phi-1 demonstrates strong performance in code generation with a relatively small parameter size, achieving impressive results on HumanEval and MBPP benchmarks. The use of a diverse dataset, including synthetically generated textbooks, is innovative and contributes to the model's effectiveness.", "Weaknesses": "The paper does not provide detailed analysis or comparisons with larger models beyond simple performance metrics, which makes it difficult to fully understand the model's strengths and limitations. Additionally, the emergence of surprising properties is mentioned but not explored in depth.", "Questions": "What specific emergent properties were observed in phi-1 compared to phi-1-base and phi-1-small? Are there any detailed examples or case studies that illustrate these properties?"}}
{"paper_id": "8fQlGQkj0S", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel and rigorous approach to analyze in-context learning, separating it into task retrieval and task learning. The derivation of risk upper bounds for in-context learning and the demonstration of the unique phenomenon where increased in-context samples can lead to increased risk are significant contributions.", "Weaknesses": "The presentation could be improved to make the complex theoretical concepts more accessible to a broader audience. Some assumptions in the generative models may limit the generalizability of the findings.", "Questions": "How sensitive are the results to the assumptions made about the generative models? Can the phenomenon of increased risk with more in-context samples be mitigated in practical implementations?"}}
{"paper_id": "hRos9WldRK", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach to learning with noisy labels, leveraging a joint reweighting mechanism to improve robustness against label noise.\n2. L2B dynamically adjusts weights between real and pseudo-labels as well as across samples, which enhances the model's performance and ability to generalize.\n3. The method has demonstrated compatibility with existing methods and competitive results across several benchmark datasets.\n4. The authors ensure the practical applicability of their approach by extending it to tasks such as image segmentation.", "Weaknesses": "1. While the results are promising, the paper may benefit from more detailed ablation studies to isolate the impacts of different components of L2B.\n2. The dynamic weight adjustment mechanism may introduce complexity, requiring more explanation or clarity on its implementation and computational overhead.", "Questions": "1. How does the implicit relabeling process in L2B handle cases where the model's initial predictions are extremely noisy?\n2. Can L2B be effectively applied to other domains outside image-based tasks, such as natural language processing with noisy data?\n3. Are there any theoretical guarantees or conditions under which the L2B method performs optimally?"}}
{"paper_id": "ttRSBZiCiu", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces the WaveFluid model, an innovative approach to speech synthesis using probability flow-based models. It shows improvements in sample quality and efficiency, indicating a potential advancement in vocoder technology.", "Weaknesses": "The paper primarily highlights architectural improvements and efficiency gains but does not provide a clear revolutionary concept beyond existing models. More experimentation across diverse conditions could strengthen the claims of effectiveness.", "Questions": "How does the model perform under different audio conditions or with varying amounts of training data? Are there specific scenarios where the WaveFluid model underperforms compared to other state-of-the-art vocoders?"}}
{"paper_id": "WYsLU5TEEo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The proposed unified framework efficiently addresses both interpretability and robustness using a single model setup, which is innovative. The use of GANs for generating both counterfactual and adversarial examples is a strong point. Evaluation on two distinct tasks demonstrates the method's versatility.", "Weaknesses": "It is unclear how well the technique scales to more complex datasets or tasks beyond the two evaluated. The method's reliance on GANs might pose challenges in terms of training stability and computational efficiency.", "Questions": "How does the model perform on tasks with more complex datasets? Is there a detailed discussion on the model's computational demands and training stability given the inclusion of GANs?"}}
{"paper_id": "KKZaj2QS3G", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a critical gap in time series analysis related to handling noise effectively and proposes a novel sampling strategy that enhances representation learning in noisy conditions.\n2. The introduction of a new encoder architecture using dilated convolution within the Inception block is innovative and demonstrates improved scalability and robustness.\n3. The method shows strong empirical results, consistently outperforming state-of-the-art methods across several tasks, including forecasting, classification, and abnormality detection, while using significantly fewer parameters.", "Weaknesses": "1. Although the proposed method shows strong empirical results, the paper may benefit from providing theoretical insights or analysis to support why the sampling strategy and encoder architecture specifically enhance performance in noisy conditions.\n2. While experiments are comprehensive, the paper could explore more diverse datasets or scenarios outside of the UCR datasets to further validate the generality of the approach.", "Questions": "1. How does the proposed sampling strategy perform on time series data with varying levels of noise, and is there a threshold where the method starts to degrade?\n2. Could the dilated convolution within the Inception block be adapted for other types of data beyond time series, and how would that impact architecture design?"}}
{"paper_id": "LWDRiFzbHQ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel method for assessing model generalization ability on out-of-distribution test sets without using test ground truths. The concept of vicinal risk proxy (VRP) is innovative and shows strong correlation improvements with model accuracy over existing indicators. The approach also provides a more holistic view of model robustness by considering neighboring test samples.", "Weaknesses": "While the approach is promising, the presentation could be improved for clarity, especially in explaining VRP's mechanics and how it integrates with current indicators. Additional empirical studies on diverse datasets could help solidify the method's effectiveness across various scenarios. More details on computational complexity and efficiency would also enhance the comprehensiveness of the evaluation.", "Questions": "How does the vicinal risk proxy handle models with inconsistent performance across sample neighborhoods? Could applying VRP lead to overfitting to specific out-of-distribution artifacts if neighborhoods are not representative? What are the computational overheads associated with VRP in contrast to standard indicators?"}}
{"paper_id": "Yp01vcQSNl", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel graph transformer architecture that effectively incorporates edge directionality, which is a significant aspect often neglected in existing models. The dual encoding mechanism coupled with the directional attention module provides a robust representation for directed graphs. The proposal of new directional graph datasets is a strong contribution to the field.", "Weaknesses": "The presentation could be improved for clarity, particularly in explaining the technical details of the directional attention module and dual encodings. Additionally, experiments on a wider range of datasets and comparatives with more existing models would strengthen the empirical evaluation.", "Questions": "How does the proposed method scale with larger, more complex graphs? Are there any limitations in terms of computation or memory requirements when integrating this approach into existing systems?"}}
{"paper_id": "LndMyiBl3n", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces SheAttack, an efficient approach to conducting Restrict Black-box attacks on GNNs that does not require prior knowledge of node labels or victim models. It innovatively uses the Modified Silhouette Score (MSS) to generalize across graphs with different homophily levels and demonstrates strong performance in both homophilic and heterophilic settings.", "Weaknesses": "The approach might be limited in terms of scalability on very large graphs, and the theoretical analysis provided may require simplification or further clarification for broader accessibility.", "Questions": "How does SheAttack cope with varying levels of computational resources, especially in resource-constrained environments? Are there scenarios where the use of MSS might not be suitable for assessing graph quality?"}}
