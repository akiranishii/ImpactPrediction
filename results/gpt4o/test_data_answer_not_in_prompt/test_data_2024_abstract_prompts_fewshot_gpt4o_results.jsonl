{"paper_id": "B0OwtVEejJ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach by integrating weight-entanglement with gradient-based NAS methods, potentially enhancing the performance of NAS searches in weight-entangled spaces. This approach promises improved memory efficiency and enhanced performance, which are significant contributions to the NAS field.", "Weaknesses": "The paper might not clearly articulate the limitations or potential downsides of integrating weight-entanglement with gradient-based NAS methods. Although it mentions comparative assessments, details on experimental setup and results might require additional clarity for validation.", "Questions": "How does the proposed integration affect the computational overhead compared to traditional NAS methods without weight-entanglement? Are there specific scenarios where the proposed method might underperform compared to existing NAS approaches?"}}
{"paper_id": "ZlEtXIxl3q", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel approach to infer fitness functions in protein engineering using contrastive loss functions like the Bradley-Terry loss. This provides a fresh perspective on handling multimodal function estimation, particularly in regimes where traditional losses such as Mean Squared Error (MSE) may fall short. The argument backed by a fitness-epistasis uncertainty principle is insightful and likely to have significant practical implications.", "Weaknesses": "While the theoretical motivation for using contrastive loss functions is well articulated, additional experimental results or comparisons might be necessary to demonstrate the superiority over other cutting-edge methods, beyond demonstrating MSE ineffectiveness. The paper could also benefit from providing more detailed descriptions or visualizations of the experimental setups and results to enhance clarity.", "Questions": "1. How does the proposed method perform compared to other sophisticated or recent approaches beyond MSE and baseline methods? \n2. Are there particular scenarios or types of datasets where the proposed contrastive loss functions method might not yield any benefits over MSE? \n3. Can the approach be generalized to other types of epistatic models beyond protein engineering?"}}
{"paper_id": "6yJuDK1DsK", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "FEATHER offers a novel approach with a clear benefit in reducing the number of parameters needed for lifelong test-time adaptation, thus enhancing performance and efficiency. The approach is technically sound, showcasing robustness against error accumulation and does not require source data for warm-starting, which can prove advantageous in many applications.", "Weaknesses": "Details about the impact of disentangling source and target knowledge via FEATHER could be clearer. More comprehensive comparisons, especially qualitative analyses of errors, would strengthen the presented results. Details on how FEATHER interfaces with and augments current SOTA methods would be beneficial.", "Questions": "How does FEATHER manage potential drift when continually adapting over extended periods or heavily transformed distributions? Can you provide more specifics on potential applications of FEATHER in real-world scenarios, perhaps comparing its performance with other lightweight models?"}}
{"paper_id": "lt6xKGGWov", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper introduces a novel approach to feature selection that uses neural estimation of mutual information, which allows capturing complex relationships between features and targets.\n2. By evaluating subsets of features as an ensemble, the method can consider interactions between features, which is a significant advancement over methods that evaluate one feature at a time.", "Weaknesses": "1. The abstract lacks detailed explanations of the methodology and does not provide insight into the experimental setups and results.\n2. The claim of superiority over existing methods is made without presenting specific comparative results within the abstract, which leaves an open question about the evidence supporting these claims.", "Questions": "1. How does the proposed method compare quantitatively against existing feature selection methods in terms of accuracy and computational efficiency?\n2. Are there specific types of datasets or scenarios where this method shows a particularly strong advantage?"}}
{"paper_id": "Bvrc6kobWd", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel perspective on understanding the role of margins in contrastive learning through gradient analysis, which provides new insights into representation learning improvements. The proposed approach successfully isolates and analyzes the specific effects of margins, leading to improved generalization performance on both seen and unseen datasets. The experimental validation supports the claims made.", "Weaknesses": "While the paper introduces an interesting perspective on margins, it lacks detailed explanations on how the findings can be systematically incorporated into practical implementations of contrastive learning. The paper could benefit from clearer visualizations or more rigorous evaluations, especially in comparing the proposed enhancements with existing methods.", "Questions": "1. How can the findings on margins and gradient effects be generalized beyond the specific domains tested in the experiments?\n2. Are there any limitations or potential downsides to emphasizing positive samples and scaling gradients based on angles and logits?\n3. How does the proposed perspective integrate with existing margin-based methods to maximize performance?"}}
{"paper_id": "FJjHQS2DyE", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces Conditional Adversarial Support Alignment (CASA), which addresses the limitations of existing UDA models by focusing on support alignment under label distribution shift.\n2. A novel theoretical target risk bound is presented, which provides a solid theoretical foundation for the proposed method.\n3. Extensive empirical validation demonstrating superior performance over state-of-the-art methods under various conditions.", "Weaknesses": "1. The paper may not fully explore the limitations or potential failure cases of CASA under different domain adaptation scenarios.\n2. Some of the empirical evaluations may still need further detail, particularly in terms of the comparison to diverse types of distribution shifts beyond label distribution shifts.", "Questions": "1. How does CASA perform with more complex domain adaptation tasks involving multi-modal data or extreme shifts in feature space?\n2. Can the proposed theoretical target risk bound be extended to other types of domain adaptation frameworks beyond UDA?\n3. What are the computational overheads associated with the CASA framework compared to other state-of-the-art methods?"}}
{"paper_id": "xelrLobW0n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces TRACE, a novel and challenging benchmark for evaluating continual learning in large language models, which is a relatively unexplored aspect of their capabilities. TRACE covers a wide range of tasks that test various dimensions of model performance, including domain-specific knowledge and multilingual capabilities. The authors also propose the Reasoning-augmented Continual Learning (RCL) approach, demonstrating its potential to mitigate catastrophic forgetting in LLMs.", "Weaknesses": "The paper reveals a significant drop in model performance on general and instruction-following tasks after training on TRACE datasets, which might raise concerns about the practicality of deploying models trained in such a manner. Additionally, while RCL shows promise in addressing some issues, the explanation of its implementation details and the extent of its effectiveness could be clearer.", "Questions": "Can the approach of Reasoning-augmented Continual Learning (RCL) be integrated with other existing continual learning strategies, and if so, how does it affect their overall performance? Additionally, what specific types of reasoning tasks were found to be most beneficial in preserving model abilities while training on TRACE?"}}
{"paper_id": "bYQkOPvgDw", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel probabilistic graphical model framework specifically tailored for handling label noise in graph neural networks, addressing a significant gap in current research. PRGNN is designed to perform well in high noise-rate situations and on graphs with varying heterophilies, offering robustness that many existing methods lack. The approach is well-supported by extensive experiments, demonstrating strong performance across different noise types and rates.", "Weaknesses": "The methodology, while innovative, could be complex to implement and understand fully for some practitioners not familiar with Bayesian networks. Additionally, the dependence on clean label sets being available or extractable from noisy ones could be a limiting factor in some real-world applications.", "Questions": "1. How does PRGNN perform compared to non-Bayesian approaches for handling label noise in GNNs?\n2. Are there any specific limitations or scenarios where PRGNN might not perform as well, particularly in real-world applications?\n3. How sensitive is the performance of PRGNN to the quality and size of the clean label set?"}}
{"paper_id": "sKPzAXoylB", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed method, Utility-based Perturbed Gradient Descent (UPGD), effectively addresses both the catastrophic forgetting and loss of plasticity issues simultaneously, which is a significant improvement over existing methods that only tackle one. UPGD's adaptability in a streaming learning setup with numerous non-stationarities is commendable. The paper provides a thorough comparison with existing methods, highlighting UPGD's superior performance. The reinforcement learning experiments further validate the approach.", "Weaknesses": "The paper could be improved by providing more detailed explanations on the implementation and mechanisms of UPGD. The presentation, while adequate, could be more polished to enhance readability, particularly for complex concepts. A deeper theoretical discussion on why UPGD succeeds where others fail might add more strength to the work.", "Questions": "How does UPGD determine the utility of units during training, and how sensitive is it to the initial conditions or hyperparameters? Do the authors have insights on how it performs across a wider variety of tasks beyond reinforcement learning? Could the approach be extended to handle other types of continual learning challenges, such as domain adaptation or transfer learning?"}}
{"paper_id": "H8Qg1IIMaR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a relevant and timely issue, particularly the robustness of large language and vision-language models, which are increasingly being used in practical applications. It identifies a specific vulnerability, namely permutation sensitivity in MCQA tasks, which is crucial for understanding model reliability.", "Weaknesses": "The abstract does not provide details on the methodology or the extent of the experiments conducted, making it challenging to fully assess the soundness of the empirical results. Additionally, there is no mention of potential solutions or mitigations for the identified vulnerability.", "Questions": "What methods were used to test permutation sensitivity, and across how many models and tasks were these methods applied? Are there proposed strategies for mitigating the vulnerabilities identified, or is the focus solely on highlighting the issue?"}}
{"paper_id": "tth2qXY7RU", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "The paper introduces a novel quantization method, Super Floating-Point (SuFP), which provides significant improvements in computational capability and energy efficiency. The proposed method effectively balances accuracy and resource demands by using multi-region piecewise quantization and a tensor-wise scalable bias. The hardware implementation is compact, utilizing only integer arithmetic and shifters.", "Weaknesses": "The paper could provide more detailed technical explanations about the SuFP implementation and hardware integration. Additionally, more information on the benchmarks used for evaluation and how SuFP compares in terms of implementation complexity to other methods would be beneficial.", "Questions": "1. How does SuFP handle variance in data distributions across different models and tasks?\n2. Could the authors provide more detailed case studies or examples of SuFP in action on real-world hardware?\n3. What are the trade-offs between SuFP and other quantization techniques regarding implementation complexity and deployment feasibility?"}}
{"paper_id": "fjf3YenThE", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "H9DYMIpz9c", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to data distillation specifically designed for auto-regressive tasks, which is a significant and timely contribution. Farzi, the proposed method, shows impressive empirical results, achieving near or even improved performance on significantly reduced datasets. The method's use of efficient reverse-mode differentiation and the factorization of event-space demonstrates both sound technical understanding and practical ingenuity.", "Weaknesses": "The paper might lack detailed explanations on the limitations or edge cases where Farzi might not perform well. Additionally, while the technique demonstrates impressive performance, there could be concerns about the generalization of results across other datasets or contexts not covered in the experiments.", "Questions": "How does Farzi handle different types of data noise or variability in datasets that are not explicitly mentioned? Have there been any theoretical analyses regarding the stability or convergence of models trained on Farzi Data compared to those trained on the full datasets? Are there any specific characteristics or properties that the datasets used in the experiments share which might have influenced the results?"}}
{"paper_id": "4kLVvIh8cp", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel oracle-efficient algorithm for offline reinforcement learning (RL) with non-linear function approximation, which is a significant contribution to the field. The proposed PNLSVI algorithm offers a well-defined approach with three innovative components, including a variance-based weighted regression scheme, variance estimation subroutine, and a pessimistic value iteration planning phase. The algorithm demonstrates a tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret. This work extends previous results from simpler function classes to a more general framework, making it the first statistically optimal algorithm for nonlinear offline RL, which is noteworthy.", "Weaknesses": "The presentation could be improved, specifically in terms of clarity and structure to enhance the reader's comprehension. While the abstract outlines the algorithm's components and contributions, more detailed explanations and illustrative examples would help in understanding the implementation and performance of the algorithm. Additionally, the presentation of empirical results or specific comparison against existing methods wasn't detailed in the abstract, which might leave readers questioning the practical impact and validation of the proposed approach.", "Questions": "1. How does the proposed algorithm perform empirically compared to state-of-the-art offline RL methods with non-linear function approximation?\n2. Are there any assumptions or limitations in the function class complexity that might affect the generalizability of the proposed algorithm?\n3. What specific application domains or scenarios can benefit the most from the PNLSVI algorithm? Can the approach be adapted for different types of non-linear function approximations or models?"}}
{"paper_id": "CSpWgKo0ID", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The study uses behavioral game theory to uncover specific behavioral signatures in LLMs, providing deep insights into their social interaction dynamics. This approach is innovative and adds a new dimension to understanding LLMs' cooperative and coordination behaviors.", "Weaknesses": "While the study identifies behavioral patterns, it does not provide a detailed exploration of the underlying mechanisms driving these behaviors in LLMs, leaving open questions about their interpretability.", "Questions": "How do the authors plan to extend their research to understand the internal decision-making processes within LLMs that lead to observed behavioral signatures?"}}
{"paper_id": "gwbQ2YwLhD", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides a significant theoretical contribution by identifying conditions where least squares-based losses are minimal for incorrect DAGs in multi-dimensional cases. It extends previous results beyond linear 2-node systems and substantiates its findings with extensive experiments on synthetic and real-world data. The exploration of how data scaling affects structure learners is particularly relevant and valuable to the field.", "Weaknesses": "While the theoretical results are compelling, the presentation could be clearer, especially in the mathematical derivations. The paper would benefit from an expanded explanation of the implications of these findings in practical applications beyond the provided experiments. Additionally, the presentation of experimental results could be more detailed to enhance understanding.", "Questions": "How do the proposed scale-influenced findings interact with real-world data that has inherent noise and variability? Are there recommended strategies for practitioners to mitigate scale issues in practical scenarios where complete control over data preprocessing might not be feasible?"}}
{"paper_id": "eIYDKNqXuV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The proposed Village-Net Clustering appears to offer a novel approach to dealing with complex manifold data, boasting competitive performance in terms of normalized mutual information (NMI) scores against state-of-the-art methods. Its ability to autonomously determine the optimal number of clusters and its scalability to large datasets mark significant strengths of the algorithm.", "Weaknesses": "The abstract does not provide detailed insights into the theoretical underpinnings of the algorithm or how it specifically improves upon existing approaches in unsupervised clustering. Additionally, the effectiveness of the algorithm is demonstrated solely by benchmarking against datasets with known ground-truth labels, which may not adequately represent its robustness in real-world applications with unknown classifications.", "Questions": "1. How does the Walk-likelihood Community Finder specifically contribute to the improvement of the clustering results compared to other network-based community detection methods?\n2. What are the specific datasets used for benchmarking, and how diverse are they in terms of type and complexity?\n3. Are there any potential limitations or conditions where the Village-Net Clustering might underperform, particularly in relation to the dimensionality and type of data?"}}
{"paper_id": "bAMPOUF227", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel framework that effectively integrates task-specific fine-tuned Language Models into the inference stage of Large Language Models, particularly showing improvements in generalizability and factuality. The proposed plug-in method enhances established models like Llama 2 and ChatGPT, thereby underscoring its practical applicability. The comprehensive empirical analysis and the provision of extensive resources such as datasets and model checkpoints add significant value to the contributions.", "Weaknesses": "Although the framework demonstrates clear benefits, it heavily relies on discriminative models, which may not always be feasible or scalable across all task domains. The paper lacks a detailed discussion on the computational overhead and potential limitations when deploying such models in production environments. The novelty of the approach could be questioned as it builds upon existing work by integrating pre-trained models rather than developing new ones.", "Questions": "1. How significant is the computational overhead introduced by integrating task-specific fine-tuned models into LLMs at the inference stage?\n2. Could the proposed framework be extended to integrate other types of models beyond discriminative ones? If so, how?\n3. Are there specific types of tasks or domains where this approach is less effective? How do these align with the suite of tasks and datasets you selected for evaluation?"}}
{"paper_id": "eNoiRal5xi", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel objective, UDIM, that seems to effectively enhance domain generalization by dealing with unknown domains in a unique manner. The idea of crafting empirically unknown domains by perturbing source instances is innovative. Theoretical validation of merging SAM with UDIM provides a strong foundation to the proposed method. The empirical results indicate significant improvements over existing SAM variants.", "Weaknesses": "The presentation could be improved for better clarity and understanding, especially regarding the complex theoretical formulations. There might be a need for a more detailed explanation of the empirical crafting of unknown domains and how this is practically executed. The abstract does not provide a clear understanding of the computational costs involved with UDIM.", "Questions": "How well does UDIM scale with larger datasets and more complex models, considering the need for empirical crafting of new domains? What are the potential computational overheads introduced by UDIM in comparison with standard SAM methods?"}}
{"paper_id": "uU0Adp7Sfo", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. CCGAN introduces an innovative approach that combines collaborative and competitive aspects in GANs, providing potential improvements over traditional GAN models. 2. The theoretical framework regarding the conversion of the regularization term to a divergence for better game-theoretical foundations is well-motivated and technically sound. 3. The experiments conducted across multiple datasets demonstrate strong performance and highlight the effectiveness of the proposed method in generating high-quality samples.", "Weaknesses": "1. The paper could improve on clarity in detailing the specific methodologies and nuances of the collaborative-competitive loss functions to enhance understanding. 2. While promising, the application and adaptation of the method to other domains with irregularly shaped objects or more complex tasks may not be straightforward and requires further exploration.", "Questions": "1. How does CCGAN compare against other advanced GAN variations specifically designed for multi-modal generation and super-resolution tasks? 2. What are the specific challenges encountered during the hyper-parameter tuning of this model, and how do the authors suggest practitioners balance the collaborative and competitive components effectively?"}}
{"paper_id": "dKju7tbe6D", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The IPRM demonstrates strong reasoning capabilities and generalization, which is validated through extensive experiments on various benchmarks.\n2. The model achieves state-of-the-art results in zero-shot performance on challenging datasets, indicating its robustness and applicability to real-world scenarios.\n3. The visualization of the computational steps enhances the interpretability and diagnostic capabilities of the model.\n4. The novel combination of iterative and parallel reasoning offers both efficiency and robustness in handling complex tasks.", "Weaknesses": "1. Although the IPRM is presented as having fewer parameters, the paper does not elaborate on the exact resource usage, which makes it difficult to fully ascertain its efficiency in comparison to existing methods.\n2. The abstract does not provide detailed insights into the architectural innovations or the specific mechanisms that enable concurrent operations, which might limit the initial understanding of the novelty.\n3. While the results on CLEVR-Humans and other datasets are impressive, further validation on a diverse set of real-world datasets would strengthen the claims.", "Questions": "1. Can the IPRM be extended to other domains beyond visual question answering and reasoning, such as natural language processing or robotics?\n2. What specific architectural changes allow for the combination of iterative and parallel reasoning mechanisms in a neural framework?\n3. Are there any potential limitations or failure cases observed with the IPRM when handling different types of reasoning tasks?"}}
{"paper_id": "DL7JWbdGr3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses an important problem by leveraging pre-training strategies used successfully in other domains for epidemic modeling, which is innovative and significant. The proposed Pre-trained Epidemic Time-Series Models (PEMS) demonstrate superiority over state-of-the-art approaches on various datasets, including those with diverse dynamics and new epidemics like Covid-19. The work is also efficient, utilizing smaller data fractions effectively.", "Weaknesses": "While the model shows impressive results, the paper lacks detailed technical explanations of the self-supervised learning tasks and how exactly they're tailored to epidemic dynamics. Additionally, the generalization to other domains of epidemiological modeling is presumed but not deeply explored.", "Questions": "How do the self-supervised learning tasks specifically handle the heterogeneous dynamics of different diseases? Are there any inherent limitations identified when applying PEMs to vastly different types of epidemics beyond those tested?"}}
{"paper_id": "1PaDPHDhwe", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel class-specific scaling strategy that is applicable to existing debiasing algorithms without requiring additional training.\n2. The instance-wise adaptive scaling technique proposed represents an advancement that can enhance both average and robust accuracies.\n3. The introduction of a unified metric to evaluate the performance of the algorithms provides a comprehensive measurement of the trade-off between robust and average accuracies.", "Weaknesses": "1. The paper may provide limited insight into the computational costs associated with the proposed scaling techniques, which could be pivotal for their practical applicability across different models and datasets.\n2. There might be a lack of in-depth discussion on potential limitations or edge cases where the proposed methods may not perform well.", "Questions": "1. Can the proposed scaling strategies be applied effectively across a wider range of domains beyond computer vision and natural language processing?\n2. How does the proposed unified metric compare against previously existing metrics in terms of ease of interpretability and practical applicability for the evaluation of model performance?"}}
{"paper_id": "iI7hZSczxE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach to disentangled representation learning for time series data, specifically targeting electricity consumption data. The method effectively addresses real-world attribute correlations by utilizing contrastive learning and a single autoencoder, which improves interpretability and facilitates generalization. The introduction of Time Disentangling Score (TDS) as a reliable metric for evaluating disentanglement performance adds value to the evaluation process.", "Weaknesses": "The approach might be limited in its applicability to other time series domains without modifications, given the specific focus on electricity consumption. The reliance on contrastive learning could pose scalability issues for very large datasets. Furthermore, the presentation of the method could be more detailed in explaining how exactly the disentanglement is achieved through the proposed DIOSC framework.", "Questions": "1. How does the proposed DIOSC framework handle scenarios where time series data is missing or irregularly sampled?\n2. Can the Time Disentangling Score (TDS) be generalized to other time series datasets outside of the context of electricity usage?\n3. What are the computational costs associated with the l-variational inference layers and self-attention in DIOSC, and how do they scale with increasing data sizes?"}}
{"paper_id": "qrGjFJVl3m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The Qwen-VL series presents a comprehensive vision-language model that extends existing language models with robust visual understanding capabilities.\n2. The models set new records on a variety of benchmarks, demonstrating their performance and versatility across different tasks and settings.\n3. The introduction of a multilingual multimodal cleaned corpus for training adds significant value by increasing diversity and robustness of the models.\n4. The decision to make the models publicly available is a strong contribution to fostering future research in the field.", "Weaknesses": "1. The abstract lacks details on the specific techniques used in the 3-stage training pipeline, and more information about these stages would help assess the novelty of the approach.\n2. While performance benchmarks are mentioned, there is no information on the datasets or comparative baselines used, which could be important for evaluating the claimed superiority.", "Questions": "1. Can you provide more details on the 3-stage training pipeline and how it specifically contributes to the models' performance?\n2. How does the model handle potential biases in the multilingual multimodal corpus, and what measures are in place to ensure fair representation across different languages?\n3. Are there any plans for further improvements or additional capabilities for future iterations of Qwen-VL models?"}}
{"paper_id": "qup9xD8mW4", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel approach for dataset distillation in reinforcement learning, a domain where traditional methods are inapplicable due to the absence of a fixed dataset. The method, HaDES, demonstrates impressive results by discovering minimal synthetic state-action datasets that achieve competitive performance levels. The generalization ability of these datasets to different architectures and hyperparameters further showcases the method's robustness. Additionally, the application of HaDES to neuroevolution in RL and its success in standard supervised distillation tasks underline its versatility and potential impact.", "Weaknesses": "The paper may not have explored the full range of limitations or potential failure cases of the HaDES method. The requirement to discover tiny yet effective datasets might not always hold for more complex tasks or environments, and the significance of the results should be further supported with comparisons to baseline methods. Furthermore, details on how the synthetic datasets are visualized for task insights could be clarified to strengthen interpretability claims.", "Questions": "1. How well does HaDES perform in more complex or high-dimensional RL tasks beyond the continuous control tasks mentioned?\n2. Are there specific architectures or hyperparameters where HaDES struggles to achieve generalization, and what might be the reason for any observed limitations?\n3. Can the insights gained from visualizing the synthetic datasets directly inform improvements in policy training or task design beyond interpretability?"}}
{"paper_id": "Zw8YxUWL4R", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to improving text-to-image diffusion models by incorporating an Extended Textual Conditioning space (P+), which is demonstrated to offer finer control and enhanced image synthesis capabilities. The introduction of Extended Textual Inversion (XTI) is a significant advancement over the original Textual Inversion (TI), showing better expressiveness and faster convergence. The conducted experiments thoroughly demonstrate the method's effectiveness in personalizing models and achieving superior reconstruction and editability.", "Weaknesses": "The paper might not address potential limitations or challenges in generalizability of the proposed method across different datasets or model configurations. Additionally, while the method shows promise in object-style mixing, it may be necessary to evaluate its performance across more diverse and complex scenes.", "Questions": "How does the proposed XTI method generalize across diverse datasets and model architectures? Are there any limitations or conditions where the method might not perform as expected? Furthermore, are there any specific computational costs or memory usage considerations associated with using P+ in practical scenarios?"}}
{"paper_id": "3NXhwkZGjz", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The approach addresses a challenging problem in domain adaptation by introducing a novel method that leverages multiple prediction hypotheses and their consolidation to create a reliable pseudo-labeled set. The paper demonstrates state-of-the-art performance with extensive experimental results and proposes a methodology that can be integrated with existing approaches.", "Weaknesses": "The paper lacks detailed insight into the limitations and potential failure modes of the proposed method. Further discussion on the computational cost and scalability of the three-step adaptation process would be beneficial. Additionally, the method might rely heavily on the accuracy of initial hypothesis generation, which could be problematic in certain scenarios.", "Questions": "How sensitive is the proposed method to the initial quality of the multiple prediction hypotheses? What strategies can be employed if the initial hypotheses are significantly off? Can the three-step adaptation process handle real-time domain adaptation scenarios?"}}
{"paper_id": "Fn655mJ4bv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents an interesting novel interpretation technique for structured output models that emphasize the importance of feature impacts via computational paths among output variables. The consideration of output variable correlations to improve explanation quality is a significant contribution.", "Weaknesses": "The method's applicability is limited to black-box models, which could pose challenges for models where feature interaction is extremely complex or unclear. Moreover, the explanation of the proposed energy-based training process could be further detailed, especially regarding scalability and computational efficiency.", "Questions": "How does the method scale with increasing complexity of structured output models and larger datasets? Are there specific scenarios where this interpretation technique outperforms existing methods?"}}
{"paper_id": "VyMW4YZfw7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper revisits spectral GNN methods and provides a fresh perspective by demonstrating that simpler nonparametric estimation techniques can achieve state-of-the-art performance. This insight challenges the complexity of deep-learning-based architectures and suggests a potential area for improvement in the field. Moreover, the paper presents an ablative study on hyperparameters, which is valuable for understanding the influence of these factors on performance.", "Weaknesses": "The paper may lack a detailed exploration of why nonparametric estimation methods perform so well compared to more complex architectures. Additionally, the work might benefit from a broader comparison with a wider array of benchmarks and not just the common SSNC datasets. The extent to which shifts in evaluation conventions affect GNN performance is an important point but may need more extensive examination.", "Questions": "How do the simpler nonparametric methods compare in terms of computational efficiency and scalability against more complex GNN architectures? Can the insights about evaluation conventions apply to other domains or tasks where deep learning models are heavily used?"}}
{"paper_id": "09xFexjhqE", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper identifies a significant issue with existing robust fine-tuning methods and proposes a novel solution that effectively disentangles the optimization process. AutoLoRa achieves state-of-the-art results across multiple tasks, indicating its broad applicability and practical utility. The proposed approach automates hyperparameter tuning, which is a significant improvement in usability.", "Weaknesses": "While the paper provides a strong empirical evaluation, it may benefit from additional theoretical grounding to better understand why the proposed LoRa branch effectively mitigates the identified gradient divergence issue. More detailed figures and explanations would improve the clarity of the presentation.", "Questions": "How does the introduction of the LoRa branch affect the computational efficiency of the fine-tuning process? Are there any specific types of tasks or datasets where AutoLoRa does not perform as well, or does it consistently outperform other methods across all evaluated scenarios?"}}
{"paper_id": "nmBjBZoySX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. Innovative approach integrating adaptability, dynamics, and automation in the identification of graph lottery tickets, which potentially advances the state-of-the-art in the area of sparse GNN training. \n2. Theoretical proof provided for the mitigation of over-smoothing and improved sparse structures, which grounds the framework in solid theoretical contributions.\n3. Demonstrated superior performance on various graph datasets, indicating strong empirical results and applicability to real-world graphs.", "Weaknesses": "1. The framework's complexity and potential computational overhead are not clearly discussed, which might affect its practical scalability and ease of implementation.\n2. Details on how the theoretical proofs translate to practical performance metrics could have been better elaborated to connect theory and application more clearly.\n3. The framework may still face challenges in extreme real-life large-scale graph datasets, which are not discussed in detail.", "Questions": "1. How does AdaGLT handle diverse graph structures in terms of node and edge attributes that differ from the types presented in the experiments?\n2. What are the specific computational resource requirements compared to existing GLT algorithms, particularly in terms of memory and time complexity?\n3. Can the AdaGLT framework be easily integrated with various GNN architectures, or does it require specific structural adjustments?"}}
{"paper_id": "di52zR8xgf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents significant advancements in text-to-image synthesis by enhancing the Stable Diffusion model. The utilization of a larger UNet backbone and novel conditioning schemes contribute to higher quality image generation, making it competitive with state-of-the-art models. The introduction of a refinement model further improves visual fidelity, showcasing robust empirical results.", "Weaknesses": "While the technical improvements are clear, the novelty might be seen as incremental given the reliance on existing architectures and diffusion models. Further exploration or validation on diverse datasets could strengthen claims about the model's versatility and robustness.", "Questions": "1. How does the model perform across different domains or less common image categories? \n2. What are the computational requirements for training and deploying SDXL compared to its predecessors?\n3. Is the refinement model integrated into the primary generation process, or does it operate as a separate post-processing step?"}}
{"paper_id": "0b328CMwn1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel concept, activation prompt (AP), which enhances visual prompting by applying universal perturbations to activation maps within intermediate layers. Extensive experimentation across 29 datasets and various architectures shows significant improvement in performance and efficiency over input-level visual prompting. The analysis offers insights into layer preferences for prompting, enhancing the understanding of model architecture differences like CNNs and ViTs.", "Weaknesses": "The paper could include more discussion on potential limitations or pitfalls of the proposed AP approach when applied to other model architectures beyond CNNs and ViTs. Additionally, the mathematical and theoretical explanations, though explored, could benefit from further simplification for broader accessibility.", "Questions": "How well does the activation prompt method generalize to modalities other than vision, such as text or audio? Are there specific challenges anticipated when extending this method to such domains?"}}
{"paper_id": "AMDKqZcZbi", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel task inspired by biological systems and presents a model capable of rapid and continual learning. The approach is biologically inspired and demonstrates significant performance improvements over standard artificial neural network baselines in both continual and few-shot learning settings. The model successfully integrates ideas from neuroscience, particularly from the entorhinal-hippocampal circuit, providing insights into memory retention and rapid adaptation across environments.", "Weaknesses": "While the biologically inspired approach is innovative, the presentation of the model details and the experiments could be more thorough to aid understanding. The paper might benefit from clearer explanations on the implementation of the biologically inspired components and their comparison against baselines. Additionally, the evaluation could be expanded to include more diverse tasks to fully establish the generalizability of the proposed model.", "Questions": "1. Could the approach be extended to other domains beyond spatial navigation? If so, what would be the potential challenges?\n2. How does the performance of the model degrade over time in tasks that require long-term retention of different environments?\n3. Are there any limitations observed when increasing the complexity or number of environments in the sWM task?"}}
{"paper_id": "am7BPV3Cwo", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel statistical framework (ImOOD) that effectively addresses the challenges of OOD detection on imbalanced data distributions, providing a deeper theoretical understanding of the inherent biases.\n2. The proposed methods, integrating post-hoc normalization and training-time regularization, demonstrate significant performance improvements over state-of-the-art methods on benchmark data sets, showcasing practical applicability and robustness.\n3. The insight about class-aware bias in imbalanced OOD detection problems is a valuable addition to the field.", "Weaknesses": "1. While the theoretical analysis is a strong point, the paper's presentation could be improved for better clarity and accessibility to a broader audience, particularly those less familiar with statistical frameworks.\n2. The framework might rely heavily on certain assumptions about the data distributions, which could limit its applicability to more diverse real-world datasets.\n3. The paper does not thoroughly discuss potential limitations or failure cases of the proposed methods under different scenarios.", "Questions": "1. How does the ImOOD framework handle situations where the class imbalance in ID data is extremely severe, with one class having significantly fewer samples than others?\n2. Can the proposed post-hoc normalization and training-time regularization techniques be extended or adapted to other types of neural network architectures beyond those used for image classification?\n3. What are the computational overheads associated with implementing the ImOOD framework compared to conventional OOD detection methods, and how might this impact real-time applications?"}}
{"paper_id": "R1crLHQ4kf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel and effective approach for detecting adversarial examples in automatic speech recognition systems by analyzing output probability distributions. The method is simple, widely applicable, and shows excellent performance in distinguishing adversarial examples from clean and noisy data. The extensive empirical evaluation across different ASR systems and datasets highlights the robustness and practicality of the approach.", "Weaknesses": "The abstract does not mention how computationally intensive the approach is, which could be a barrier to its real-time applicability. Although adaptive attacks were considered, it is unclear to what extent such attacks can be optimized to circumvent the detection method while maintaining imperceptibility.", "Questions": "How does the proposed detection method perform under real-time conditions, considering computational cost and delay introduced by the detection process? Can the adversarial examples be effectively mitigated without compromising the audio quality or perceptibility?"}}
{"paper_id": "i7P2mK3x3o", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a flow-based model that extends normalizing flows to transport between arbitrary distributions, which is a novel and valuable contribution. The model's ability to perform downstream tasks such as density ratio estimation and distribution interpolation demonstrates its versatility and potential applicability to various domains.", "Weaknesses": "While the paper clearly outlines the model's potential applications, it lacks a thorough discussion on the limitations or challenges of implementing the approach in practical settings. Additionally, more details on computational complexity and performance on real-world datasets would strengthen the contribution.", "Questions": "How does the proposed model handle cases where the distributions P and Q have significantly different support or dimensionality? Are there specific constraints or regularization techniques required to ensure the model's stability and generalization in such scenarios?"}}
{"paper_id": "YIls9HEa52", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel extension of the rSLDS model by incorporating a semi-Markov process with latent geometry, thus addressing known limitations related to trial-varying data and state organization. This enhancement allows for flexible state cardinality, which is a significant advancement over traditional methods. The application of PDE theory to derive efficient formulations of dynamical statistics is both innovative and promising. Validation on synthetic data and application to mouse electrophysiological data demonstrate the model's potential.", "Weaknesses": "The paper might lack clarity in the presentation of complex mathematical concepts, potentially making it hard for readers unfamiliar with the background to fully grasp the contributions. There could be more emphasis on practical implications or comparisons with existing models in real-world settings.", "Questions": "1. How does the proposed model perform in comparison to other state-of-the-art models in terms of computational efficiency and accuracy in real-world datasets?\n2. Are there any plans to further refine the model's ability to generalize across different datasets, especially those with higher variability?\n3. Can the model's application be extended to other biological datasets beyond mice electrophysiological data, such as human neural data?"}}
{"paper_id": "koYsgfEwCQ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper introduces a novel approach for object-centric representation learning in dynamic 3D scenes, which is a significant advancement over traditional 2D methods. By integrating object-centric voxelization and NeRF decoding, DynaVol is able to decompose scenes more effectively and offer capabilities such as editing geometric shapes and manipulating object motion, which 2D methods cannot achieve. The proposed framework demonstrates superior performance in unsupervised scene decomposition tasks.", "Weaknesses": "The abstract does not mention the specific datasets or evaluation metrics used to validate the model's performance. Additionally, while the abstract claims comprehensive advancements, it lacks details on computational efficiency and scalability to more complex or larger scenes.", "Questions": "What are the limitations of applying DynaVol to scenes with a large number of objects or very complex interactions? Does the approach require specific hardware capabilities for effective implementation and training?"}}
{"paper_id": "o6AN2ZoNw2", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a straightforward and effective framework for open-vocabulary semantic segmentation called P-Seg, which utilizes pseudo-masks and language. The method demonstrates state-of-the-art performance on multiple benchmarks without the need for fine-tuning, showing impressive generalizability. The approach benefits from scalability with data and improves further with self-training, making it a promising baseline for future research in this domain.", "Weaknesses": "While the framework is conceptually simple and effective, it may lack novel contributions beyond leveraging existing techniques (e.g., MaskFormer) with pseudo-masks and language inputs. Additionally, details on the specific implementation and training processes may be necessary to fully understand the approach and its scalability.", "Questions": "1. What specific datasets and size were used to train P-Seg, and how did the choice of datasets impact performance? 2. Can the framework handle highly diverse open-vocabulary inputs, and how does it perform on lesser-known classes in the datasets? 3. Are there any specific limitations or challenges faced during the integration of language inputs with pseudo-masks, and how were they addressed?"}}
{"paper_id": "HXZK1Z8tHa", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The study addresses a key limitation of transformer-based networks in image restoration by introducing a novel architecture, ShareFormer, that effectively balances latency, trainability, and performance.\nThe idea of sharing attention maps among neighboring blocks and incorporating residual connections on 'Value' is innovative and improves the network's optimization and efficiency without requiring convolutional modules.", "Weaknesses": "The proposal may lack detailed explanations or theoretical justifications on why the specific mechanism of sharing attention maps significantly reduces latency and enhances trainability.\nThere is limited discussion of potential limitations or scalability of the proposed method for different image resolutions or domains.", "Questions": "How does the performance of ShareFormer compare with other state-of-the-art models on different datasets and image resolutions?\nWhat are the potential limitations of the approach, and how does it scale with larger and more diverse datasets?"}}
{"paper_id": "QO3yH7X8JJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper presents a novel approach to arbitrary-scale super-resolution (ASSR) using diffusion-based generative models (DGMs) that does not require additional training, fine-tuning, or model-specific fine-tuning. The exploitation of pre-trained DGMs without extra training offers a practical and computationally efficient solution, evidenced by superior results compared to state-of-the-art methods. The introduction of the Perceptual Recoverable Field (PRF) metric offers a new angle on balancing low-level fidelity and high-level signature, rooted in comprehensive theoretical analysis and empirical support.", "Weaknesses": "The paper, while showcasing superior results, may benefit from more detailed discussion on the theoretical aspects underpinning the PRF metric and its empirical validation methodology. Although the approach does not require additional training, it hinges heavily on pre-trained models; hence, generalization to various contexts depends on the robustness and variety of these pre-trained models.", "Questions": "How does the choice of pre-trained DGMs affect performance, and what characterizes a 'suitable' model for ASSR tasks in this context? Could the proposed method be limited to certain types of visual content or degrade with highly diverse content? What are the implications for using different noise injection strategies and their impact on the proposed PRF metric?"}}
{"paper_id": "sNtDKdcI1f", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a critical issue in the application of Reinforcement Learning from Human Feedback (RLHF), namely the tendency to produce longer outputs, and demonstrates that these length biases impact perceived improvements in model performance. The authors provide a detailed analysis of the correlation between length and reward and explore interventions to address this bias.", "Weaknesses": "The interventions proposed in the study to mitigate the impact of response length are not uniformly effective across different settings, indicating that the solution might not be broadly applicable. Furthermore, the conclusion that length-based rewards can reproduce most improvements suggests that current reward models may not evaluate quality as intended.", "Questions": "What specific interventions were tested to mitigate the preference for longer outputs, and why were these particular interventions chosen? How do the authors plan to develop reward models that can better discern quality independent of response length? Are there plans to test the proposed interventions on more diverse datasets or language models to further validate their effectiveness?"}}
{"paper_id": "HFtrXBfNru", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper offers a solid theoretical foundation by establishing a lower bound on representation distortion. The introduction of the Smart baseline, which uses an adaptive feature extractor through self-supervised graph reconstruction, is shown to be effective in both synthetic and real-world datasets. The empirical results are convincing and showcase the utility of the proposed method in improving estimation performance over time.", "Weaknesses": "The presentation could be improved as some key concepts might not be clearly explained to readers unfamiliar with the domain. The reliance on synthetic data for part of the theoretical proof might limit the perceived applicability of the findings. Additionally, more details on the implementation and computational requirements of Smart could enhance the practicality of the approach.", "Questions": "How does the Smart baseline compare computationally to traditional and other state-of-the-art methods? Is there any detailed analysis on how Smart performs on other non-graph-based datasets?"}}
{"paper_id": "4u0ruVk749", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses an important problem of estimating individualized treatment effects in the presence of unobserved confounders by leveraging diffusion models. The authors propose a novel approach to model unobserved confounders using the reverse diffusion process, which is innovative and theoretically interesting. The empirical results show that the proposed method consistently improves over state-of-the-art methods on important metrics such as sqrt(_PEHE) and _ATE.", "Weaknesses": "The explanation of the proposed method, particularly the use of diffusion models for latent confounding variables, may be complex for readers not familiar with these concepts. Clarity could be improved in terms of how the variational bound is derived and utilized in the experimental setup. Additionally, the abstract lacks detailed information about the benchmark datasets used, and it would be helpful to know how they represent real-world scenarios.", "Questions": "How does the proposed method handle different levels or types of apriori knowledge in practical scenarios? Can you provide more details about the type and characteristics of the synthetic and benchmark datasets used in the experiments?"}}
{"paper_id": "Pa6SiS66p0", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel benchmark for studying multi-modal continual learning (CL), highlighting the potential of leveraging multiple modalities to mitigate catastrophic forgetting in deep neural networks (DNNs). The proposed method for integrating and aligning information across modalities strengthens the case for multi-modal approaches in CL. The framework allows for both single- and multimodal inference, which could have broad implications for future research.", "Weaknesses": "While the paper introduces a benchmark and presents promising findings, additional details on the implementation, evaluation metrics, and comparative analysis against existing methodologies could strengthen the claims. There is also limited discussion on the computational overhead introduced by handling multiple modalities and how it can be managed in practical applications.", "Questions": "1. How does the computational cost of incorporating multiple modalities compare to single-modal approaches in terms of efficiency and scalability?\n2. Could the methodology be extended to include more diverse types of modalities, and what challenges might arise in such extensions?\n3. What specific applications or domains do the authors envision as most benefiting from this multi-modal CL approach?"}}
{"paper_id": "rkplYfqUr0", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel generative framework, Gen-Z, for zero-shot text classification that addresses the limitations of current discriminative prompting methods, showing consistent performance improvements over existing benchmarks. The incorporation of label descriptions allows for the integration of additional context, adding flexibility and robustness to the system. The framework also supports personalization by allowing the integration of author or subject information.", "Weaknesses": "The framework's reliance on natural language descriptions may pose challenges in contexts where descriptions are hard to craft or are not readily available. The experimental validation, while promising, might lack coverage of broader language models beyond the ones tested. The personalization capability, although valuable, may introduce biases if not managed properly.", "Questions": "How does the framework handle cases where generating meaningful or unique label descriptions is challenging? Are there any limitations or potential biases introduced when personalizing classifications with author or reader information?"}}
{"paper_id": "pTU2X9mUBe", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The dataset, LaDe, is large-scale and provides comprehensive information essential for advancing research in last-mile delivery logistics.\n2. It includes diverse data from multiple scenarios and cities, offering potential for a wide range of applications.\n3. The paper validates the dataset on three tasks with classical baseline models, demonstrating its applicability and usefulness.", "Weaknesses": "1. The paper could provide more detailed insights on how privacy concerns are addressed when making such a large, real-world dataset publicly available.\n2. Although the strengths of LaDe are discussed, a comparative analysis with any existing datasets in related domains (if any) could strengthen the contribution claims.", "Questions": "1. How was privacy and data security ensured for the individuals whose data is included in this dataset?\n2. Are there any plans to update LaDe periodically or add more data from different contexts to keep it relevant?"}}
{"paper_id": "uhtQyRrTzY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a significant bottleneck in dense pixel-specific representation learning by proposing a method to generate large-scale multi-view datasets without the need for additional annotations. The automatic dataset curation approach is innovative and demonstrates substantial performance improvements on various dense geometric and object understanding tasks compared to traditional methods reliant on annotated synthetic data or expensive datasets like ImageNet-1K. The results strengthen the potential of scalable data generation for real-world applications.", "Weaknesses": "The methodology, while innovative, may still face limitations in its ability to handle diverse and complex real-world scenarios compared to controlled synthetic environments. Additionally, there's a potential limitation in capturing detailed metadata which might be important for certain applications. The paper could also expand on the details of the dataset curation process to ensure reproducibility.", "Questions": "1. How does the curation process ensure the quality and diversity of multi-view pairs in complex real-world settings without metadata?\n2. What are the computational requirements and processing time for generating these large-scale datasets automatically?\n3. Can this method be easily adapted to work with non-video real-world data sources, or are there specific requirements for the input data format?"}}
{"paper_id": "WZfatbNdLV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces TrASPr, a model that effectively combines transformer architectures and side information to predict RNA splicing outcomes specific to tissue type. This integration of multiple advanced techniques results in a model that significantly outperforms existing models, highlighting its potential impact in the study of alternative splicing.\nAdditionally, the proposed use of Bayesian Optimization (BO) guided by TrASPr to design splicing conditions is innovative, showing promise for generating actionable insights into RNA splicing regulation.", "Weaknesses": "While the proposed TrASPr model and its applications seem promising, the abstract lacks detailed information on the datasets used, the specific metrics of performance improvement, and comprehensive comparisons with existing models. Moreover, while the generative aspect using Bayesian Optimization is intriguing, its practical utility and effectiveness in real-world biological scenarios are not fully validated in the abstract.", "Questions": "1. What specific datasets were used to evaluate TrASPr, and how do they compare to those used by models it outperforms?\n2. Can the authors provide detailed metrics that showcase TrASPr's performance improvement over existing models?\n3. How robust is the Bayesian Optimization approach in generating biologically relevant splicing outcomes?\n4. Are there any experimental validations planned to verify the predictions and designed outcomes generated by TrASPr and the BO model?"}}
{"paper_id": "YkRwadXWHd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The proposed multi-modal framework effectively combines video, keypoints, and optical flow to improve feature representation in CSLR. The introduction of hierarchical knowledge distillation (HKD) to reduce computational costs while maintaining performance is innovative. The extensive experiments on benchmark datasets show the approach achieves state-of-the-art results, demonstrating strong empirical support.", "Weaknesses": "The paper might lack details on the implementation specifics of the HKD framework and how it manages to maintain performance with reduced computational burden. The fusion techniques investigated are not detailed in the abstract, which might be crucial for understanding the novelty and effectiveness of the approach. Some parts of the narrative, such as explaining hierarchical transfer or the exact benefits of the proposed fusion techniques, could be clearer.", "Questions": "1. What specific fusion techniques were investigated, and how do they contribute to the overall performance improvement?\n2. How does the proposed framework handle potential trade-offs between accuracy and computational efficiency?\n3. Can the approach be generalized or transferred to other sign languages or datasets beyond those tested?"}}
{"paper_id": "q4Bim1dDzb", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper introduces a unified framework for inverse rendering that efficiently models geometry, materials, and illumination. The use of unified voxelization and spherical Gaussians streamlines the process significantly. The approach reduces training time dramatically while maintaining high reconstruction quality, supported by extensive experiments.", "Weaknesses": "The paper may lack detailed comparisons with alternative state-of-the-art methods beyond just training time and reconstruction quality metrics. Furthermore, the specifics of the lightweight neural networks used for modeling could benefit from more thorough exploration.", "Questions": "How does UniVoxel perform in terms of generalization to unseen scenes compared to other methods? Also, are there specific limitations in the types of scenes where UniVoxel might struggle in terms of performance or quality?"}}
{"paper_id": "39cPKijBed", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a significant issue of dataset bias in diffusion models, which is crucial for improving the quality of generated samples.\nThe introduction of a time-dependent importance reweighting method is novel and offers a tangible improvement over previous approaches.\nThe theoretical establishment of a connection with traditional score-matching and demonstration of convergence to an unbiased distribution adds robustness to the methodology.\nThe experimental results are compelling and demonstrate superiority over existing baselines across multiple datasets and bias settings.", "Weaknesses": "While the approach and results are impressive, the paper could benefit from a more intuitive explanation of the mathematical foundations for readers less familiar with the domain.\nThe tractability of the objective function is suggested but not thoroughly explored in terms of computational efficiency which might be a concern for practical implementations.", "Questions": "How does the time-dependent reweighting method compare in computational cost to other methods, and is it feasible for large-scale data?\nAre there specific scenarios or datasets where the proposed approach might not perform as well, and what would be the limitations in such cases?"}}
{"paper_id": "aG3EARrrd1", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel predictive statistical model, OSRT, for handling streaming multivariate scattered data, demonstrating improvements in both efficiency and accuracy compared to existing online RBF methods. The dynamic nature of OSRT, which adapts its network depth and utilizes a sparse refinement technique, highlights its potential to handle evolving datasets effectively.", "Weaknesses": "While the proposed model appears promising, the paper could improve in its presentation. More detailed explanations of the algorithmic steps and the underlying theory could enhance clarity. Additionally, the model evaluation, although showing promising results, would benefit from a more comprehensive comparison with a broader range of existing techniques.", "Questions": "1. How does the OSRT model perform in terms of computational complexity compared to other online learning models?\n2. Could you provide more insight into the choice of datasets used for evaluation and their respective characteristics?\n3. Are there any specific limitations or scenarios where OSRT may not perform as expected?"}}
{"paper_id": "Ts95eXsPBc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel approach by incorporating spatial information into transformer-based episodic memory models, which fills a gap in current AI methodologies that usually focus only on the temporal dimension. The proposed method not only enhances accuracy in spatially-related tasks but also demonstrates improved memory utilization efficiency. The Adaptive Memory Allocator presents an innovative way to optimize memory usage through reinforcement learning. The potential applications across various tasks like prediction, generation, and reasoning indicate the broad impact of this research.", "Weaknesses": "The abstract lacks details on the specific implementation of the Spatially-Aware Transformer models and the Adaptive Memory Allocator, which makes it difficult to fully assess the soundness of the proposed methods. Additionally, there is no information on the size and complexity of the datasets or environments used in the experiments, which is crucial for understanding the potential scalability and generalizability of the approach.", "Questions": "How does the incorporation of spatial information specifically enhance the performance of transformer models in place-centric tasks compared to traditional temporal-only models? What are the limitations of the proposed Spatially-Aware Transformers when applied to larger or more complex environments? Can the Adaptive Memory Allocator be generalized to other types of memory systems beyond the scope of the presented tasks?"}}
{"paper_id": "LfhG5znxzR", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel concept of codebook features by quantizing neural network hidden states, showcasing the potential of sparse, discrete representations for improved interpretability. The approach is tested on varied datasets, demonstrating versatility and practical applicability. The open-sourcing of the codebase enhances reproducibility and encourages further research.", "Weaknesses": "While the results are promising, it may be challenging to scale this approach to larger architectures or datasets without performance degradation. The paper could provide more detailed analysis on the trade-offs in performance and interpretability, as well as computational overhead introduced by the vector quantization bottleneck.", "Questions": "How does the training time and computational resource requirement compare with standard models without the vector quantization bottleneck? Are there specific types of neural network architectures or applications where this approach is particularly advantageous or disadvantageous?"}}
{"paper_id": "bUGzjiUsIq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses an important and understudied problem in multi-label classification with partial annotations, which makes the study relevant and valuable. The use of a reinforcement learning approach to tackle the class imbalance and partially annotated data is innovative, and the proposed PAPG framework shows promising results across different tasks. The introduction of local and global rewards to address class imbalance is a novel contribution that adds depth to the methodology.", "Weaknesses": "While the framework appears effective, the abstract does not provide sufficient details about the datasets used or the specific metrics of improvement over baselines, making it difficult to fully assess the empirical soundness. Further, the reliance on reinforcement learning may introduce additional complexity and computational overhead, which could limit practical applications if not managed properly.", "Questions": "1. What datasets and tasks were used to evaluate the framework, and how do they compare in terms of size and diversity?\n2. Can the authors provide more granular results or specific examples of tasks where PAPG most significantly outperforms traditional methods?\n3. How is the effectiveness of local and global rewards quantified, and is their impact on results analyzed independently of other factors?"}}
{"paper_id": "uMAujpVi9m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel approach to pretrain pocket representations for biomedical applications by leveraging high-resolution atomic protein structures and pretrained small molecule encoders. This results in a significant increase in the available data for training, which is a major contribution to the field. The method, ProFSA, demonstrates state-of-the-art performance across a range of important tasks.", "Weaknesses": "The approach relies heavily on the accuracy and quality of existing pretrained small molecule encoders and protein structure databases, which could be a limitation if there are errors or inconsistencies in these resources.", "Questions": "How does the performance of ProFSA compare to existing methods when tested on unseen datasets? Are there any limitations to the types of proteins or ligands that can be used with this approach?"}}
{"paper_id": "uqxBTcWRnj", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel approach that combines Transitional Dictionary Learning with a game-theoretic diffusion model, which effectively discovers visual parts and implicit relations in visual data. The proposed metrics align well with human evaluations, which indicates robustness. The paper also demonstrates a significant improvement over existing unsupervised part segmentation methods by addressing symbolic knowledge extraction in a novel manner.", "Weaknesses": "The presentation could be improved to enhance clarity, especially in the description of the proposed methods and metrics. While the proposed approach and metrics are novel, the paper might lack discussion on the limitations and potential challenges in broader applications, and how well the framework scales with more complex datasets.", "Questions": "How does the TDL framework perform on more diverse and larger-scale datasets beyond the three abstract compositional visual object datasets? Can the proposed approach be easily extended to handle dynamic or temporal data where the compositional parts might change over time?"}}
{"paper_id": "o4CLLlIaaH", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel paradigm for generalizable neural radiance fields by proposing the Generalizable neural Point Field (GPF), which is innovative and addresses several key issues present in previous methods. The use of point-based rendering instead of image-based rendering is a significant shift that shows promise in improving rendering quality and reducing distortions. The model's capability to deliver better geometries and rendering quality in various settings is impressive and demonstrated through extensive experiments.", "Weaknesses": "While the paradigm shift to point-based rendering is promising, the paper may lack clarity in the presentation of the novel concepts, particularly the nonuniform log sampling strategy and how the learnable kernel is spatially augmented with features. Additionally, more detailed ablation studies or comparisons with existing image-based methods could further strengthen the paper.", "Questions": "Can the authors provide more detailed insights or results regarding the computational efficiency of their proposed paradigm, particularly in terms of rendering speed? How does the nonuniform log sampling strategy contribute to the overall performance, and are there specific scenarios where it is especially beneficial?"}}
{"paper_id": "KNzL1nglNB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper tackles the important issue of label insufficiency, proposing a novel method, Label-encoding Risk Minimization (LRM), that can be integrated into existing models. The theoretical analysis relating LRM to ERM is informative and substantiates the foundational basis of their approach. It also demonstrates effectiveness across multiple scenarios including semi-supervised learning, unsupervised domain adaptation, and semi-supervised heterogeneous domain adaptation.", "Weaknesses": "The presentation of the paper could be clearer in illustrating how LRM specifically improves upon the limitations of ERM. Some sections that describe implementation details might benefit from additional clarity to better understand the pipeline. While the authors mention a theoretical analysis, the depth and comprehensibility of this analysis largely determine how convincing the method is.", "Questions": "1. How does the proposed LRM specifically address the issue of neural collapse, and which scenarios benefit the most from this approach?\n2. Can the method be adapted easily to new domains beyond those tested, and what challenges might arise in such adaptations?\n3. When will the code be made available to the public, and what data or benchmarks will it support?"}}
{"paper_id": "pvhyBB86Bt", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses the important problem of initialization in deep neural networks. It extends existing theoretical analyses of the Jacobian's behavior throughout training, providing a theoretically sound explanation for initializations impact. The work builds on recent advances in random matrix theory, making the methods robust to different scenarios such as sparse networks.", "Weaknesses": "The presentation could be clearer, particularly around the explanations of complex mathematical derivations that rely on advanced concepts in random matrix theory. More empirical results would bolster the claims about the applicability to non-iid settings.", "Questions": "What are the specific limitations of the proposed framework in terms of network architectures or training regimes? How significant are the empirical differences between various initialization schemes observed during training in practical scenarios?"}}
{"paper_id": "Mylk5iamJC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper offers a strong theoretical foundation by formalizing OSR in the context of learning theory, demonstrating a connection between CSR and OSR for generative models. The introduction of the neural Latent Gaussian Mixture Model (L-GMM) and its collaborative training algorithm is innovative, effectively addressing the challenge of applying the same model for both CSR and OSR. Experimental results show that the proposed model outperforms traditional discriminative models in both image recognition and segmentation tasks.", "Weaknesses": "While the theoretical formalization and experimental validation are strong, more insights into the limitations or possible challenges in scaling or deploying this model in real-world scenarios could be beneficial. Additionally, while the paper addresses both CSR and OSR, it might be challenging for readers unfamiliar with generative models to fully grasp the nuances of the approach without additional context.", "Questions": "How does the L-GMM model perform when scaled to more complex datasets or non-image recognition tasks? Are there any specific limitations or computational costs associated with the collaborative training algorithm that might affect its practicality in large-scale applications? What are the potential pitfalls or limitations of using a generative model in situations where CSR or OSR is not well-defined?"}}
{"paper_id": "MsOcVFzv8D", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a solid theoretical foundation for multi-domain text classification by employing margin discrepancy and Rademacher complexity to establish a new generalization bound. This theoretical grounding is a significant contribution to the field as it addresses a prevalent challenge in the design of MDTC algorithms. Additionally, the proposed margin discrepancy-based adversarial training (MDAT) method shows promising empirical results, surpassing state-of-the-art baselines on benchmark datasets.", "Weaknesses": "While the theoretical analysis is robust, the paper may benefit from a more comprehensive explanation of the implementation details of the MDAT approach, as well as broader empirical validation across more diverse datasets. Additionally, the presentation could be improved to enhance clarity, particularly in the theoretical sections, where complex concepts might be difficult for readers to digest without significant background knowledge.", "Questions": "How does the margin discrepancy-based adversarial training method compare with other adversarial training methods beyond the two reported benchmarks? Are there specific settings or domains where MDAT particularly excels or falls short? Furthermore, can the insights from this theoretical framework be applied to other areas of domain adaptation beyond text classification?"}}
{"paper_id": "fBlHaSGKNg", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The proposed UPA methodology effectively addresses a key challenge in Semi-Supervised Learning (SSL) regarding the efficient selection of samples for annotation under budget constraints.\n2. The use of a modified Frank-Wolfe algorithm to minimize $$-Maximum Mean Discrepancy ($$-MMD) is innovative and extends the capabilities of low-budget learning.\n3. Empirical results demonstrate the efficacy of UPA in improving the performance of popular SSL methods over several existing Active Learning (AL) and SSAL approaches.", "Weaknesses": "1. The novelty of the $$-Maximum Mean Discrepancy and its direct impact on the generalization ability require more detailed theoretical grounding and clarity.\n2. Presentation could benefit from additional clarity on the implementation details of the modified Frank-Wolfe algorithm and its integration with current models.\n3. There is a lack of detailed analysis on why UPA outperforms other methods in terms of sample diversity and representativeness.", "Questions": "1. How does UPA balance the trade-off between selecting diverse samples and ensuring representativeness in a real-world scenario where data may have skewed distributions?\n2. Can the proposed methodology be generalized to tasks outside the scope of SSL, such as supervised learning frameworks, while still maintaining its efficiency under annotation constraints?\n3. Are there any particular datasets or conditions under which UPA may not perform as effectively, and how do these scenarios impact the evaluation in the study?"}}
{"paper_id": "HwQ8NVvdmm", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents a novel approach to integer-based fully quantized training, which addresses the accuracy degradation issue associated with aggressive quantization.\n2. The use of Hadamard transforms for low-precision integer matrix multiplications is innovative and demonstrates promising results in maintaining accuracy with reduced bit-width.\n3. The extensive evaluation on human activity recognition datasets and CIFAR100 strengthens the empirical validation of the proposed method.", "Weaknesses": "1. The method may require further exploration and validation over a wider range of tasks and datasets to ensure generalizability and robustness.\n2. The specific impact of stochastic rounding and tiled matrix multiplication on different tasks is not comprehensively analyzed in the abstract.\n3. The overall complexity and potential trade-offs of implementing the proposed technique on various edge platforms are not discussed.", "Questions": "1. How does the proposed technique perform on more complex datasets beyond CIFAR100 and human activity recognition tasks?\n2. What are the implications of adopting the proposed quantization strategy on edge devices in terms of latency and power consumption?\n3. Can the effectiveness of stochastic rounding and tiled matrix multiplication be isolated to understand their individual contributions to the overall performance?"}}
{"paper_id": "cZo6pDtDZr", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents significant improvements in the estimation and testing of collision probability with respect to differential privacy constraints. Notably, it reduces sample complexity, offering a more efficient solution compared to previous methods. The introduction of a sequential testing algorithm that handles unknown probability differences is also a key advancement. These contributions are potentially impactful in fields where sample efficiency and data privacy are critical.", "Weaknesses": "The paper may need more extensive empirical comparisons across different scenarios to fully verify the practical advantages of the proposed methods. Moreover, details on handling various distribution types, particularly non-uniform ones, may not be fully addressed in the scope of existing tests.", "Questions": "1. How does the proposed algorithm perform compared to others under non-uniform distributions?\n2. Are there specific application domains where the efficiency of the new methods makes a significant difference beyond theoretical improvements in sample complexity?"}}
{"paper_id": "F7QnIKlC1N", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses an important problem in molecular modeling by leveraging Graph-Transformers, a novel approach that can potentially expedite the determination of molecular ground-state conformations. The proposed Molecule Structural Residual Self-Attention mechanism appears to offer strong performance benefits, as demonstrated by results on Molecule3D and QM9 datasets.", "Weaknesses": "The paper does not provide substantial details on the internal workings of the Molecule Structural Residual Self-Attention mechanism, making it difficult to fully assess its novelty and implementation challenges. Furthermore, while the results are promising, the comparative analysis against existing methods lacks depth, particularly in terms of computational efficiency and scalability.", "Questions": "How does the computational efficiency of the proposed method compare to traditional DFT approaches in terms of speed and resource usage? Can the Molecule Structural Residual Self-Attention mechanism be easily integrated into other models, and what are the expected performance gains?"}}
{"paper_id": "8JCn0kmS8W", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "WavJourney presents an innovative approach by integrating Large Language Models with various audio generation models to produce storytelling audio from textual descriptions, which is a novel contribution. The experimental results exhibit state-of-the-art performance on text-to-audio generation benchmarks. The paper is well-organized, clearly presenting the methodology and its implications. Public availability of the code and data enhances reproducibility and encourages further research.", "Weaknesses": "The reliance on Large Language Models for generating structured audio scripts may limit flexibility as LLMs might not handle every context perfectly, especially in far less common scenarios. The method's effectiveness in generating audio for highly dynamic or unconventional narratives remains to be seen. There are also potential challenges regarding the integration of various audio models seamlessly.", "Questions": "1. How well does WavJourney handle audio generation for less structured or abstract text prompts?\n2. Are there limitations in the types of audio elements (e.g., genres or specific sound effects) that the current framework can handle effectively?\n3. Can WavJourney's framework be adapted to support real-time audio generation, such as interactive games or live performances?"}}
{"paper_id": "XbLffB0T2z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses an important issue in machine learning security: the transferability of data poisoning attacks across different learning paradigms. The proposed method, Transferable Poisoning, is shown to produce high-frequency perturbations that improve the transferability of poisoned samples, enhancing attack effectiveness across various learning algorithms.", "Weaknesses": "The paper may rely heavily on assumptions about the practicality and scalability of the proposed attacks, which might not hold in all real-world scenarios. Additionally, the experimental validation, while extensive, might not cover all relevant conditions and use cases where the proposed method would be applicable.", "Questions": "How is the performance of the proposed Transferable Poisoning method affected by different types of noise in the dataset? Are there any specific conditions under which this method might not perform as well?"}}
{"paper_id": "4y3GDTFv70", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "ODSgo2m8aE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a relevant problem in GNN training by focusing on fairness and the effects of biases inherent in graph data. The use of the Lipschitz bound to limit output perturbations and enhance fairness is a novel application that could prove beneficial for the field. The theoretical grounding provides an insightful take on the effect of biases and output stability.", "Weaknesses": "While the paper introduces the concept of using Lipschitz bounds in GNNs for fairness, it might lack clarity in explanations and detailed demonstrations of practical effectiveness or applications. The methodology for calculating the Lipschitz bound in the context of GNNs may not be entirely clear, and empirical validations might require more robust datasets or comparative baselines.", "Questions": "1. How does the Lipschitz bound specifically differ when applied to non-Euclidean spaces such as those handled by GNNs compared to Euclidean spaces?\n2. What specific datasets were used for experimentation, and how were they selected to ensure the presence of biases?\n3. Could the authors elaborate on how their theoretical model directly impacts real-world applications or offer practical guidelines for practitioners?"}}
{"paper_id": "RlfD5cE1ep", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper extends prior work on self-supervised learning by considering the effects of feature normalization within the learning dynamics. By introducing the cosine loss in place of the L2 loss, the study sheds light on the role of feature normalization in preventing representational collapse. This approach offers a deeper theoretical understanding which could benefit the field.", "Weaknesses": "While the included theoretical analysis is insightful, it remains to be seen how it translates to empirical improvements or practical applications in real-world scenarios. The analysis might be overly specialized, and the results are primarily theoretical without demonstrating clear empirical gains.", "Questions": "How does feature normalization specifically alter the empirical performance across diverse datasets compared to traditional approaches? Can the sixth-order dynamics that emerge through the introduction of the cosine loss be rigorously validated through extensive empirical studies?"}}
{"paper_id": "18TfucMNTr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel approach to training deep neural networks by incorporating noisy training as an optimization technique, which is a special case of optimization by continuation. This could help in avoiding local minima and improve training outcomes for deep architectures.", "Weaknesses": "The abstract lacks specific details about the experiments conducted or the quantitative improvements achieved. It remains somewhat high-level and does not offer detailed insights into the implementation or evaluation on various benchmarks.", "Questions": "How does the proposed method specifically handle saddle points differently compared to traditional optimization methods? What datasets or benchmarks were used to validate the claims?"}}
{"paper_id": "JGTC6WgO4T", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel framework for multi-source black-box domain adaptation, addressing privacy and security concerns by avoiding the use of source data directly. The development and application of the Pseudo label Refinery Network (PRN) demonstrate an innovative approach to enhancing unsupervised domain adaptation. The theoretical support for the performance of the proposed framework is a significant strength, as is the collection of empirical results demonstrating the method's effectiveness across several benchmarks.", "Weaknesses": "While the approach is innovative, the novelty is somewhat limited to the combination of existing ideas with the PRN. The paper might benefit from adding more detailed analysis to justify the choices made within the PRN design to further enhance clarity and impact. There may also be additional discussions needed on the limitations and potential generalization of the method to other domains or settings beyond those tested.", "Questions": "Can the Pseudo label Refinery Network be generalized to other types of domain adaptation problems beyond the benchmarks tested in the paper? What are the limitations of the method in cases with highly divergent source domains, and how effectively does it handle such situations?"}}
{"paper_id": "fQHb1uZzl7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel Transformer-based architecture that effectively integrates feature and cost aggregation in dense matching tasks. The approach takes advantage of self- and cross-attention mechanisms and designs an interleaved process that operates from coarse to fine resolutions, which is shown to significantly improve the accuracy of correspondence estimation. The proposed method is evaluated on standard benchmarks and demonstrates substantial improvements in performance, showcasing the potential of the integrative approach.", "Weaknesses": "While the architecture itself is effective, the paper does not fully explore the limitations or potential challenges of the method, such as computational complexity or scalability to larger datasets. Additionally, the novelty of simply combining feature and cost aggregation using Transformers, while showing improvement, may not be groundbreaking as the techniques themselves are extensions of existing frameworks.", "Questions": "1. How does the computational complexity and scalability of the proposed model compare to other existing methods in practice?\n2. Can the framework be adapted easily to other domains beyond semantic and geometric matching?\n3. Are there specific scenarios or datasets where this approach may not perform well or where certain trade-offs are necessary for practical application?\n"}}
{"paper_id": "YjG29CIOP7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach to addressing the issues of Task-Distribution Shift and Task-Distribution Corruption in Data-Free Meta-Learning.\n2. The proposed methods, TEAPOT and ROSY, provide innovative strategies to enhance robustness and generalization.\n3. The experimental results demonstrate the effectiveness of the proposed methods compared to existing baselines.\n4. The concept of maintaining memory of old tasks and adaptive model selection enhances the robustness of DFML systems.", "Weaknesses": "1. The complexity of the methods and their integration with existing DFML techniques may increase the computational overhead.\n2. The paper might lack a detailed discussion on the limitations and potential scenarios where the proposed methods might fail.\n3. Clarity in the presentation of results and methods could be improved to make the contributions more accessible to a broader audience.", "Questions": "1. How does the memory-based management in TEAPOT affect the scalability of the approach when dealing with a large number of tasks or models?\n2. In what scenarios does ROSY perform poorly, and how does the reliability score adapt in such cases?\n3. Are there any specific types of model poisoning attacks that ROSY is not robust against, and how can these be addressed in the future?"}}
{"paper_id": "DmDpu3r8iU", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel framework, the Value Understanding Measurement (VUM), which aims to quantitatively assess the extent to which Large Language Models understand human values. The use of the Schwartz Value Survey and the creation of a dialogue dataset with GPT-4 add depth to the evaluation process. The investigation into the 'know what' and 'know why' dimensions provides a unique perspective on the alignment of LLMs with human values.", "Weaknesses": "The concept of evaluating understanding in LLMs can be complex and subjective, and while the VUM framework attempts to quantify this, the actual measurements of 'know what' and 'know why' might still be somewhat interpretative. It is unclear how scalable the proposed dialogue dataset is and whether it comprehensively covers the breadth of human values that LLMs would need to understand in real-world scenarios.", "Questions": "How does the framework ensure the broad and representative coverage of the possible values that LLMs should understand? Can the VUM be adapted for LLMs that have been trained on non-English datasets or those that incorporate cultural contexts beyond what is typically present in datasets like the Schwartz Value Survey?"}}
{"paper_id": "kXHEBK9uAY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel and efficient hierarchical planning method that addresses key challenges faced by diffusion-based generative models, particularly in computational efficiency and generalization for long-horizon tasks. The empirical evaluations on standard offline reinforcement learning benchmarks demonstrate superior performance over existing methods, showcasing both the practicality and scalability of the approach.", "Weaknesses": "The paper could benefit from more clarity on implementation details of the jumpy planning strategy and how it specifically contributes to improvements in training and planning speed. Additionally, while the hierarchical approach is effectively motivated, more clarity on potential limitations or scenarios where this method might underperform would provide a more balanced view.", "Questions": "How does the proposed Hierarchical Diffuser handle different levels of noise in the offline datasets compared to existing methods? Are there specific types of tasks or datasets where the approach might not perform as well? Also, can the method be easily adapted to online reinforcement learning settings?"}}
{"paper_id": "i7LCsDMcZ4", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper introduces novel spiking-layer relevance propagation methodologies (SLTRP and SLRP) for the first time, advancing the field of SNNs by addressing spatial representation issues.\n2. The proposed EventRPG method demonstrates strong empirical performance, achieving state-of-the-art results on multiple datasets.\n3. The concept of generating saliency maps for SNNs is a promising direction with potential applications beyond the current scope.", "Weaknesses": "1. The paper may lack detailed theoretical insights into the proposed SLTRP and SLRP rules, which could limit understanding of their mechanisms and generality.\n2. More comprehensive comparison with existing methods (beyond the ones reported) could strengthen the case for the proposed approach.\n3. The presentation could be improved by providing deeper qualitative insights or visualizations of the generated saliency maps.", "Questions": "1. How does the proposed relevance propagation compare with traditional methods in terms of computational complexity, especially under real-time scenarios?\n2. What are the potential limitations of the proposed methods when applied to other spiking neural network architectures or different datasets?\n3. Could you provide more examples or analysis on how the generated saliency maps contribute towards improving SNN explainability?"}}
{"paper_id": "jHdz0CIS2y", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel self-supervised training strategy for voice conversion that improves model performance by incorporating self-synthesized examples. The approach creatively avoids the explicit disentanglement of speech attributes, thus retaining more nuanced characteristics such as accent and emotion. Extensive experiments demonstrate that the proposed method achieves state-of-the-art results in zero-shot voice conversion with high metrics in naturalness, speaker similarity, and intelligibility.", "Weaknesses": "The abstract does not provide detailed insights into the computational complexity or scalability of the proposed approach. Additionally, potential limitations of the approach such as the types of voices or languages for which the method might be less effective are not discussed.", "Questions": "How does the self-synthesized example creation process affect computational efficiency compared to traditional methods? Are there specific types of voices or languages where this method performs less effectively, and how does it handle extreme accents or emotions?"}}
{"paper_id": "NkYCuGM7E2", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper proposes the innovative use of Large Language Models (LLMs) for decision-making in autonomous driving scenarios. The approach is notable for integrating high-level cognitive reasoning capabilities with low-level control systems, potentially improving the adaptability and robustness of autonomous driving systems to complex driving environments. The work could inspire future research in combining LLMs with traditional control systems in the context of safety and decision-making.", "Weaknesses": "The abstract does not provide sufficient details on the algorithms or experiments, making it difficult to assess the novelty and practical applicability of the methods proposed. There is no mention of how interpretability is achieved or evaluated, which is crucial for real-world applications. LLMs' resource requirements and limitations, such as computational overhead, are not discussed, which could affect the feasibility of deploying such systems in real-time applications.", "Questions": "1. How are the LLMs integrated with existing low-level controllers, and what specific algorithms or techniques are used? \n2. What datasets or simulation environments were used for the experiments, and what was the baseline for comparison? \n3. Can the system handle or learn from novel or unseen obstacles that weren't part of the training environment? How is the performance evaluated in such cases?"}}
{"paper_id": "RtDok9eS3s", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a thoughtful simplification of the transformer block architecture without compromising performance or training speed. The empirical results show a significant improvement in training throughput and parameter efficiency, which is a strong contribution to the field. The use of signal propagation theory to guide the simplification is a well-justified approach.", "Weaknesses": "The presentation of the work could be improved in terms of clarity and organization, especially in detailing the modifications made and their individual contributions to the overall improvements observed. Additional clarity on how the simplified model affects downstream tasks could also enhance the impact.", "Questions": "How do the simplified transformers perform on more complex downstream tasks beyond basic language modeling? Are there specific scenarios where the modified architecture does not perform well compared to standard transformers?"}}
{"paper_id": "YCPDFfmkFr", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses the complex issue of differentiability in optimization layers, particularly in convex quadratic programming (QP) layers, offering a unified approach to handle both feasible and infeasible QP solutions. The method leverages primal-dual augmented Lagrangian techniques, which appear to enhance the expressive capability of QP layers, potentially leading to improved predictive performance in learning tasks. Furthermore, the open-source software package QPLayer aids in the practical implementation of these ideas.", "Weaknesses": "The paper does not sufficiently detail the potential limitations or constraints of the proposed approach, such as computational overhead or scalability concerns. The presentation, while focused on technical content, might benefit from improved clarity and organization to make the complex concepts more accessible to readers unfamiliar with the domain.", "Questions": "How does the computation time of the proposed method compare with existing alternatives, particularly in large-scale scenarios?\nCan the differentiability approach be generalized to other types of optimization layers beyond convex QP layers?\nAre there specific applications or case studies where the enhanced QP layers have shown significant advantages over traditional methods?"}}
{"paper_id": "ueTdErd5Ib", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The method introduced in the paper offers a novel approach to combining learning with the optimization process, effectively tackling uncertainty in stochastic settings. The theoretical guarantees enhance the trustworthiness of the results, and the experimental outcomes, particularly the significant improvement in worst-case scenarios, underscore the method's practical utility.", "Weaknesses": "Though theoretically sound, the presentation could benefit from clearer exposition for broader accessibility. Some aspects of implementation might require more detailed clarification to better understand the practical implications and limitations of the approach.", "Questions": "How does the method scale with more complex datasets or larger feasible regions? Are there any specific limitations in terms of computational cost or implementation that might impact its application in other domains beyond the electricity generation problem?"}}
{"paper_id": "sDmjlpphdB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents an innovative approach to prompt design in LLMs using a Mixture-of-Expert paradigm, which successfully divides complex problem spaces into simpler sub-tasks. 2. The two-phase process for demo and instruction assignment is grounded in theoretical connections and showcases significant improvement over existing methods. 3. Extensive benchmarking demonstrates that the proposed method improves performance, offering up to 43% gains over the prior art.", "Weaknesses": "1. The complexity of the proposed methodology might pose challenges for implementation and scalability in real-world applications. 2. The dependence on semantic similarity for demo clustering requires reliable and efficient metrics, which are not extensively discussed in the paper. 3. The joint search process for instruction optimization may have high computational costs, which could limit its practicality.", "Questions": "1. How sensitive is the method to the choice of demos and their clustering process? Does the method require a specific number of clusters to be effective?\n2. Can the framework handle dynamically changing task requirements, or is it strictly designed for static task definitions?\n3. What are the computational overheads involved in the joint search process, and how do they impact the scalability of the method to larger or more complex tasks?"}}
{"paper_id": "Mdk7YP52V3", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides a theoretical explanation of the difficulties encountered in heteroskedastic neural regression models, using a novel approach grounded in statistical physics. It successfully demonstrates phase transitions in these models, which has implications for their regularization. This not only deepens the understanding of model behaviors but also offers a more efficient scheme for optimizing regularization.", "Weaknesses": "The presentation could be improved for clarity, especially for readers who may not have a background in statistical physics, potentially limiting the accessibility of the insights provided. The paper might benefit from additional empirical validation of the proposed regularization optimization scheme.", "Questions": "How well does the proposed regularization scheme generalize across different dataset types and sizes? Are there any potential limitations in applying this theoretical framework to other types of machine learning models?"}}
{"paper_id": "YGTSLDAPqb", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a relevant problem in domain adaptation and presents a novel method, \"Connect Later,\" to improve OOD performance. The approach leverages self-supervised pretraining with targeted fine-tuning, demonstrating significant improvements over standard fine-tuning in real-world datasets. The experiments are well-conducted, showing state-of-the-art results for specific tasks.", "Weaknesses": "While the concept of 'Connect Later' is promising, the paper may lack sufficient detail on how targeted augmentations are specifically designed and implemented. Additionally, the improvements in one of the datasets (iWildCam-WILDS) are relatively modest, indicating the need for further examination of the method's generalizability across diverse domains.", "Questions": "Can the authors provide additional insights into the process of designing 'targeted augmentations' and their role in different domains? How transferable is the 'Connect Later' method to other types of datasets beyond the ones tested here?"}}
{"paper_id": "JTcaziw7G1", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach to Private Retrieval Augmented Generation (PRAG) that leverages multi-party computation (MPC) to maintain privacy during neural information retrieval. The use of an MPC-friendly protocol for inverted file approximate search (IVF) is particularly innovative, providing sublinear communication complexity and ensuring that neither database content nor queries are exposed. This represents significant progress in privacy-preserving data access and presents clear practical significance for secure data retrieval.", "Weaknesses": "The presentation of the paper could be improved. There is a need for more detailed explanations regarding certain technical aspects of the proposed protocols and algorithms, including clarity on how the MPC protocol balances privacy and efficiency. Additionally, empirical evaluation across different applications or comparative benchmarks against existing private information retrieval systems would strengthen the practical applicability claims.", "Questions": "1. How does the proposed PRAG approach compare to other state-of-the-art private retrieval systems in terms of performance and scalability?\n2. Can the approach be adapted or extended to support more complex retrieval tasks or domain-specific queries?\n3. What are the potential limitations in terms of the types of queries or datasets where this approach can be applied effectively?"}}
{"paper_id": "lEkFq4RUCX", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed DDF metric appears to be a novel and effective approach to measuring the dissimilarity between 3D point clouds. The paper demonstrates the metric's applications in various tasks, achieving higher accuracy in a computationally efficient manner.", "Weaknesses": "The presentation of the paper might need improvements to communicate the concept more clearly to a broader audience. Additionally, the reliance on reference points may limit the applicability of the metric in certain scenarios or necessitate careful selection of these points.", "Questions": "How sensitive is the DDF metric to the choice of reference points, and are there guidelines for selecting them? What are the limitations of this approach compared to existing point cloud metrics?"}}
{"paper_id": "Fq8tKtjACC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The model phi-1 achieves notable pass rates on coding benchmarks despite its smaller size, demonstrating efficient use of resources and effective design choices. The integration of both web-sourced 'textbook quality' data and synthetically generated exercises suggests a thoughtful approach to data curation and training.", "Weaknesses": "The paper lacks detailed comparative analysis with state-of-the-art models besides the provided benchmarks. Also, the potential limitations or drawbacks of using synthetically generated data alongside real data are not discussed in depth.", "Questions": "1. How does phi-1 compare in terms of training efficiency and generalization capabilities to other similarly-sized models? 2. What specific strategies were employed to select the 'textbook quality' data from the web? 3. Are there any domains or tasks where phi-1 underperforms, and if so, what are the potential reasons for the same?"}}
{"paper_id": "8fQlGQkj0S", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a rigorous analysis of in-context learning, differentiating between task retrieval and task learning. The work introduces generative models for pretraining data and in-context samples which contribute to understanding ICL at a theoretical level.", "Weaknesses": "While the theoretical contributions are substantial, the presentation could be made clearer, especially regarding the practical implications of the unique phenomenon in task retrieval. Moreover, the paper might benefit from additional experimental evaluations beyond numerical computations.", "Questions": "What are the practical implications of the finding that more in-context examples can worsen task retrieval? How does this finding influence the design of in-context learning systems?"}}
{"paper_id": "hRos9WldRK", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel method, Learning to Bootstrap (L2B), that effectively addresses the challenge of learning with noisy labels by dynamically adjusting weights and incorporating implicit relabeling.\n2. L2B shows compatibility with existing noisy label learning approaches and demonstrates competitive performance on benchmark datasets.\n3. The paper presents extensive experimental validation on several datasets, showcasing its broad applicability and effectiveness in mitigating noisy label challenges without requiring extensive validation samples.", "Weaknesses": "1. While the method shows promising results, the paper could benefit from more in-depth theoretical analysis or mathematical proof regarding the dynamic weighting mechanism beyond empirical evidence.\n2. The dependency on self-predictions for pseudo-labeling might be problematic in scenarios where initial model predictions are poor due to severe noise, which is not thoroughly addressed.", "Questions": "1. How does L2B ensure that the initial self-predictions do not adversely impact the training when label noise is very high?\n2. Can L2B be extended or adapted for other machine learning domains outside of computer vision, such as NLP or reinforcement learning, where label noise also presents a significant challenge?\n3. Are there any scenarios where L2B might struggle, particularly in datasets with extremely high levels of noise or with highly imbalanced classes?"}}
{"paper_id": "ttRSBZiCiu", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel WaveFluid model which enhances syntheses tasks by leveraging architectural improvements and adversarial training. The approach taken provides a more efficient way of conducting speech synthesis, particularly through the use of reparameterization techniques, which appear promising given the reported results in terms of sample quality and inference speed.", "Weaknesses": "The explanation of how the WaveFluid model precisely improves upon existing models through adversarial training and reparameterization could be more detailed. Additionally, there is limited information provided concerning the comparison metrics and evaluation criteria against existing vocoders, which are crucial for substantiating claims of competitiveness.", "Questions": "1. Can you provide more detailed metrics or qualitative results on how the WaveFluid model was assessed in comparison to existing models? \n2. What specific challenges does the proposed adversarial training approach address that were not handled by linear PDE solutions?\n3. Was there any specific reason for choosing mel-spectrogram as the conditioning factor in your experiments?"}}
{"paper_id": "WYsLU5TEEo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel framework that successfully combines interpretability and robustness against adversarial attacks using image-to-image translation GANs, addressing two pressing issues in neural image classifiers simultaneously. The framework demonstrates significant potential by producing competitive saliency maps and improving model resilience to adversarial attacks.", "Weaknesses": "The proposed method may require complex training procedures involving GANs, which can be computationally expensive and hard to reproduce. The evaluation is limited to specific tasks like concrete crack segmentation and fruit defect detection, which might not generalize across different domains. Clarification on the method's scalability and applicability to other classification tasks would strengthen the argument for its broader utility.", "Questions": "Would this framework scale effectively to more complex datasets and tasks beyond concrete crack segmentation and fruit defect detection, such as those found in other domains with richer semantic content? How does the model handle input images containing multiple overlapping classes, and does it maintain robustness in such scenarios?"}}
{"paper_id": "KKZaj2QS3G", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses key gaps in time series representation learning by explicitly considering noise, a critical factor affecting task efficacy. The novel sampling strategy and the proposed encoder architecture using dilated convolution within the Inception block present significant contributions that enhance scalability and robustness. The method's ability to outperform existing state-of-the-art techniques across multiple tasks indicates the practicality and effectiveness of the approach.", "Weaknesses": "Though the paper introduces innovative methods, the implementation details of the novel sampling strategy and the proposed encoder architecture are absent, making it challenging to assess reproducibility. The presentation could be improved, particularly in discussing how noise is specifically addressed and integrated into the contrasts and learning process.", "Questions": "What datasets were used to evaluate the performance improvements claimed? Are there specific types of noise the model is better at handling? How does the proposed sampling strategy differ fundamentally from other existing strategies, and can you provide more details on its implementation?"}}
{"paper_id": "LWDRiFzbHQ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed method, vicinal risk proxy (VRP), introduces a novel way of assessing model generalization by utilizing neighboring test samples, which effectively addresses the limitations of existing indicators that operate on individual samples. This approach provides a robust means of predicting model accuracy on out-of-distribution test sets without ground truth labels. The method is applicable to existing indicators like average confidence and effective invariance, providing consistent improvements and showing strong correlation with model accuracy across various test scenarios.", "Weaknesses": "While the VRP method offers promising results, the effectiveness of the approach relies on the assumption that neighboring samples share similar model responses, which may not always hold true, especially in highly heterogeneous datasets. The paper's presentation could be improved by providing more comprehensive empirical results and case studies, as well as a clearer explanation of the computational efficiency and scalability of VRP in practice.", "Questions": "1. How does the VRP method handle cases where neighboring samples are not available or are too diverse to provide reliable model responses?\n2. Can VRP be applied effectively to other types of machine learning models beyond classification tasks, such as regression models?\n3. What is the computational complexity of the VRP approach compared to traditional single-sample-based methods? How does it scale with large datasets?"}}
{"paper_id": "Yp01vcQSNl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a gap in existing graph transformer models by introducing a novel architecture that explicitly accounts for edge directionality in directed graphs. This approach highlights significant advancements over prior work, as it shows state-of-the-art performance on newly proposed directional graph datasets. The dual encoding strategy and directional attention module are well-motivated and provide a robust mechanism for capturing directionality. The empirical evaluations and the introduction of new datasets where directionality is intrinsic are commendable contributions.", "Weaknesses": "While the proposed method is well-explained, the abstract does not provide detailed insights into the complexity of the approach or potential limitations regarding scalability to very large graphs. Information on how this dual encoding interacts with larger networks and its computational overhead remains unclear. Some further validation experiments might be necessary to fully guarantee the general applicability of the proposed methods.", "Questions": "How does the proposed architecture scale with increasing graph size and complexity? Are there any specific computational trade-offs or limitations when implementing the dual encoding mechanism? Also, could you elaborate on how the method performs on graphs where directionality is not as crucial and provide comparisons in those scenarios?"}}
{"paper_id": "LndMyiBl3n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces the Modified Silhouette Score (MSS) as a novel measure for assessing graph quality, which is shown to be generalizable across different homophily levels. The proposed SheAttack efficiently reduces node distinguishability without prior knowledge of labels or the victim model, which is a significant advantage for real-world applications.", "Weaknesses": "While the paper presents a new method for restricted black-box attacks, it lacks detailed comparison with state-of-the-art methods under similar constraints. The computational efficiency and scalability of SheAttack in large-scale networks are not thoroughly evaluated.", "Questions": "How does the Modified Silhouette Score compare to existing measures of graph quality in terms of computational cost and effectiveness? Can the approach be extended to address more complex graph structures or dynamics beyond what is tested in the synthetic and real-world experiments?"}}
