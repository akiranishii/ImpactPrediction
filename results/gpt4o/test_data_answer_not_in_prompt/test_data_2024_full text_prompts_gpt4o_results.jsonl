{"paper_id": "B0OwtVEejJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel method (TangleNAS) that effectively combines the strengths of single-stage gradient-based NAS with the memory efficiency of weight-entangled spaces. It provides extensive empirical evidence demonstrating the improved performance over traditional methods across various search spaces and tasks. The approach shows a significant reduction in memory consumption while maintaining high search efficiency.", "Weaknesses": "The paper may not fully address potential limitations and scalability issues when applying the proposed method to very large architectures or datasets. Some of the empirical details, such as specific training hyperparameters and their impact on results, could be explored further.", "Questions": "How does TangleNAS handle extremely large search space sizes, and what are its limitations in practice for real-world large-scale applications? Could the integration of weight-entanglement in cell-based spaces be further optimized or have adverse side effects?"}}
{"paper_id": "ZlEtXIxl3q", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper offers an innovative approach to modeling global epistasis using contrastive losses, specifically the Bradley-Terry loss. It presents a convincing argument for the efficacy of this loss function over traditional MSE loss in capturing the latent fitness function from incomplete data, supported by relevant simulations and experiments.", "Weaknesses": "The connection between contrastive learning and its robustness to global epistasis is not deeply theorized, lacking strong theoretical backing. The paper's results rely heavily on simulations and benchmark tasks without addressing complex noise patterns found in real-world data. The presentation is dense and could be clearer in explaining some complex concepts.", "Questions": "How does the contrastive loss perform in the presence of more complex noise often found in empirical assays? Are there specific types or architectures of neural networks that could further improve the results obtained using the Bradley-Terry loss in these scenarios?"}}
{"paper_id": "6yJuDK1DsK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "FEATHER provides a lightweight approach for lifelong test-time adaptation by adding efficient adapters to a pre-trained model, maintaining performance while reducing the number of trainable parameters. It does not require access to the source data, which makes it easier to integrate into existing systems.", "Weaknesses": "The evaluation mainly focuses on benchmark datasets with predefined corruptions, which might not fully capture real-world scenarios. The paper could explore the impact of domain shifts in more diverse applications.", "Questions": "How does the performance of FEATHER compare to other methods in completely unseen environments where domain characteristics significantly differ from the training and benchmark data?"}}
{"paper_id": "lt6xKGGWov", "review": {"Soundness": 3, "Presentation": 2, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel approach for feature selection using neural estimation of mutual information, which shows promising results in comparison with existing methods.", "Weaknesses": "The presentation is somewhat disorganized, with sections missing necessary clarity. There is over-reliance on synthetic data without real-world validation. Some technical details, like algorithm steps and parameter choices, could be further elaborated.", "Questions": "How well does the method scale with large datasets in practice? Can the approach handle missing or noisy data effectively in real-world scenarios?"}}
{"paper_id": "Bvrc6kobWd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel perspective on understanding the role of margins in contrastive learning through gradient analysis. It identifies multiple specific effects of margins on learning and evaluates these through extensive experiments.", "Weaknesses": "The assumptions made, such as using only cosine similarity and one-hot targets, might limit the generality of the findings. The analysis does not delve deeply into all identified effects, like scaling gradients by the ratio, leaving some insights unexplored.", "Questions": "Can the gradient analysis insights on margins be extended to non-cosine similarity-based contrastive learning methods or those with non-one-hot targets?"}}
{"paper_id": "FJjHQS2DyE", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel framework called Conditional Adversarial Support Alignment (CASA) for addressing Unsupervised Domain Adaptation (UDA) with label shift. It provides a new theoretical target risk bound and demonstrates superior performance over other state-of-the-art methods through extensive experiments. The theoretical foundation is strong and the proposed method addresses a significant limitation in existing approaches by focusing on conditional support alignment.", "Weaknesses": "The presentation could be improved for better clarity, particularly in explaining the theoretical contributions and their implications. Some sections of the paper, especially those dealing with complex mathematical concepts, may require multiple readings for full comprehension. Additionally, there might be concerns regarding the scalability of the approach for larger datasets or more complex domain adaptation settings.", "Questions": "How does the approach scale with high-dimensional data or a large number of classes? Are there any limitations in terms of computational cost or efficiency when applying CASA to real-world datasets with significant label shift?"}}
{"paper_id": "xelrLobW0n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces TRACE, a new benchmark for evaluating continual learning in language models, which is important for understanding model behavior in sequential task learning. The benchmark covers a wide variety of tasks including domain-specific knowledge, multilingual capabilities, and mathematical reasoning. The introduction of Reasoning-augmented Continual Learning (RCL) as a method to mitigate catastrophic forgetting is a valuable contribution.", "Weaknesses": "The evaluation focuses mainly on specific metrics without extensive exploration of the broader impact on various types of language models. There could also be more clarity in the presentation of some results and deeper discussions on potential limitations or biases introduced by the TRACE benchmark. The results might not generalize well to all types of LLMs due to the focus on only a few key models.", "Questions": "How does TRACE ensure that tasks remain challenging as models evolve and potentially get pre-exposed to training data? What are the specific criteria for the inclusion of tasks in TRACE, and how dynamic is this benchmark as LLMs develop? Are there plans to expand RCL to other models or tasks?"}}
{"paper_id": "bYQkOPvgDw", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses the important problem of label noise in Graph Neural Networks by proposing a novel probabilistic graphical model. The method is demonstrated to be robust across different noise types and graph heterophilies, showing promising results in high noise scenarios.", "Weaknesses": "The paper could benefit from a clearer explanation of the technical details, particularly in the methodology section. Some variance in results is observed, and the dependency on hyperparameters and model configuration needs further exploration.", "Questions": "How does the proposed model scale with significantly larger datasets, and is there a trade-off between performance and computational complexity? Can the approach be integrated with other types of GNN architectures, and if so, how?"}}
{"paper_id": "sKPzAXoylB", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses two major issues in continual learning: catastrophic forgetting and loss of plasticity, with a novel method UPGD that is shown to be effective in challenging streaming learning and reinforcement learning environments. The method is thoroughly evaluated and compared against a range of baseline methods.", "Weaknesses": "While the approach is novel and sound, the reliance on utility approximations could be a limitation in scenarios where precise utility measurement is critical. Furthermore, the paper primarily focuses on synthetically constructed continual learning tasks; real-world applications could further validate the approach.", "Questions": "How does UPGD perform in real-world continuous learning applications beyond the synthetic benchmarks used? Are there any implications for the choice of hyperparameters on the performance of UPGD?"}}
{"paper_id": "H8Qg1IIMaR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper highlights a previously overlooked vulnerability in large language and vision-language models concerning multiple-choice question answering. The experiments are well-detailed and the findings are significant, raising important questions about the reliability of these models in practical applications.", "Weaknesses": "The paper could benefit from a deeper exploration of potential solutions or mitigations to the identified vulnerability. Additionally, a more thorough theoretical analysis of the systemic issues leading to permutation sensitivity is needed.", "Questions": "Are there any theoretical insights that can help explain the observed permutation sensitivity beyond empirical findings? What specific training or architectural changes would the authors suggest to mitigate this issue in future models?"}}
{"paper_id": "tth2qXY7RU", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents an innovative quantization method that significantly improves memory footprint and computational efficiency without compromising model accuracy. It introduces a novel data type, Super Floating-Point (SuFP), and demonstrates its effectiveness across a wide range of models, outperforming existing methods such as MSFP and BSFP. The proposed method also shows strong adaptability to different data distributions and is highly hardware-efficient.", "Weaknesses": "While the proposed method demonstrates impressive results, the paper lacks detailed analysis or potential limitations in real-world applications or under edge cases. Additionally, the complexity and feasibility of integrating SuFP into existing systems and infrastructures are not fully addressed.", "Questions": "How does SuFP perform under different variations or noise introduced during inference or training? Are there any specific scenarios where SuFP might not perform as well as expected? How feasible is the adoption of SuFP in current AI hardware systems in terms of cost and complexity?"}}
{"paper_id": "fjf3YenThE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a specific problem in zero-order hard-thresholding by introducing variance reduction techniques and provides theoretical convergence analysis.", "Weaknesses": "The paper could benefit from clearer presentation of results and better visualizations, as well as a more detailed comparison with existing methods.", "Questions": "How does the proposed method compare in practical scenarios to existing gradient-based methods beyond the adversarial attack setting? Are there specific applications where the benefit of the method is most pronounced?"}}
{"paper_id": "H9DYMIpz9c", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach to autoregressive data distillation through the FARZI method, which achieves impressive data compression without loss of performance. It explores an important area in machine learning with significant potential to optimize training costs and resources.", "Weaknesses": "The scaling to very long sequences remains a challenge, and the method's practical application to larger models and datasets is yet unexplored due to computational constraints.", "Questions": "How can FARZI be adapted to scale efficiently for very long sequences, such as video or music? What are the specific challenges in scaling to larger models or datasets like T5 or C4, and how might they be overcome?"}}
{"paper_id": "4kLVvIh8cp", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces an oracle-efficient algorithm for offline reinforcement learning (RL) with non-linear function approximation and provides a theoretical analysis demonstrating instance-dependent regret bounds.", "Weaknesses": "The paper makes strong assumptions on data coverage which may not hold in practical scenarios. Additionally, the presentation might be dense for readers not familiar with theoretical RL.", "Questions": "How does the algorithm perform empirically compared to existing methods, particularly in terms of computational efficiency and robustness against assumption violations?"}}
{"paper_id": "CSpWgKo0ID", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel application of behavioral game theory to analyze LLMs in interactive settings, offering insights into how LLMs behave in cooperation and coordination games. It uses a variety of canonical game-theoretic situations to evaluate LLMs, making the results broadly relevant for understanding LLM interactions.", "Weaknesses": "The experiments reveal limited ability to coordinate with simple human-like strategies, which might suggest fundamental limitations of the LLM's current iteration rather than insights into behavior. The methodology, while thorough, could be enhanced with more diverse game setups, including real-time human interactions. Some areas of analysis could delve deeper into the implications of LLM defectiveness and uncooperativeness in broader contexts.", "Questions": "How do these findings translate to real-world applications where LLMs interact with humans, especially in cooperative tasks? Would different LLM architectures yield significantly different results in repetition of these experiments? How generalizable are these findings across different versions of LLMs and prompts?"}}
{"paper_id": "gwbQ2YwLhD", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important problem in structure learning, specifically the impact of variable scales on learning directed acyclic graphs (DAGs). It provides theoretical analysis of conditions where scale can lead to incorrect DAG predictions, extending analysis to log-likelihood based losses. Theoretical findings are supported by extensive empirical evaluations.", "Weaknesses": "The assumptions made in the theoretical analysis, such as Additive Noise Model with Invertible Functions and Immiscible Structures, may not hold in all real-world scenarios. There is also a lack of discussion regarding the potential limitations of the proposed Scale Robust Loss (SRL) in continuous structure learners.", "Questions": "How does the proposed Scale Robust Loss (SRL) perform in real-world applications where assumptions made in the theory may not hold? Are there any additional datasets where the robustness of SRL can be validated beyond the ones used in the experiments?"}}
{"paper_id": "eIYDKNqXuV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed Village-Net Clustering algorithm demonstrates impressive computational efficiency and competitive performance on a range of datasets, especially in terms of Normalized Mutual Information (NMI) scores. The method also autonomously determines the optimal number of clusters, which is a notable advantage.", "Weaknesses": "The accuracy of the algorithm is closely tied to the ability of K-Means to partition the data effectively, which can be a limitation for complex datasets with non-convex structures. The paper could benefit from more detailed explanations of the algorithm and its parameters, as well as direct comparisons with a broader range of state-of-the-art methods.", "Questions": "How does the algorithm perform in scenarios with highly imbalanced cluster sizes or noise? Could the authors provide more insights into the choice of hyperparameter values, specifically the impact of different k values on various types of datasets?"}}
{"paper_id": "bAMPOUF227", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper proposes a novel framework, SuperContext, that effectively integrates task-specific fine-tuned models with large language models to enhance their in-context learning ability, particularly for out-of-distribution generalization and factuality. The empirical results are promising and show significant improvements over traditional in-context learning methods.", "Weaknesses": "The reliance on specific large language models and the necessity for fine-tuned task-specific models might limit the general applicability of the approach. Moreover, the interpretation aspect, although present, could be improved for better understanding of the underlying mechanisms and user transparency.", "Questions": "How does the approach fare with other recent large-scale models like PaLM or Claude? Can the method be generalized beyond the evaluated tasks or is its effectiveness limited to those explored in the paper?"}}
{"paper_id": "eNoiRal5xi", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach for domain generalization through Unknown Domain Inconsistency Minimization (UDIM), which extends Sharpness-Aware Minimization (SAM) by considering both data and parameter space perturbations. The theoretical underpinning is solid, and empirical results show consistent performance improvements over existing SAM variants across multiple benchmark datasets.", "Weaknesses": "The paper could provide more detailed insights into the computational complexity and scalability of the proposed method, particularly in terms of the Hessian matrix computation. Additionally, while empirical results are strong, the explanation of parameter sensitivity and choice could be more comprehensive.", "Questions": "How does the computational cost of UDIM compare to standard SAM in large-scale settings? Can the approach be extended to other types of tasks beyond domain generalization, and if so, how effective would it be?"}}
{"paper_id": "uU0Adp7Sfo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper proposes a novel CCGAN framework that incorporates collaboration with competitive elements to generate data beyond authentic distributions. It provides a theoretical foundation for equilibrium and reduces training collapse issues, validated through experiments across multiple datasets.", "Weaknesses": "While the theoretical contributions are interesting, the paper relies heavily on experimental validation without extensive ablation studies. The interpretation of results for real-world applicability, especially outside image data, might need further clarification.", "Questions": "How does the proposed framework handle scalability for larger datasets or higher dimensional data? Can the paper discuss any additional computational overhead introduced by the collaborative elements?"}}
{"paper_id": "dKju7tbe6D", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel iterative and parallel reasoning mechanism (IPRM) that effectively combines iterative computation with parallel processing, leading to strong performance on multiple visual reasoning tasks. It achieves state-of-the-art results and shows excellent generalization capabilities.", "Weaknesses": "The presentation of the paper could be improved for clarity in some technical sections. The detailed explanation of the method might be challenging to follow for readers not intimately familiar with the domain. While the experimental results are impressive, more qualitative analysis or real-world implications could strengthen the argument.", "Questions": "How does the model perform compared to newer large-scale vision-language models available today? Are there any significant trade-offs between model size and performance that could be explored further?"}}
{"paper_id": "DL7JWbdGr3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach to epidemic forecasting by pre-training models using data from multiple diseases, which is both innovative and potentially impactful. It leverages self-supervised learning tasks to improve model generalization and performance on unseen epidemics, demonstrating a significant improvement over existing methods.", "Weaknesses": "The presentation could be clearer, especially in terms of explaining the technical details and results. The paper is dense with information, which might make it difficult for readers unfamiliar with the domain to fully grasp the methodology and implications.", "Questions": "How does the model perform when integrated with data sources other than time-series, such as social media or mobility data? Can the approach be adapted for real-time updates and predictions during an ongoing epidemic outbreak?"}}
{"paper_id": "1PaDPHDhwe", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel robust scaling strategy that effectively identifies trade-off landscapes and optimizes desired performance points without additional training, showing consistent improvement across multiple benchmarks. The approach is versatile and applicable to existing algorithms.", "Weaknesses": "The dependency on validation set size can affect the robustness of the results, especially in IRS. Some performance gains are marginal when group supervision is utilized. The method, although simple and effective, may not deeply explore theoretical underpinnings of the robust scaling process.", "Questions": "How does the robust scaling strategy compare to other known post-processing techniques outside of group robustness, such as calibration methods, in terms of computational efficiency and effectiveness?"}}
{"paper_id": "iI7hZSczxE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach to disentangling time series representations using contrastive learning, showing promising results in energy disaggregation tasks. The introduction of TDS as a metric for evaluating disentanglement is innovative and useful.", "Weaknesses": "The paper assumes a level of statistical knowledge that might make it inaccessible to a broader audience. Some theoretical assumptions may not hold in practical scenarios with different types of correlations.", "Questions": "How does the proposed method handle real-world time series data that may not follow the assumptions made in the theoretical model? Are there plans to address potential limitations related to the independence-of-support assumption in practice?"}}
{"paper_id": "qrGjFJVl3m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The work introduces a versatile and highly performant vision-language model series, Qwen-VL, which achieves state-of-the-art performance on several benchmarks. The models support multiple languages and are capable of image understanding, text-reading, localization, and handling multiple images, indicating strong design and training methodology.", "Weaknesses": "The paper does not extensively discuss any potential limitations or failure modes of the proposed models. It could benefit from an analysis of the computational resources required and the model's inference speed.", "Questions": "How does Qwen-VL handle adversarial inputs or noisy data in real-world applications? Are there specific domains where the performance degrades significantly?"}}
{"paper_id": "qup9xD8mW4", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel framework for behaviour distillation in reinforcement learning, extending dataset distillation techniques to this domain. The approach is well-founded, combining evolutionary strategies with supervised learning to optimize small synthetic datasets. The results show that this method achieves competitive performance in continuous control tasks, and the synthetic datasets generated can be used across various architectures and hyperparameters. The paper is well-written and the experiments are thorough, highlighting the potential of the proposed method for neuroevolution and multi-task learning.", "Weaknesses": "The method relies on evolutionary strategies which can be computationally expensive and require a large population size for effectiveness. Additionally, the approach requires careful tuning of hyperparameters for both the inner and outer optimization loops. The scalability to high-dimensional pixel-based environments is limited, which is not addressed in detail.", "Questions": "How does the method scale to more complex environments, such as those involving high-dimensional inputs like images? Are there any plans to address the computational cost and scalability issues in future work?"}}
{"paper_id": "Zw8YxUWL4R", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel method for extending textual conditioning in text-to-image models, providing greater control over synthesis. The introduction of Extended Textual Inversion (XTI) is shown to be more expressive and faster than previous methods. The paper includes extensive experiments to support its claims.", "Weaknesses": "The paper could improve its presentation clarity, especially in explaining some technical details. While XTI is faster than TI, the process is still relatively slow. The disentanglement among U-net layers is not perfect, which limits control.", "Questions": "How does XTI perform under different model architectures? Are there specific scenarios where the method fails or performs suboptimally compared to existing techniques?"}}
{"paper_id": "3NXhwkZGjz", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach to Source-Free Unsupervised Domain Adaptation (SFUDA) by consolidating multiple prediction hypotheses and rationales, achieving state-of-the-art results. The method is well-structured with a clear three-step process and provides empirical validation through extensive experimentation on multiple datasets. It also demonstrates how the approach can be integrated into existing methods to enhance performance.", "Weaknesses": "The approach relies on having access to the entire target training set, which may not be applicable in real-time adaptation scenarios or when online adaptation is required. The method's dependency on GradCAM for rationale representation may limit its applicability to models where such interpretations are feasible, potentially excluding certain architectures or situations where interpretability is challenging.", "Questions": "How does the method perform when applied to non-visual domain adaptation tasks where GradCAM might not be applicable? Could the rationale consolidation process be adapted to function without access to the entire target set, supporting more dynamic or real-time applications?"}}
{"paper_id": "Fn655mJ4bv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel method for interpreting structured output models, specifically designed to account for dependencies between output variables. This approach, called SOInter, is unique as it focuses on explaining single target outputs while considering the structural information among all outputs. The method shows promising results on both synthetic and real-world datasets, demonstrating improved interpretation performance over existing methods like Lime and Kernel-Shap.", "Weaknesses": "The approach requires training two neural networks, which increases its complexity and could be a burden in computation and memory, particularly for deployment in large-scale applications. Additionally, the method relies on perturbing inputs as a form of feature selection, which may not be ideal in cases where zero values hold special meanings within the dataset.", "Questions": "How does SOInter perform in real-time applications where computation speed is critical? Are there specific types of structured output models where SOInter is less effective? Could the incorporation of structural dependencies introduce any bias or limitations that need further exploration?"}}
{"paper_id": "VyMW4YZfw7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a simpler, non-deep learning approach using traditional nonparametric spectral methods to achieve competitive performance in semi-supervised node classification tasks. The results indicate that these simpler methods can match or exceed some state-of-the-art results under certain conditions.", "Weaknesses": "The results heavily depend on the choice of kernel and other hyperparameters, which appear to be dataset-specific and might be difficult to generalize. The study may not adequately address cases where node features alone dominate the responses, and the claims about performance improvements over complex GNNs might not be universally applicable.", "Questions": "How can the proposed method be adapted or extended to outperform traditional GNNs in situations where the node features X play a more predominant role in SSNC tasks?"}}
{"paper_id": "09xFexjhqE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper proposes AutoLoRa, a novel approach to disentangle robust fine-tuning via a low-rank branch to enhance adversarial robustness in downstream tasks. The method addresses the issue of divergent gradient directions in existing robust fine-tuning approaches and introduces automated heuristic strategies to schedule hyperparameters.", "Weaknesses": "The paper's evaluation is primarily conducted on standard benchmarks with a limited assessment of scalability and real-world deployment complexities. The improvement over existing methods, while present, may require more diverse testing scenarios to establish its significance comprehensively.", "Questions": "Does the introduced heuristic strategy generalize across diverse domains and architectures outside those tested? Could there be potential limitations or challenges in scaling AutoLoRa for large-scale models?"}}
{"paper_id": "nmBjBZoySX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an adaptive, dynamic, and automated framework for identifying Graph Lottery Tickets (GLTs) in GNNs, offering significant advancements in sparsity handling and scalability for deeper networks. The AdaGLT approach is theoretically grounded and demonstrates superior empirical results across various GNN architectures and datasets, addressing existing challenges in scalability, elasticity, and inflexibility of previous GLT methods.", "Weaknesses": "The paper could benefit from more detailed discussion on computational cost and potential limitations in diverse graph types and structures. Additionally, the reliance on certain assumptions for theoretical proofs may limit the general applicability of the approach.", "Questions": "How does AdaGLT perform in terms of computational efficiency compared to existing GLT methods? Are there any specific types or structures of graphs where AdaGLT may face limitations?"}}
{"paper_id": "di52zR8xgf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 5, "Strengths": "The paper presents significant improvements in latent diffusion models for high-resolution image synthesis through a larger UNet architecture, new conditioning schemes, and a refinement model for enhanced visual fidelity. The inclusion of user studies to validate the model's effectiveness and the use of multiple innovative techniques shows strong experimental validation. The open-source approach encourages further innovation and reproducibility.", "Weaknesses": "The model's increased complexity and size raise concerns about inference cost, both in terms of VRAM usage and sampling speed. While the results are impressive, the paper could benefit from more detailed analysis on potential biases in the model given the dataset it was trained on.", "Questions": "Are there specific scenarios where the new model displays shortcomings compared to previous versions? How does the model's performance vary across different types of image content beyond what's presented in user studies?"}}
{"paper_id": "0b328CMwn1", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel concept of activation prompting (AP) that extends visual prompting (VP) to intermediate activation layers, leading to improved performance and efficiency over conventional VP. The study is supported by extensive experiments across 29 datasets demonstrating the effectiveness of AP compared to state-of-the-art methods.", "Weaknesses": "The paper's approach implicitly relies on the size of the pretrained model for superior performance, potentially limiting its applicability to smaller models. The theoretical insights, while interesting, might not be directly applicable or understandable to all potential readers.", "Questions": "How does the performance of AP compare to VP and NORM-TUNE for smaller models like ResNet-18? Are there specific use cases where AP might not provide significant improvements over existing methods?"}}
{"paper_id": "AMDKqZcZbi", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach to rapid and continual learning using a biologically inspired neural model, MESH, which effectively addresses catastrophic forgetting. It demonstrates the ability to learn and generalize across multiple environments, outperforming traditional methods.", "Weaknesses": "The presentation of the paper could be improved for better clarity and understanding, particularly in the explanation of the technical aspects of the MESH architecture. Some sections could benefit from additional detail or simplification.", "Questions": "How does the proposed method scale with more complex tasks or environments beyond the sequential Morris Water Maze? Are there performance benchmarks compared to other state-of-the-art models in similar settings?"}}
{"paper_id": "am7BPV3Cwo", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a compelling generalized statistical framework, termed ImOOD, for addressing OOD detection on imbalanced data. It offers both a novel theoretical explanation for observed performance gaps and practical techniques (post-hoc normalization and training-time regularization) for improving OOD detection performance. The empirical results on standard benchmarks are convincing and demonstrate significant improvements over existing methods.", "Weaknesses": "The paper assumes full-data learning for satisfactory OOD detection performance, which might not be feasible in all real-world applications due to the expensive nature of manually collecting auxiliary OOD data. The solution may require tuning or adaptation when deployed in varied environments, which could introduce overhead.", "Questions": "How does the ImOOD framework perform in scenarios where the assumption of separate and finite ID and OOD data spaces does not hold? Can the proposed approach be effectively adapted to online or continual learning environments where the data distribution may gradually change over time?"}}
{"paper_id": "R1crLHQ4kf", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel and effective method for detecting adversarial audio examples by leveraging characteristics of the output distribution in ASR systems. The experimental results show strong performance across different models and languages, with AUROC exceeding 99% on clean and noisy data.", "Weaknesses": "The paper lacks a detailed discussion on the computational overhead and real-time applicability of the proposed method. Some aspects, like the adaptive attack, might require further exploration to ensure the method's robustness in practical scenarios.", "Questions": "How does the proposed method perform in real-time scenarios with live audio input? Is there a significant computational overhead involved in extracting the output distribution characteristics and training the classifiers?"}}
{"paper_id": "i7P2mK3x3o", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel approach for learning dynamic optimal transport between high-dimensional distributions using flow neural networks. The approach is empirically validated on several tasks including density ratio estimation and image-to-image translation, showing strong performance compared to baselines.", "Weaknesses": "The paper lacks a deep theoretical foundation for the proposed method, and some aspects of the experimental setup and results could be more thoroughly detailed. Additionally, the computational complexity might be a concern for very high-dimensional problems.", "Questions": "How does the proposed method compare in terms of computational efficiency with state-of-the-art methods in very high dimensions? Are there any theoretical guarantees regarding the convergence of the proposed method to the true optimal transport?"}}
{"paper_id": "YIls9HEa52", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces an innovative model, the infinite recurrent switching linear dynamical system (irSLDS), improving upon limitations of existing models by introducing a flexible state space and geometry over discrete states, validated on both synthetic and real-world data.", "Weaknesses": "The paper could provide a more thorough evaluation of computational efficiency compared to existing methods and offer more insight into potential limitations or drawbacks of the proposed approach.", "Questions": "How does the computational complexity of the irSLDS compare to existing models in practice, especially for large datasets? Are there specific scenarios or datasets where the proposed method might not perform well?"}}
{"paper_id": "koYsgfEwCQ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "DynaVol introduces a novel approach for unsupervised learning of dynamic scenes, providing object-centric volumetric representations in a 3D context. The method shows superior performance in both synthetic and real-world scenes for tasks like scene decomposition and novel view synthesis compared to previous state-of-the-art methods. Additionally, the ability to edit scenes dynamically is a significant advantage.", "Weaknesses": "The paper may have a high level of complexity that could pose understanding challenges for readers unfamiliar with the specific domain. The method heavily relies on the successful segmentation of objects, which might not be as effective in scenes with significant occlusion or very complex textures.", "Questions": "Can the model handle real-time scene edits and rendering, or is it limited to offline processes due to computational demands? What are the limitations in terms of the number of objects or the complexity of motion patterns the system can effectively manage?"}}
{"paper_id": "o6AN2ZoNw2", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a simple yet effective framework for open-vocabulary semantic segmentation that achieves state-of-the-art results on multiple benchmarks. The use of pseudo-masks and language as supervision, combined with the ability to generalize well without fine-tuning, are significant strengths. The method's scalability and improvement with self-training are also noteworthy.", "Weaknesses": "While the method achieves excellent results, the contribution is somewhat incremental, relying heavily on existing concepts like pseudo-mask generation and image-text contrastive learning. Moreover, the success seems partly due to leveraging existing large-scale datasets, which might limit its applicability in more constrained scenarios.", "Questions": "How does the framework perform on tasks that require understanding more complex scenes or involve less common category queries? Are there any specific limitations or failure cases observed during testing?"}}
{"paper_id": "HXZK1Z8tHa", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces an innovative method, ShareFormer, which offers significant improvements in inference speed and trainability for image restoration tasks. The use of shared attention maps reduces latency while incorporating residual connections on the 'Value' enhances trainability without additional convolutional modules. The experimental results, presented clearly, demonstrate the effectiveness of the approach across various image restoration tasks.", "Weaknesses": "While the paper presents strong results, the explanation of the proposed method is complex and may be difficult for readers to understand without in-depth knowledge of Transformers and attention mechanisms in deep learning. Some sections could benefit from clearer visualizations and more detailed explanations, especially for the technical aspects of the implementation.", "Questions": "How does the introduction of residual connections on 'Value' specifically translate to improvements in trainability? Are there any potential drawbacks or limitations to using shared attention maps in contexts outside of image restoration tasks?"}}
{"paper_id": "QO3yH7X8JJ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents an innovative approach to arbitrary-scale super-resolution using pre-trained diffusion models without additional fine-tuning or distillation. The proposed Diff-SR method is supported by theoretical analysis and experimental results, demonstrating superior performance compared to existing state-of-the-art solutions across various super-resolution scenarios.", "Weaknesses": "The paper could benefit from clearer explanations and deeper discussions of the limitations of the proposed method. Additionally, the experimental results could be more comprehensive, with comparisons on more diverse datasets and a broader range of upscaling tasks.", "Questions": "How does the proposed method handle extremely high-resolution inputs, and what are its limitations in such scenarios? Can the approach be generalized to other tasks beyond super-resolution, such as denoising or deblurring?"}}
{"paper_id": "sNtDKdcI1f", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a thorough investigation into the potential overemphasis on output length in RLHF optimization, offering insightful analysis and diagnostics on the issue. It proposes several interventions and shows their effects on mitigating length bias, contributing to understanding RL challenges.", "Weaknesses": "The experiments rely solely on open-source datasets and models, which might limit the generalizability of the findings to larger, more sophisticated models used in industry. The paper could have explored more diverse objectives beyond 'helpfulness' for broader applicability.", "Questions": "How would these findings translate to larger models or different RLHF objectives, such as 'harmlessness'? Are there plans to investigate other potential biases beyond length in these models?"}}
{"paper_id": "HFtrXBfNru", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an underexplored problem of temporal generalization estimation in evolving graphs, providing a theoretical analysis and proposing SMART, a self-supervised approach using augmented graph reconstruction. The approach is backed by both synthetic and real-world experiments, demonstrating effectiveness.", "Weaknesses": "Though promising, the method's reliance on the assumption of graph evolution patterns could limit its applicability. The implementation details of SMART could be further clarified to aid reproducibility. Additionally, comparisons to a wider range of related techniques could deepen the analysis.", "Questions": "How does SMART perform on evolving graphs with more complex dynamics, such as those with varying labels or heterogeneous attributes? How sensitive are the results to the assumptions made about graph evolution and feature distribution?"}}
{"paper_id": "4u0ruVk749", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper proposes a novel method to estimate individual treatment effects using diffusion models, which addresses the problem of unobserved confounders in causal inference. The method appears to improve upon current state-of-the-art methods in some benchmarks, indicating its potential applicability.", "Weaknesses": "The paper could benefit from clearer exposition and more thorough explanations of the assumptions and limitations of the proposed method. The evaluation, while comprehensive, could include more challenging real-world datasets to better assess generality.", "Questions": "How sensitive is the proposed method to the choice of hyperparameters, specifically the parameters controlling the diffusion model? What are the specific computational costs associated with employing diffusion models in this context?"}}
{"paper_id": "Pa6SiS66p0", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel method for integrating multiple modalities to effectively mitigate catastrophic forgetting in continual learning scenarios. It introduces a new benchmark for evaluating multi-modal continual learning, demonstrates the advantages of multi-modal learning over unimodal approaches, and provides a compelling case for further exploration in this area.", "Weaknesses": "While the paper presents the potential benefits of multi-modal learning, there may be a lack of depth in exploring the challenges and potential drawbacks in practical deployment scenarios. The experiments are limited to specific benchmarks and may not fully account for diverse real-world conditions.", "Questions": "How does the method perform on different datasets with varying types and numbers of modalities beyond the specific benchmark introduced? What are the challenges faced in scaling the proposed method to handle more modalities or larger datasets?"}}
{"paper_id": "rkplYfqUr0", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel generative zero-shot framework for text classification that improves robustness and performance over existing baseline methods. The use of contextualized label descriptions to improve generative prompting is innovative.", "Weaknesses": "The approach may not generalize well to non-English languages or other NLP tasks beyond text classification. There is limited exploration of potential biases introduced by using contextual demographic information.", "Questions": "How does the system perform on tasks other than text classification, such as question answering or NLI? Is there a potential for bias when using personalization attributes, and how can it be addressed?"}}
{"paper_id": "pTU2X9mUBe", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces LaDe, a large-scale, comprehensive, diverse, and publicly available last-mile delivery dataset, which addresses a significant gap in the logistics and data mining research fields. The dataset's diversity and comprehensiveness provide numerous opportunities for benchmarking and advancing research in various tasks such as route prediction, ETA prediction, and spatio-temporal graph forecasting.", "Weaknesses": "The paper could benefit from more detailed explanations regarding the potential risks of dataset bias given the selection of specific cities and the implications for generalizability across different urban contexts. Additionally, the dataset's largest scale and unique characteristics might present challenges for researchers who lack the resources to manage such large datasets.", "Questions": "How can researchers ensure that solutions developed using LaDe generalize well to cities not represented in the dataset? Are there any plans for periodic updates or expansions to the dataset to include more regions or to address potential biases?"}}
{"paper_id": "uhtQyRrTzY", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel method MIMIC for constructing large-scale multi-view datasets from both real-world videos and simulated environments without requiring additional annotations. The resulting dataset, MIMIC-3M, outperforms existing state-of-the-art datasets on dense vision tasks, such as depth estimation and surface normal prediction, showing the method's significant practical impact.", "Weaknesses": "The presentation could be improved for clarity, especially in describing the methodology and experiments. The paper does not explore dynamic scenes or assess the implications and potential limitations that arise from the method mainly handling static environments.", "Questions": "Could the method be extended to effectively handle dynamic scenes, where motion plays a significant role? How does the performance compare when scaled to even larger datasets beyond MIMIC-3M?"}}
{"paper_id": "WZfatbNdLV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces TrASPr, a novel predictive model that effectively uses transformers for RNA splicing predictions and demonstrates improved performance over existing models. It also formulates RNA sequence design as a Bayesian Optimization problem, providing a creative approach to sequence generation with specific splicing characteristics.", "Weaknesses": "The paper relies on inherently noisy and limited RNA-Seq data for training and testing, which might affect the robustness of the model. Some claims about biological plausibility of mutations generated by BOS are not thoroughly evaluated.", "Questions": "How does the model generalize to splicing events that are not cassette exons? How scalable is the BOS algorithm when targeted for splicing prediction across different species or larger datasets?"}}
{"paper_id": "YkRwadXWHd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper proposes a novel hierarchical knowledge distillation framework that successfully combines multiple modalities for continuous sign language recognition, achieving state-of-the-art results on several benchmark datasets.", "Weaknesses": "The paper may lack comprehensive evaluation or ablation studies on the effect of each component of the proposed method. Some fusion methods seem less effective, and comparisons are not detailed.", "Questions": "What are the specific impacts of the different fusion techniques on the model's performance, and is there a reason why certain techniques were less effective?"}}
{"paper_id": "q4Bim1dDzb", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel unified voxelization framework, UniVoxel, that efficiently models geometry, materials, and illumination using explicit scene representations. It significantly reduces optimization time while achieving high-quality results compared to state-of-the-art inverse rendering methods. The use of Spherical Gaussians for illumination is particularly innovative.", "Weaknesses": "The presentation could be improved for clarity and ease of understanding. Some details on the implementation and how specific components integrate within the framework would enhance the paper. There might also be concerns about the general applicability of the method to diverse real-world scenarios.", "Questions": "How does the method perform with more complex and varied real-world scenes compared to controlled synthetic environments? Are there any limitations regarding the scalability of the voxel resolution?"}}
{"paper_id": "39cPKijBed", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel methodology for mitigating dataset biases in diffusion models using a time-dependent importance reweighting approach. It provides a well-founded theoretical basis for the approach and demonstrates its effectiveness through comprehensive experiments on various datasets and bias settings. The method outperforms existing techniques.", "Weaknesses": "While the theoretical framework and empirical results are strong, the paper could improve in how the practical challenges of implementation are communicated. The presentation of complex ideas could be more concise and clear to reach a broader audience.", "Questions": "How does the proposed method handle cases with highly imbalanced reference datasets? Are there any limitations observed in real-world applications that need further investigation?"}}
{"paper_id": "aG3EARrrd1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The OSRT model offers a novel approach to handling streaming multivariate scattered data with improved efficiency and accuracy. The combination of online tree decomposition with adaptive RBF exploration allows the model to dynamically adjust to incoming data. Experiments demonstrate it performs well in comparison to existing models.", "Weaknesses": "The paper lacks clear explanations in some sections, making it difficult to fully understand the mechanics of the OSRT model. It would benefit from more detailed algorithm descriptions and clearer visual representation of the results. Additionally, the scalability of the model on even larger real-world datasets needs further exploration.", "Questions": "How does the OSRT model perform on different type of data streams outside of the benchmark datasets used in the experiments? Are there any limitations regarding computational resources when handling very large-scale datasets in real-time?"}}
{"paper_id": "Ts95eXsPBc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach by incorporating spatial awareness into transformer models, which is a significant advancement in the development of embodied agents. The proposed Spatially-Aware Transformer (SAT) and Adaptive Memory Allocator (AMA) demonstrate strong performance in a variety of tasks, indicating improved memory utilization and spatial reasoning capabilities. The paper is well-written and clearly presents the methodology and experimental results.", "Weaknesses": "The approach relies on the availability of spatial annotations, which may not be present in all applications. Additionally, the AMA method relies on predefined strategies, which might limit flexibility compared to a truly learned memory management strategy. There is also limited discussion on the feasibility of obtaining spatial information in more complex real-world settings.", "Questions": "How does the model handle environments where spatial information is noisy or partially observed? Are there any plans to test the approach in real-world robotic applications or simulations that closely mimic real-world challenges?"}}
{"paper_id": "LfhG5znxzR", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach that integrates vector quantization bottlenecks in neural networks to achieve sparse and discrete hidden states, enhancing interpretability and control. The method is validated on various datasets and demonstrates the capability to guide model behavior effectively.", "Weaknesses": "The paper primarily focuses on Transformer models and language datasets, potentially limiting its generalizability to other neural network architectures and applications. The presentation could benefit from clearer explanations and more concise summaries.", "Questions": "How does the introduction of codebook features affect the model's training time and computational cost compared to traditional training methods? Are there plans to generalize this approach to other architectures beyond Transformers, such as convolutional neural networks?"}}
{"paper_id": "bUGzjiUsIq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses the problem of partially annotated multi-label classification, which is an important but underexplored area. It introduces a novel RL framework, PAPG, that combines reinforcement learning with supervised learning to handle class imbalance and limited annotations effectively. The method shows significant improvement over existing approaches across multiple tasks.", "Weaknesses": "The paper might lack clarity in some parts, making it difficult for readers to fully grasp the proposed approach without deep expertise. The experiments are comprehensive across tasks, but more diverse datasets could enhance validity.", "Questions": "How well does the framework generalize to different kinds of multi-label problems or datasets beyond those tested? Are there specific areas where PAPG struggles compared to traditional methods?"}}
{"paper_id": "uMAujpVi9m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach to pocket representation learning via protein fragment-surroundings alignment, successfully creating over 5 million complexes. The proposed method achieves state-of-the-art performance across various tasks like pocket druggability prediction, pocket matching, and ligand binding affinity prediction. The approach is well-supported with empirical evidence and demonstrates strong potential for scalability and adapting to larger datasets.", "Weaknesses": "While the method is effective, it relies heavily on pre-trained molecular encoders, which might limit its adaptability across different domains without such pre-trained models. There are also challenges related to ensuring the relevant interaction features are consistently represented, especially given the discrepancies between simulated and real interactions.", "Questions": "How does the method perform on datasets from predicted structural models like AlphaFold, and how well can it be generalized to such applications? What are the potential limitations or challenges when applying this methodology to protein-protein interactions?"}}
{"paper_id": "uqxBTcWRnj", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel framework for bridging neural and symbolic representations using Transitional Dictionary Learning. It effectively incorporates game-theoretic methods and online clustering to demonstrate significant improvements over state-of-the-art unsupervised part segmentation methods. The proposed metrics align well with human evaluation, confirming the model's interpretability.", "Weaknesses": "The complexity of the method may pose challenges in understanding for readers not familiar with the underlying theories, and the computational demands might be high due to the game-theoretic approach and clustering involved. Limited details on the broader impacts of the framework are provided.", "Questions": "How does the model perform on real-world datasets with more complexity and noise? Are there any specific limitations or failure cases of the proposed framework observed during experiments?"}}
{"paper_id": "o4CLLlIaaH", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel point-based paradigm for building generalizable NeRF that effectively addresses occlusions using an explicit visibility model. It presents a robust log sampling strategy and a learnable kernel for feature aggregation, achieving state-of-the-art performance on multiple datasets.", "Weaknesses": "The initialization of the point scaffold relies on an external PatchmatchMVS process, which could potentially limit the integration and efficiency of the approach.", "Questions": "How does the reliance on PatchmatchMVS for initializing the point scaffold affect the scalability and adaptability of the proposed approach in more complex or diverse scenarios?"}}
{"paper_id": "KNzL1nglNB", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a significant challenge in machine learning concerning label insufficient scenarios and introduces a novel Label-encoding Risk Minimization (LRM) approach, which is theoretically sound and empirically validated across multiple scenarios. The findings have broad implications for improving model performance when labels are scarce.", "Weaknesses": "The paper assumes that the label encodings are consistent across both labeled and unlabeled data, which may not always hold true in real-world scenarios, potentially limiting the general applicability of the method. Additionally, the theoretical analysis, while sound, may benefit from further expansion to cover more varied and imbalanced datasets.", "Questions": "How does the LRM perform in scenarios where label encodings for unlabeled data might be noisy or inconsistent compared to the labeled data? What are the implications for performance in highly imbalanced datasets, and are there modifications to the method to account for this?"}}
{"paper_id": "pvhyBB86Bt", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper extends the analysis of the input-output Jacobian beyond initialization, providing a theoretical explanation for the role of initialization and studying Jacobian behaviour during training and in sparse networks. The use of recent results in random matrix theory offers a robust theoretical framework.", "Weaknesses": "The framework is currently limited to MLP architectures and may not easily generalize to more complex and modern architectures. The derivation of some results depends on approximations that may not hold in all cases.", "Questions": "How do the proposed theoretical insights extend to convolutional neural networks or other modern architectures? What are the practical implications of the descibed phases of Jacobian behaviour for training strategies?"}}
{"paper_id": "Mylk5iamJC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel Latent Gaussian Mixture Model (L-GMM), which effectively bridges closed-set and open-set recognition tasks using a generative approach integrated with collaborative training. The proposed model shows strong performance in both closed-set and open-set scenarios, as evidenced by comprehensive experiments on image classification and segmentation tasks.", "Weaknesses": "The paper does not clearly address potential limitations of the proposed model, such as scalability to larger datasets or computational complexity compared to existing methods. The evaluation benchmarks, while standard, may not fully capture certain edge cases or real-world deployment challenges.", "Questions": "How does L-GMM perform in terms of computational efficiency compared to purely discriminative models? Are there scenarios where L-GMM might fail or underperform significantly?"}}
{"paper_id": "MsOcVFzv8D", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides a theoretical foundation for MDTC by deriving a generalization bound based on Rademacher complexity, addressing the lack of theoretical guarantees in existing methods. It proposes a novel margin discrepancy-based adversarial training approach that empirically outperforms state-of-the-art methods on two benchmarks.", "Weaknesses": "The presentation could be improved for clarity, especially in explaining the complex theoretical concepts and their practical implications. There might be a steep learning curve for those not familiar with the technical details.", "Questions": "Are there specific scenarios where the proposed approach might not perform well, and how does the approach handle highly imbalanced domain distributions?"}}
{"paper_id": "fBlHaSGKNg", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel method called Unleashing the Power of Annotation (UPA) which efficiently selects samples for annotation in semi-supervised learning. The method is model-agnostic and demonstrates improved performance over existing methods on benchmark datasets, even with constrained annotation budgets. The theoretical foundation for -MMD and the empirical results support the paper's claims.", "Weaknesses": "While the method shows potential, the theoretical analysis may rely on assumptions that do not always hold in practical scenarios. The presentation could be more polished, as the alignment of concepts and results might be tightly packed, making certain sections difficult to follow. Additionally, the experiments mainly focus on image datasets, which might limit applicability to other domains.", "Questions": "How does the method perform on domains other than image classification? Can the selection method be extended or adapted for different types of data or tasks beyond SSL frameworks?"}}
{"paper_id": "HwQ8NVvdmm", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel approach to applying fully quantized training in the context of class incremental learning by leveraging Hadamard transforms. The technique demonstrates significant improvements in energy efficiency while maintaining accuracy within acceptable degradation limits. The evaluation covers a range of datasets and continual learning techniques, showing the method's broad applicability.", "Weaknesses": "The paper lacks a detailed comparison with other quantization methods in continual learning scenarios. It also does not address potential limitations or challenges in deploying the technique in real-world edge devices. The clarity of methodology could be improved, as some sections require a deeper understanding of the underlying mathematics.", "Questions": "How does the proposed technique perform in real-world edge applications with varying computational constraints? Are there specific tasks or scenarios where this approach might not be applicable or where performance might degrade significantly?"}}
{"paper_id": "cZo6pDtDZr", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces near-optimal algorithms for private estimation and sequential testing of collision probability, which improves on existing works by significantly reducing sample complexity. The theoretical analysis is robust and is supported by experiments that demonstrate the practical efficiency of the algorithms.", "Weaknesses": "While the theoretical contributions are solid, the presentation could be improved for clarity, especially in the proofs and descriptions of the algorithms. Additionally, some terms and techniques (such as the decoupling technique) are complex and might require further explanation for accessibility.", "Questions": "How does the algorithm perform on real-world data sets beyond the synthetic tests presented? Is there a potential for extending this work to other distributional properties beyond collision probability?"}}
{"paper_id": "F7QnIKlC1N", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel method, GTMGC, based on Graph-Transformer, which predicts 3D molecular conformations from 2D structures effectively. The proposed method outperforms state-of-the-art methods and the widely used RDKit software, demonstrating exceptional prediction performance across different datasets. The introduction of the Molecule Structural Residual Self-Attention (MSRSA) mechanism is a significant contribution that enhances model interpretability and efficiency.", "Weaknesses": "The paper could benefit from a more detailed explanation of the limitations and potential challenges in applying the GTMGC method to diverse molecular modeling tasks. Additionally, while the MSRSA module shows promise, the paper doesn't fully explore its generality across a broader range of molecular structures.", "Questions": "How does the performance of GTMGC vary with molecules of significantly larger size or complexity beyond those in the benchmark datasets? Are there computational efficiency considerations when scaling up to such molecules?"}}
{"paper_id": "8JCn0kmS8W", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "The paper introduces a novel framework, WavJourney, which connects large language models with various expert audio generation models for compositional audio creation. It demonstrates how LLMs can be used to generate structured audio scripts that are transformed into executable programs. The results show that WavJourney achieves state-of-the-art performance on standard benchmarks and introduces a new multi-genre audio storytelling benchmark.", "Weaknesses": "The framework relies heavily on the capabilities of large language models, which might limit its extensibility. The paper also highlights that the recomposition of audio elements might lead to unnatural synthetic results, particularly concerning music. Lastly, the reliance on multiple AI models may affect the system's efficiency.", "Questions": "How does WavJourney handle variability in LLM outputs, particularly when they fail to strictly adhere to specified instructions? Additionally, what kind of improvements can be expected in reducing the system's execution time for generating complex audio content?"}}
{"paper_id": "XbLffB0T2z", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses the problem of transferability in availability data poisoning attacks effectively. The proposed method, Transferable Poisoning, demonstrates superior transferability across different learning paradigms compared to existing methods. The empirical results are thorough and show significant improvements in attack effectiveness.", "Weaknesses": "The presentation of the paper could be clearer, especially in the explanation of Transferable Poisoning and its methodology. The theoretical underpinnings could be strengthened to provide a deeper understanding of why the proposed method is more transferable.", "Questions": "How does the proposed Transferable Poisoning perform on real-world datasets that are not benchmarks? Are there practical implications for model training security given the improved transferability of the poisoning attack?"}}
{"paper_id": "4y3GDTFv70", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel and interesting latent space theory to explain the emergent abilities of large language models, supporting claims with theoretical proofs and simulation experiments.", "Weaknesses": "The discussion heavily relies on assumptions about unambiguous and -ambiguous languages which may not hold comprehensively across all natural languages. The paper's analysis is largely asymptotic and might not fully account for practical limitations in model and data size.", "Questions": "How applicable is the theory to real-world LLMs that are trained with diverse and noisy data? Are there limitations in scalability that could be observed before reaching the asymptotic regime discussed?"}}
{"paper_id": "ODSgo2m8aE", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses an important aspect of fairness in Graph Neural Networks (GNNs) by proposing a Lipschitz bound tailored for these models. The use of Lipschitz regularization is innovative in improving individual fairness without substantial computational overhead. The experiments are comprehensive, demonstrating improvements in fairness with minimal loss in accuracy.", "Weaknesses": "The theoretical foundations could be expanded to offer more insight into the unique challenges posed by GNNs versus other neural network architectures. Additionally, while the paper presents a novel approach, its generalizability to other datasets and GNN architectures beyond those tested is not thoroughly explored.", "Questions": "How does the effectiveness of the proposed Lipschitz-bound approach vary with different types of node features and graph structures? Can the approach be adapted for other GNN architectures or for datasets with different fairness criteria?"}}
{"paper_id": "RlfD5cE1ep", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel theoretical understanding of how feature normalization prevents collapse in non-contrastive learning dynamics, offering a new perspective on the role of feature normalization in self-supervised learning frameworks. It extends existing theories by incorporating cosine loss, which reflects common practical implementations.", "Weaknesses": "The paper is heavily theoretical and might lack practical validations with real-world datasets. The reliance on high-dimensional analysis could limit the applicability of the findings in lower-dimensional, real-world scenarios. Results depend on several assumptions that may not hold for all settings.", "Questions": "How would these theoretical results apply to other non-contrastive learning frameworks like BYOL or SwAV? Furthermore, how significant is the impact of initial conditions and parameter choices on the observed dynamics in practical ML tasks?"}}
{"paper_id": "18TfucMNTr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel application of Gaussian continuation methods to improve the training process of deep neural networks. The approach has been shown to be effective in avoiding local minima and improving generalization. The paper includes comprehensive numerical studies demonstrating the efficacy of the proposed method on various test problems.", "Weaknesses": "While the method shows promise, it might be computationally expensive due to the need for multiple function evaluations approximated by Monte Carlo methods. The paper also relies heavily on theoretical assumptions that may not always hold in practice, such as the existence of a continuous path in the continuation method. The treatment of saddle points, although addressed with regularization, could be further explored.", "Questions": "How does the computational overhead of the continuation method compare to traditional methods in large-scale applications with high-dimensional parameter spaces? Are there cases where the proposed method fails, and if so, what might be the dominant factors contributing to this? How sensitive is the method to the choice of hyperparameters, such as the regularization weight for the gradient estimate?"}}
{"paper_id": "JGTC6WgO4T", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach to multi-source black-box domain adaptation, addressing the limitations of existing methods in a practical setting. The theoretical analysis provides a solid foundation for the proposed Label Space-Induced Pseudo Label Refinement. The experimental results on benchmark datasets demonstrate competitive performance.", "Weaknesses": "The presentation could be improved for clarity, especially in the theoretical sections. The detailed configuration of the Pseudo Label Refinery Network might be complex and could require more intuitive explanation. Some experimental details, such as parameter tuning, are not thoroughly discussed.", "Questions": "How does the proposed method perform in the presence of significant domain shifts? Are there any limitations regarding computational complexity or scalability to larger datasets?"}}
{"paper_id": "fQHb1uZzl7", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel Transformer-based network that effectively unifies feature and cost aggregation for improved semantic and geometric correspondence. The approach demonstrates significant improvements over existing methods across multiple benchmarks.", "Weaknesses": "The presentation could be improved for clarity, especially in the description of the integrative self-attention and cross-attention mechanisms. Some implementation details and hyperparameter choices are not thoroughly explained.", "Questions": "How does the approach perform in real-world scenarios beyond the evaluated benchmarks? Are there any potential applications in other fields besides semantic and geometric matching?"}}
{"paper_id": "YjG29CIOP7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach to data-free meta-learning, addressing critical vulnerabilities such as task-distribution shift (TDS) and task-distribution corruption (TDC) effectively. The proposed TEAPOT and ROSY frameworks show significant improvements in handling these issues, which is supported by extensive experimental evaluation.", "Weaknesses": "The scope for the applicability of the proposed method could be limited to scenarios where the assumptions hold about the task distributions and model selection. There may be concerns on how this method scales or performs with significantly different or larger model pools.", "Questions": "How does the approach perform when significantly more OOD models are present than in the experiments reported? Can this approach be generalized to black-box pre-trained models?"}}
{"paper_id": "DmDpu3r8iU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel framework, Value Understanding Measurement (VUM), to assess both 'know what' and 'know why' aspects of value understanding in LLMs. The evaluation of five representative LLMs provides empirical insights into the impact of model scale and context on value understanding. The study also highlights the potential risks of LLMs fabricating reasoning based on context.", "Weaknesses": "The reliance on GPT-4 for evaluating semantic similarity concerning values may limit the sensitivity and specificity of the measurements. The dataset used for evaluation, despite being based on the Schwartz Value Survey, may not comprehensively capture the complexity and dynamic nature of human values. Additionally, the paper would benefit from more extensively justifying the choice of models and settings used in the experiments.", "Questions": "How does the framework handle the potential biases introduced by using GPT-4 as an evaluator for semantic similarity? Can the VUM framework be generalized to other domains beyond the Schwartz Value Survey, and how would it adapt to different cultural contexts of values?"}}
{"paper_id": "kXHEBK9uAY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel hierarchical planning framework using diffusion models that significantly improves planning efficiency and performance in long-horizon, sparse-reward environments. It successfully combines hierarchical planning with diffusion models, demonstrating improvements over state-of-the-art methods. The empirical results on standard benchmarks show clear advantages in both performance and computational efficiency.", "Weaknesses": "The approach depends heavily on the quality of offline data, and the choice of fixed sub-goal intervals might not generalize well across all environments. The work also assumes the learned value function is accurate, which might not be the case in all datasets.", "Questions": "How does the method perform when the offline data is imperfect or has limited coverage of the task space? Can the fixed sub-goal intervals adapt to more complex or dynamic environments without significant re-tuning?"}}
{"paper_id": "i7LCsDMcZ4", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces novel methods for generating saliency maps in SNNs and demonstrates improvement in classification performance using their proposed augmentation techniques.", "Weaknesses": "The evaluation primarily focuses on classification tasks, and the approach may not generalize well to other task domains. Additionally, the computational efficiency of the method is somewhat reliant on batch size.", "Questions": "How does the approach generalize to tasks beyond classification, and could computational efficiency improvements be achieved independently of batch size?"}}
{"paper_id": "jHdz0CIS2y", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel training strategy called SelfVC which shows clear improvement over previous methods for voice conversion by utilizing self-transformations and imperfectly disentangled representations. The approach demonstrates state-of-the-art results on zero-shot voice conversion tasks with excellent scalability across languages.", "Weaknesses": "The comparisons with prior work might not fully account for differences in datasets and vocoder compatibility, which could affect the results. There's also a dependency on a sufficiently large dataset for real-world performance.", "Questions": "How does the model perform on a wider range of voices, accents, and dialects beyond the datasets tested?"}}
{"paper_id": "NkYCuGM7E2", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper presents an innovative approach to using Large Language Models (LLMs) in autonomous driving scenarios, highlighting the potential for improved decision-making through commonsense reasoning and interpretability. The integration of LLMs with low-level controllers like MPC is well-executed, and the experimental results are promising, showing better performance in diverse and complex driving scenarios compared to existing methods.", "Weaknesses": "The paper lacks a detailed analysis of the limitations of using LLMs in real-world driving conditions, such as processing delays or resource constraints. Moreover, the evaluation, while extensive, could benefit from additional real-world testing beyond simulation to assess the practical viability of the approach. The reliance on language-based decision-making also raises questions about scalability and robustness in highly dynamic environments.", "Questions": "How does the system handle potential delays introduced by the LLM's decision-making process in real-time driving scenarios? Are there any considerations or plans for deployment in real-world autonomous vehicles, given the current reliance on simulated experiments?"}}
{"paper_id": "RtDok9eS3s", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach to simplify transformer architectures by removing unnecessary components, such as skip connections and normalization layers, using both theoretical insights from signal propagation and empirical data. The proposed simplifications lead to faster training throughput and reduced parameter count without sacrificing performance.", "Weaknesses": "The empirical evaluations, while thorough, are primarily conducted on relatively small scale models, limiting the generalization of findings to very large-scale models. The theoretical explanations, though well-founded, do not fully account for the observed empirical results.", "Questions": "How do these simplified architectures perform on a wider range of tasks beyond those presented? Are there any potential drawbacks related to stability or convergence issues during training at larger scales?"}}
{"paper_id": "YCPDFfmkFr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a significant problem by introducing a unified approach for differentiating feasible and infeasible convex quadratic programs, which can enhance expressive capabilities in machine learning models. The introduction of QPLayer provides practical value through open-source implementation and evidence of improved predictive performance in some tasks.", "Weaknesses": "The theoretical foundations have some gaps, particularly concerning the applicability of the implicit function theorem in degenerate cases. Additionally, some methodological choices and assumptions could be further clarified. While the results are promising, more extensive benchmarks and comparisons with state-of-the-art methods could strengthen the conclusions.", "Questions": "How well does the proposed approach generalize to other types of optimization layers beyond convex quadratic programs? What would be the impact of the identified gaps in theoretical foundations on the practical performance of the method in more varied scenarios?"}}
{"paper_id": "ueTdErd5Ib", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel framework for robust contextual stochastic optimization by integrating optimization with learning through discretization. Theoretical guarantees are provided for the proposed methodology, and extensive computational experiments validate its robustness and competitiveness against existing methods.", "Weaknesses": "The paper could benefit from more detailed explanations of the potential limitations of the proposed approach in different contexts. Additionally, empirical comparisons with more diverse datasets and optimization problems could provide deeper insights into the generalizability of the method.", "Questions": "How does the choice of discretization impact the computational complexity and scalability of the proposed approach? Can the method handle large-scale high-dimensional datasets efficiently?"}}
{"paper_id": "sDmjlpphdB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel Mixture-of-Experts approach to prompt optimization that effectively divides the problem space into regions with specialized prompts, leading to significant improvements in performance on benchmark NLP tasks. This methodology is a significant advancement over prior methods that relied on singular demo-free prompts.", "Weaknesses": "The paper could benefit from more detailed discussion on the limitations and potential biases introduced by clustering demos and the choice of text encoders for embedding. Additionally, empirical results are largely reliant on one specific language model, and generalization to other models is not explicitly addressed.", "Questions": "How well does the MoP framework generalize across different language models aside from the tested GPT-3.5-Turbo-Instruct model? Are there scenarios where the clustering of demos might introduce biases or degrade performance?"}}
{"paper_id": "Mdk7YP52V3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a theoretical analysis of the issues in overparameterized heteroskedastic regression models using field theory from statistical physics, offering a novel approach that generalizes across different models and architectures. Empirical results support the theoretical findings, and the paper suggests a more efficient method for tuning regularization.", "Weaknesses": "The paper may be difficult to understand for readers not familiar with field theory or advanced statistical physics concepts. The proposed solution requires a field-theoretical approximation that may not be directly applicable in real-world scenarios without further development.", "Questions": "How can the field-theoretical approach be adapted or applied to more complex, real-world tasks beyond the scope of the regression problems discussed in this paper?"}}
{"paper_id": "YGTSLDAPqb", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel framework called Connect Later, which effectively improves out-of-distribution (OOD) performance by using targeted augmentations in fine-tuning. The experiments on various real-world datasets demonstrate its efficacy and it achieves state-of-the-art results on several benchmarks.", "Weaknesses": "The methodology relies on expert knowledge for designing targeted augmentations, which may limit its applicability across other domains. Additionally, while the evaluation is thorough, the approach may still require significant tuning and domain-specific insights to design effective targeted augmentations.", "Questions": "How can the process of designing targeted augmentations be automated or generalized to apply across different tasks and datasets without relying heavily on expert knowledge?"}}
{"paper_id": "JTcaziw7G1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach called Private Retrieval Augmented Generation (PRAG), which leverages multi-party computation to securely perform information retrieval for large language models. The method ensures that neither the queries nor the database content is leaked, addressing an important challenge in distributed private similarity search. The approach is well validated with experiments on both synthetic and real datasets, showing it can achieve high accuracy while maintaining privacy.", "Weaknesses": "The approach incurs significant computational costs and speed limitations, especially as database sizes grow and high accuracy demands are placed. The performance of the method's approximate search feature shows trade-offs between privacy and speed, which may limit its application in some real-world scenarios.", "Questions": "How does the proposed PRAG approach compare to existing methods regarding computational resource requirements? Are the trade-offs between privacy and speed acceptable for typical applications in real-world scenarios?"}}
{"paper_id": "lEkFq4RUCX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel distance metric called Directional Distance Field (DDF) for 3D point cloud comparison, addressing the limitations of existing metrics such as Earth Movers Distance (EMD) and Chamfer Distance (CD). DDF is demonstrated to be more efficient and effective in quantifying geometric differences, achieving higher accuracy in various tasks like shape reconstruction, rigid registration, and scene flow estimation. The paper includes extensive experiments and ablation studies that support the claims of efficiency and effectiveness, and it introduces the use of reference points to compute local surface geometry differences, which is a valuable contribution.", "Weaknesses": "The paper contains highly technical content which might be difficult for those not deeply familiar with the field. Some details regarding the choice of specific hyperparameters or the sensitivity to these choices are not fully explored. The generalization ability of the method across different kinds of point clouds and potential dataset biases are not thoroughly discussed.", "Questions": "How does the proposed DDF handle noisy or highly irregular point clouds? Are there any scenarios where the DDF might perform worse than traditional methods like EMD or CD?"}}
{"paper_id": "Fq8tKtjACC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach by employing high-quality, textbook-like data for training a smaller language model in code generation tasks, demonstrating that focusing on data quality can achieve state-of-the-art results, overcoming scaling laws. This approach not only reduces computational resources but also the environmental footprint.", "Weaknesses": "While the model shows impressive results, it is heavily optimized for Python and relies on synthetic data whose detailed generation process remains proprietary. Additionally, the manual evaluation might introduce subjectivity, and the results might not generalize beyond the demonstrated use case due to dataset limitations.", "Questions": "How can the methodology be applied to languages other than Python? What are the key factors in generating high-quality synthetic data, and how can the process be scaled or automated? Would using GPT-4 for data generation significantly change the outcomes?"}}
{"paper_id": "8fQlGQkj0S", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a rigorous theoretical analysis of in-context learning, introducing novel generative models that help explain task retrieval and learning modes. The derivation of risk bounds and the identification of the U-shaped risk pattern in task retrieval are significant contributions. Theoretical findings are validated through numerical computations and Transformer experiments.", "Weaknesses": "The paper assumes linear regression tasks, which may limit the applicability of the findings to more complex, non-linear real-world NLP tasks. The presence of a U-shaped risk pattern in actual LLMs remains unverified. The paper also assumes that demonstration labels are noiseless, which may not reflect real-world conditions.", "Questions": "How well do the theoretical models and findings generalize to real-world NLP tasks with non-linear dynamics? Can the U-shaped phenomenon found in task retrieval be observed in practical applications with LLMs? How might noise in demonstration labels affect the paper's results?"}}
{"paper_id": "hRos9WldRK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel method, Learning to Bootstrap (L2B), that effectively handles label noise by simultaneously adjusting label and instance weights during training. The approach is supported by thorough experimentation on various benchmark datasets, demonstrating its robustness and superiority compared to existing methods.", "Weaknesses": "While L2B shows promising results, its implementation might involve complexity due to the meta-learning aspect. Furthermore, the paper could benefit from more detailed explanations in some sections, particularly the theoretical analysis and the impact of different components of the method.", "Questions": "How sensitive is the method to different configurations of the Gaussian mixture model used for creating the meta set? Can the proposed method be adapted easily to other types of neural network architectures beyond those tested?"}}
{"paper_id": "ttRSBZiCiu", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents WaveFluid, a novel model for high-fidelity speech synthesis through adversarial training, improving both architecture and efficiency. It introduces a two-stage approach leveraging reparameterization techniques, which shows competitive performance with existing models.", "Weaknesses": "The paper lacks detailed evidence on the robustness and scalability of the proposed model. Some sections, like the theoretical background, could be simplified for better readability.", "Questions": "How does WaveFluid handle variations in speaker characteristics beyond the datasets tested? Are there any specific limitations in different noise environments the model might struggle with?"}}
{"paper_id": "WYsLU5TEEo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a unified framework that addresses interpretability and robustness in neural image classifiers, combining counterfactual image generation with adversarial robustness training. The evaluations on two distinct domains demonstrate the effectiveness of the approach, with high classification accuracy and resilience against adversarial attacks. The framework's ability to produce high-quality saliency maps using only classification labels is a significant strength.", "Weaknesses": "The method is constrained to binary classification tasks and relies on balanced datasets for stable training, which can be problematic in contexts where datasets are imbalanced, such as anomaly detection. The paper does not clearly outline the limitations or potential drawbacks of the approach, which would be necessary to fully assess its applicability.", "Questions": "How does the approach handle imbalanced datasets, which are common in anomaly and defect detection contexts? Are there any plans to extend this method to multi-class classification tasks, and how might this be achieved?"}}
{"paper_id": "KKZaj2QS3G", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper introduces a novel approach to time series representation learning by addressing noise resilience and proposing an efficient encoder architecture. It demonstrates state-of-the-art results across various tasks such as forecasting, classification, and anomaly detection. The use of a low-pass filter and Inception-based design contributes significantly to its robustness and efficiency.", "Weaknesses": "The paper involves several hyper-parameters which might require careful tuning for different datasets or tasks, potentially affecting its adaptability in some scenarios.", "Questions": "How sensitive is the approach to the choice of hyper-parameters for different time series datasets, and is there a recommended strategy for tuning these parameters?"}}
{"paper_id": "LWDRiFzbHQ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel approach to assess model generalization using a vicinal risk proxy that does not rely on ground-truth labels. It shows consistent improvements in correlation with model accuracy across several test sets, highlighting its effectiveness in out-of-distribution scenarios.", "Weaknesses": "The paper could benefit from a more comprehensive analysis of the limitations of the proposed method, particularly in near in-distribution scenarios. Additionally, it lacks a deeper exploration of potential pitfalls or biases introduced by the similarity metrics used.", "Questions": "How sensitive is the proposed method to the choice of transformations and similarity metrics? Can this approach be effectively applied to other domains or types of data beyond image classification?"}}
{"paper_id": "Yp01vcQSNl", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel graph transformer architecture (DiGT) that explicitly models edge directionality, which is a significant contribution to the field of graph neural networks. The method demonstrates superior performance on newly proposed directional graph datasets, outperforming other state-of-the-art GNN and GT models.", "Weaknesses": "The proposed method may struggle with scalability issues due to the quadratic complexity of attention mechanisms, which is a common limitation in transformer-based models.", "Questions": "How does DiGT perform on larger graphs, and are there any strategies in place to address scalability issues beyond those mentioned in future work?"}}
{"paper_id": "LndMyiBl3n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel attack framework called SheAttack for restricted black-box attacks on graphs, which does not rely on prior assumptions about homophily and shows broad applicability across different graph settings. The methodology is backed by theoretical analysis and extensive experiments on both synthetic and real-world datasets, demonstrating effectiveness in both homophilic and heterophilic scenarios.", "Weaknesses": "The paper's performance heavily depends on the clustering process, specifically Kmeans, which introduces an additional hyperparameter to tune. The SheAttack framework, although effective, may still have room for improvement concerning the optimization of clustering strategies. Furthermore, the scalability on larger graphs could be challenging due to certain computations.", "Questions": "How does the performance of SheAttack vary with different clustering methods, and is there a plan to explore more advanced clustering algorithms? Can SheAttack be combined efficiently with strategies that improve clustering quality to enhance its performance on real-world datasets?"}}
