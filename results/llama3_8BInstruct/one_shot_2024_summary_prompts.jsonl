{"paper_id": "B0OwtVEejJ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors effectively bridge the gap between weight-entanglement and gradient-based NAS methods, yielding the advantages of both paradigms. \\n2. The TangleNAS approach is well-organized and clearly presented, showcasing comprehensive experiments across various search spaces and benchmarks. \\n3. The empirical results demonstrate the superiority of TangleNAS over conventional two-stage methods, offering a compelling approach for NAS in complex architecture spaces.\",\n  \"Weaknesses\": \"1. The novelty of the approach might be debatable, as the combination of gradient-based and weight-entangled NAS methods has been explored in other works. \\n2. The memory overhead is not significantly reduced, as the approach still requires storing the weights of all operations, even though they are zero-padded.\",\n  \"Questions\": \"1. How does TangleNAS perform on even larger and more complex search spaces, such as those involving multiple levels of abstraction or incorporating more modalities? \\n2. Can the authors provide more insights into the trade-off between any-time"}
{"paper_id": "ZlEtXIxl3q", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach to learning fitness functions with global epistasis using contrastive losses, which improves data efficiency and robustness.\\n2. The evaluation on simulated and real-world datasets demonstrates the effectiveness of the proposed method in recovering sparse latent fitness functions.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more in-depth discussion on the limitations of the proposed method, particularly in cases where the latent functions are not sparse.\\n2. The impact of the choice of contrastive loss function on the performance of the proposed method is not thoroughly explored.\\n3. The real-world datasets used for evaluation are not extensively described, making it difficult to assess the generalizability of the proposed method.\",\n  \"Questions\": \"1. How does the proposed method perform when the latent functions are not sparse?\\n2. Can the authors provide more insights into the choice of contrastive loss function and its impact on the performance of the proposed method?\\n3"}
{"paper_id": "6yJuDK1DsK", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses a significant challenge in lifelong test-time adaptation and proposes a novel and efficient solution. FEATHER's lightweight adapter is a clever idea that reduces computational cost and memory requirements.\\n2. The experimental evaluation demonstrates substantial parameter efficiency and robustness of FEATHER on various challenging benchmarks.\\n3. The paper is well-organized and clearly presented, with a concise introduction and a clear description of the proposed method.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more thorough discussion of the trade-offs between adaptation quality and computational efficiency, as FEATHER may still require significant adaptation parameters in some cases.\\n2. While FEATHER eliminates the need for source data access during adaptation, it's unclear how the method would handle the scenario where the source data is not available at all, and the model needs to adapt from scratch.\\n3. The authors could provide more insight into how the group-wise and point-wise convolutions are initialized and updated during adaptation.\",\n  \"Questions\": \"1. How does FEATHER perform when"}
{"paper_id": "lt6xKGGWov", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel approach to feature selection that uses neural estimation of mutual information to evaluate subsets of features, which can capture complex dependencies between features and targets.\\n2. The proposed method, MINERVA, is well-justified and theoretically sound.\\n3. The experimental results demonstrate the effectiveness of MINERVA in capturing non-linear dependencies and performing exact feature selection.\\n4. The code is implemented in the minerva package, making it easily accessible for future research.\",\n  \"Weaknesses\": \"1. The paper assumes that the neural network can accurately approximate the Donsker-Varadhan representation of Kullback-Leibler divergence, which might not be the case in all scenarios.\\n2. The method might be computationally expensive due to the optimization process.\\n3. The evaluation is limited to synthetic datasets, and it would be beneficial to test MINERVA on real-world datasets.\",\n  \"Questions\": \"1. How does MINERVA handle high-dimensional feature spaces, where the number of features"}
{"paper_id": "Bvrc6kobWd", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a new perspective on the role of margins in contrastive learning by analyzing their effect on gradients, which is a significant contribution to the field.\\n2. The authors' approach offers a more in-depth understanding of how margins impact contrastive learning, going beyond decision-boundary analyses.\\n3. The study demonstrates the effectiveness of emphasizing positive samples and scaling gradients based on positive sample angles and logits in improving generalization performance.\\n4. The experimental results are comprehensive and provide valuable insights into the impact of different margin-based factors on contrastive learning.\",\n  \"Weaknesses\": \"1. The paper assumes a level of familiarity with contrastive learning and margins, which may make it challenging for readers without a strong background in the area to fully understand the contributions.\\n2. The analysis could be further extended to explore other aspects of margins, such as their impact on the convergence rate or the stability of the optimization process.\",\n  \"Questions\": \"1. How do the findings on the importance of positive samples and gradient scaling generalize to"}
{"paper_id": "FJjHQS2DyE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant gap in unsupervised domain adaptation by tackling label shift, which is a more realistic assumption than covariate shift. \\n2. The Conditional Adversarial Support Alignment (CASA) framework provides a novel approach to minimize conditional symmetric support divergence, achieving more discriminative feature representations for classification. \\n3. The paper presents a complete training process, including minimizing source risk, target conditional entropy, and Lipschitz continuity of classifiers, and employing pseudo-labels to approximate support divergence reduction. \\n4. CASA outperforms state-of-the-art methods in various label shift scenarios on benchmark datasets like USPS to MNIST, STL to CIFAR, and VisDA-2017.\",\n  \"Weaknesses\": \"1. The authors assume that the label distribution shift is known, which might not be the case in real-world scenarios. \\n2. The method relies on pseudo-labels to approximate support divergence reduction, which could lead to noisy estimates. \\n3. The paper could benefit from a"}
{"paper_id": "xelrLobW0n", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel benchmark (TRACE) and evaluation metrics to assess the continual learning capabilities of Large Language Models (LLMs), which is a significant contribution to the field.\\n2. The study demonstrates the effectiveness of the Reasoning-augmented Continual Learning (RCL) approach in mitigating catastrophic forgetting and retaining reasoning capabilities of LLMs.\\n3. The paper provides a comprehensive evaluation of LLMs using the proposed benchmark and metrics, showcasing the strengths and limitations of various fine-tuning techniques.\",\n  \"Weaknesses\": \"1. The paper relies heavily on the performance of GPT-4 for reasoning annotation, which may limit the generalizability of the results to other models.\\n2. The study focuses on a specific set of tasks and metrics, which might not be representative of all real-world applications of LLMs.\\n3. The RCL approach may require significant computational resources and expertise in reasoning augmentation.\",\n  \"Questions\": \"1. How does the TRACE benchmark compare to existing continual learning benchmarks"}
{"paper_id": "bYQkOPvgDw", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed PRGNN framework effectively addresses the issue of label noise in GNNs, especially in heterophilous graphs and high noise-rate scenarios. This work is well-motivated and well-executed.\\n2. The use of probabilistic graphical models is innovative and allows for robustness against label noise without relying on the label smoothness assumption.\\n3. The framework's ability to work effectively on both homophilous and heterophilous graphs is a significant contribution.\\n4. The evaluation on datasets with varying noise levels and graph types is comprehensive.\",\n  \"Weaknesses\": \"1. The work relies heavily on the Bayesian network structure, which may not be universally applicable to all types of graphs.\\n2. The use of three independent GNNs may increase computational complexity.\\n3. The effect of the contrastive loss on the model's performance is not thoroughly explored.\",\n  \"Questions\": \"1. How does the PRGNN framework perform on graphs with varying node and edge features?\\n2. Can the"}
{"paper_id": "sKPzAXoylB", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses both catastrophic forgetting and loss of plasticity in continual learning environments, which is a significant contribution to the field.\\n2. The proposed method, UPGD, combines gradient updates with utility-based perturbations, which is an innovative approach.\\n3. The paper provides comprehensive evaluation on various non-stationary streaming problems and reinforcement learning tasks.\",\n  \"Weaknesses\": \"1. The authors assume that the Taylor series approximation of the loss function accurately represents the utility of network weights, which might not always be the case.\\n2. The method requires careful tuning of hyperparameters to balance the trade-off between protecting useful weights and rejuvenating less useful ones.\",\n  \"Questions\": \"1. How does UPGD perform on more complex tasks with multiple sub-goals or tasks?\\n2. Are there any scenarios where UPGD might not be effective, such as when the task boundaries are known or the data is highly imbalanced?\"\n}"}
{"paper_id": "H8Qg1IIMaR", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The study provides a comprehensive investigation into the robustness of large language models (LLMs) and vision-language models (VLLMs) against simple transformations like answer option permutations in multiple-choice question answering (MCQA) tasks.\\n2. The research highlights a systemic vulnerability of LLMs and VLLMs to these permutations, which significantly impacts their accuracy and performance.\\n3. The study is well-organized and clearly presented, with a logical flow of ideas and thorough analysis of the results.\",\n  \"Weaknesses\": \"1. The study primarily focuses on a specific type of attack (answer option permutations) and may not fully capture the complexity of real-world adversarial scenarios.\\n2. The use of simple permutations as an adversarial attack might not accurately reflect the variety of potential threats that these models could face in practice.\\n3. The study does not propose any new methods for improving the robustness of LLMs and VLLMs against these attacks.\",\n  \"Questions\": \"1. How do the results generalize to"}
{"paper_id": "tth2qXY7RU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed Super Floating-Point (SuFP) method demonstrates a novel approach to addressing the challenges of quantization by employing multi-region piecewise quantization and a tensor-wise scalable bias, which effectively balances precision and memory efficiency.\\n2. The evaluation is comprehensive, covering various domains and metrics, and provides a clear comparison with existing quantization methods.\\n3. The paper presents a well-organized and clearly written narrative.\",\n  \"Weaknesses\": \"1. While the SuFP method shows promising results, the reliance on integer arithmetic units and shifters may limit its applicability to certain hardware architectures.\\n2. The authors do not provide a detailed analysis of the potential trade-offs between accuracy and computational benefits for different types of data distributions.\",\n  \"Questions\": \"1. How does the SuFP method handle cases where the data distribution is highly non-uniform, and the number of outliers is very large?\\n2. Can the SuFP approach be adapted to work with other types of data types, such as integers or custom data"}
{"paper_id": "fjf3YenThE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper tackles a well-defined problem in the field of zeroth-order optimization and offers a promising solution to address the limitation of SZOHT.\\n2. The theoretical analysis and experimental results demonstrate the effectiveness of the proposed algorithm in improving convergence rates and applicability.\\n3. The method is well-motivated and the experiments are conducted on relevant tasks such as portfolio optimization and black-box adversarial attacks.\",\n  \"Weaknesses\": \"1. The paper assumes restricted strong smoothness and convexity for the theoretical analysis, which may not hold in all cases.\\n2. The algorithm's performance may degrade if the historical gradients are not representative of the current gradient.\",\n  \"Questions\": \"1. How does the algorithm handle cases where the historical gradients are not representative of the current gradient?\\n2. Can the algorithm be extended to handle non-convex objectives?\"\n}"}
{"paper_id": "H9DYMIpz9c", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 3,\n  \"Strengths\": \"1. The paper tackles a significant gap in data distillation techniques for autoregressive tasks, which can be computationally expensive and challenging due to their discrete nature and large vocabulary spaces.\\n2. The FARZI method is well-designed to address these challenges, utilizing reverse-mode differentiation of the Adam optimizer and factorizing the high-dimensional event space into a latent space.\\n3. The experimental results demonstrate the effectiveness of FARZI, achieving high performance with significantly reduced data inputs, and opening up possibilities for designing future large autoregressive models that are more resource-efficient and capable of scaling with less environmental and financial cost.\",\n  \"Weaknesses\": \"1. The paper assumes that the Adam optimizer is the baseline for comparison, which might not be the best choice for all autoregressive tasks.\\n2. The method relies heavily on the meta-learning framework, which might be challenging to extend or adapt to other tasks or domains.\\n3. The paper does not provide a thorough analysis of the computational cost of the FARZI method, which"}
{"paper_id": "4kLVvIh8cp", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper presents a novel algorithm for non-linear function approximation in offline RL, addressing a significant gap in the field.\\n2. The PNLSVI algorithm provides statistically optimal instance-dependent regret guarantees, a major contribution to the offline RL community.\\n3. The paper is well-organized and clearly explains the proposed method and its theoretical guarantees.\",\n  \"Weaknesses\": \"1. The introduction of the problem and its importance could be more detailed, providing a clearer understanding of the current state of the field and the limitations of existing methods.\\n2. While the PNLSVI algorithm is a significant contribution, its applicability to real-world scenarios may be limited by the need for efficient regression and bonus oracles.\",\n  \"Questions\": \"1. How does the PNLSVI algorithm handle cases where the function class complexity is high, potentially leading to increased computation time or memory requirements?\\n2. Are there any plans to extend the algorithm to handle other types of uncertainty, such as ambiguity or ambiguity-aware uncertainty?\"\n}"}
{"paper_id": "CSpWgKo0ID", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The study uses a structured approach from behavioral game theory to investigate the strategic behavior of LLMs, which is a valuable contribution to understanding their capabilities and limitations.\\n2. The experiments conducted with GPT-4 in various game settings provide a comprehensive evaluation of its performance in cooperative and competitive contexts.\\n3. The paper highlights the importance of understanding LLM behavior in social interactive settings and its implications for their potential applications.\",\n  \"Weaknesses\": \"1. The paper focuses on a specific type of game (two-player, two-strategy games) and its results might not be directly applicable to more complex social interactions.\\n2. The study's scope is limited to GPT-4 and it is unclear whether the findings are generalizable to other LLMs.\\n3. The paper could benefit from a more in-depth analysis of the limitations of the behavioral game theory approach in capturing the full complexity of human social interactions.\",\n  \"Questions\": \"1. How would the results change if the games were more complex, involving more"}
{"paper_id": "gwbQ2YwLhD", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a thorough investigation of the impact of scale on structure learning algorithms, generalizing previous findings to higher-dimensional and non-linear cases. This is a significant contribution to the field.\\n2. The theoretical proofs and experiments conducted on synthetic and real-world datasets provide strong evidence for the scale effect on various loss functions, including MMSE and log-likelihood losses.\\n3. The paper highlights the importance of caution when interpreting results from scale-sensitive losses and advocates for normalization and exclusion of free variances in loss calculations.\",\n  \"Weaknesses\": \"1. The paper primarily focuses on the impact of scale on structure learning algorithms, without exploring potential solutions to mitigate this issue.\\n2. The study relies on synthetic and real-world datasets, but it would be beneficial to include more diverse datasets to further validate the findings.\\n3. Some of the loss functions and algorithms explored in the paper might be considered standard, and the novelty of the contributions could be nuanced.\",\n  \"Questions\": \"1. How might the findings of this paper be"}
{"paper_id": "eIYDKNqXuV", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 3,\n  \"Strengths\": \"1. The proposed Village-Net Clustering algorithm combines the efficiency of K-Means with a novel community detection method, improving scalability and speed.\\n2. The use of a weighted network to represent cluster relationships is an interesting approach.\\n3. The algorithm's ability to automatically determine the number of clusters is a significant advantage.\",\n  \"Weaknesses\": \"1. The initial K-Means step may limit the accuracy of the overall method.\\n2. The method's reliance on K-Means as a starting point may not be suitable for all types of complex datasets.\\n3. The paper could benefit from more detailed comparisons with other state-of-the-art clustering methods.\",\n  \"Questions\": \"1. How does the choice of K-Means as the initial clustering method impact the overall performance of Village-Net Clustering?\\n2. Can the algorithm be adapted to handle datasets with non-linear or non-convex structures that are more challenging for K-Means?\"\n}"}
{"paper_id": "bAMPOUF227", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel approach to integrating SLMs and LLMs, which demonstrates the potential of combining the strengths of both paradigms.\\n2. The evaluation is thorough and well-conducted, with a diverse set of experiments and datasets used to demonstrate the effectiveness of SuperContext.\\n3. The authors provide a clear explanation of the method and its implications for the field.\\n4. The results show a significant improvement in generalizability and factuality for LLMs when using SuperContext.\",\n  \"Weaknesses\": \"1. The paper assumes that SLMs can be fine-tuned for specific tasks, which might not always be feasible or practical.\\n2. The dependence on curated datasets may limit the applicability of SuperContext to real-world scenarios.\\n3. The authors do not provide a detailed analysis of the computational resources required for training and inference with SuperContext.\",\n  \"Questions\": \"1. How does SuperContext perform when applied to tasks that require more complex reasoning or understanding of domain-specific knowledge"}
{"paper_id": "eNoiRal5xi", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes an innovative method, Unknown Domain Inconsistency Minimization (UDIM), which combines parameter space perturbation with data space perturbation to improve domain generalization.\\n2. The paper provides a solid theoretical foundation for UDIM, grounded in PAC-Bayesian theories and flat minima.\\n3. The experimental results demonstrate statistically significant improvements over SAM variants on multiple DG benchmark datasets.\\n4. The paper addresses a significant gap in existing DG methods by incorporating data space perturbation.\",\n  \"Weaknesses\": \"1. The paper assumes that unknown domains can be simulated by perturbing instances from the source dataset, which might not always be realistic.\\n2. The method relies on the availability of sufficient domain information, which might not be the case in all scenarios.\\n3. The paper does not provide a comprehensive comparison with other state-of-the-art DG methods.\",\n  \"Questions\": \"1. How does UDIM handle scenarios where the source and unknown domains have different distributions, but the domain information is limited?\\"}
{"paper_id": "uU0Adp7Sfo", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel framework, CCGAN, that addresses the limitations of traditional GANs by incorporating a collaboration-competitive scheme. This approach enables the generation of high-quality data that respects additional requirements beyond the authentic data distribution.\\n2. The experimental results demonstrate the effectiveness of CCGAN in generating images of regularly shaped objects, outperforming baseline GAN models.\\n3. The paper provides a comprehensive analysis of the benefits and challenges of the proposed approach.\",\n  \"Weaknesses\": \"1. The paper assumes that the additional knowledge required for multi-modal data generation and image super-resolution can be captured by the proposed collaboration-competitive scheme, which might not be the case in all scenarios.\\n2. The regularization term introduced in CCGAN might not be effective in all cases, and its impact on the overall performance of the model is not fully explored.\\n3. The paper does not discuss the potential limitations of the approach when dealing with complex or irregularly shaped objects.\",\n  \"Questions\": \"1. How does"}
{"paper_id": "dKju7tbe6D", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors successfully integrate iterative and parallel computation into a single neural reasoning architecture, IPRM, which enhances the efficiency and generalizability of visual reasoning models.\\n2. IPRM outperforms existing reasoning mechanisms on several benchmarks, demonstrating stronger generalization and efficiency while using fewer parameters and computation steps.\\n3. The combination of iterative and parallel processing aids the learning of more general reasoning strategies.\",\n  \"Weaknesses\": \"1. The authors do not discuss the potential limitations of IPRM, such as the possibility of overfitting or the requirement for large amounts of training data.\\n2. The comparison to other models, such as MAC and transformer-based attention mechanisms, could be more comprehensive, including additional metrics or more detailed analysis of the results.\\n3. The paper could benefit from more discussion on how IPRM can be applied to more complex visual reasoning tasks or real-world scenarios.\",\n  \"Questions\": \"1. How does IPRM handle cases where the visual input is ambiguous or incomplete, and the"}
{"paper_id": "DL7JWbdGr3", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed PEMs approach effectively leverages pre-training on diverse epidemic data, resulting in improved performance over current state-of-the-art models.\\n2. The use of self-supervised learning tasks and diverse datasets is a well-motivated approach.\\n3. The evaluation on existing forecasting benchmarks showcases the model's adaptability and utility in real-world epidemic monitoring.\",\n  \"Weaknesses\": \"1. The paper assumes a specific type of data availability (multiple datasets from different diseases) and may not generalize to other epidemic scenarios.\\n2. The evaluation on specific diseases and regions might limit the model's applicability to global epidemics.\\n3. The paper does not discuss potential challenges in data integration and heterogeneity, which might affect model performance.\",\n  \"Questions\": \"1. How does the model handle data from emerging or novel diseases with limited historical data?\\n2. Are there any plans to make the pre-trained models publicly available for other researchers to use?\"\n}"}
{"paper_id": "1PaDPHDhwe", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors propose a novel and simple post-processing technique that effectively manages the trade-off between robust and average accuracies without requiring additional training.\\n2. The method introduces a class-specific scaling strategy and an instance-wise adaptive scaling method, which are innovative and well-justified.\\n3. The paper provides a comprehensive evaluation using a novel metric, robust coverage, to capture the trade-off between robust and average accuracies.\\n4. The approach allows for a comprehensive evaluation and understanding of debiasing algorithms by revealing the trade-offs they entail.\",\n  \"Weaknesses\": \"1. The paper relies on pre-trained models, which may limit its applicability to other domains or datasets.\\n2. The instance-wise adaptive scaling method may require careful tuning of hyperparameters, which could be time-consuming and may not generalize well across different datasets.\\n3. The robust coverage metric is novel, but its interpretation and comparison to existing metrics may be challenging.\",\n  \"Questions\": \"1. How does the proposed method handle cases where the pre-trained"}
{"paper_id": "iI7hZSczxE", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed method, DIOSC, tackles a significant challenge in disentangled representation learning for time series data with correlated attributes. Its use of contrastive learning to achieve independence-of-support is an interesting approach. \\n2. The Time Disentangling Score (TDS) metric is a valuable addition to the field, providing a quantitative measure of disentanglement quality. \\n3. The experimental results demonstrate significant improvements over traditional methods, with up to 61.4% better performance in specific metrics.\",\n  \"Weaknesses\": \"1. The paper assumes that contrastive learning can be used to achieve independence-of-support, which might not be the case in all scenarios. It would be beneficial to explore other methods to address correlated attributes. \\n2. The use of self-attention mechanisms in l-variational inference layers might be computationally expensive and require significant resources. \\n3. The paper focuses primarily on energy consumption data, and it would be interesting to see the authors explore its applicability to other time series"}
{"paper_id": "qrGjFJVl3m", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The Qwen-VL series demonstrates exceptional performance in visual-language tasks, setting new benchmarks and improving the state of the art in vision-language dialogs.\\n2. The authors provide a comprehensive evaluation of their models on various benchmarks, including image captioning, VQA, and referring expression comprehension.\\n3. The paper proposes a meticulous design for endowing LLMs with visual capabilities, including a visual receptor, input-output interface, and three-stage training pipeline.\\n4. The Qwen-VL models support multiple languages and include fine-grained visual tasks for enhanced comprehension.\",\n  \"Weaknesses\": \"1. The paper assumes that the Qwen-7B language model foundation is superior to other language models, which may not be the case.\\n2. The evaluation is extensive, but it would be beneficial to include more comparisons with proprietary models.\\n3. The authors plan to further enhance Qwen-VL by integrating additional modalities and increasing the model scale, which may be challenging to achieve.\\n4. The paper does"}
{"paper_id": "qup9xD8mW4", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel method, HaDES, for behaviour distillation in reinforcement learning, which can distill RL information into a small synthetic dataset of state-action pairs.\\n2. The method shows competitive performance in traditional dataset distillation for supervised learning tasks.\\n3. The paper demonstrates the effectiveness of HaDES across various tasks and architectures.\\n4. The authors provide a clear and concise explanation of the method and its evaluation.\",\n  \"Weaknesses\": \"1. The paper assumes the existence of an inner-loop policy, which might not be applicable to all RL tasks.\\n2. The method relies on a fixed set of hyperparameters, which may not be optimal for all environments.\\n3. The authors do not discuss the potential issues of overfitting to the distilled dataset.\",\n  \"Questions\": \"1. How does HaDES handle exploration-exploitation trade-off in reinforcement learning?\\n2. Can HaDES be adapted for more complex tasks, such as multi-agent reinforcement learning?\\n3. What are the potential"}
{"paper_id": "Zw8YxUWL4R", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel and effective method for text-to-image synthesis, allowing for greater control and expressiveness in the synthesis process.\\n2. The Extended Textual Conditioning space, P+, is a valuable innovation, providing disentangled representations of shape and appearance.\\n3. The experiments conducted demonstrate the superiority of P+ and XTI over traditional methods.\",\n  \"Weaknesses\": \"1. The paper assumes the use of a pre-trained Stable Diffusion model, which may limit the applicability of the proposed method to other models.\\n2. The evaluation of XTI is primarily based on a single experiment, which may not be representative of all possible scenarios.\",\n  \"Questions\": \"1. How does the P+ space generalize to other text-to-image models beyond the Stable Diffusion model used in the experiments?\\n2. Can the authors provide more information on the computational cost of the Extended Textual Inversion method and its potential applications in real-world scenarios?\"\n}"}
{"paper_id": "3NXhwkZGjz", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel and effective approach to addressing the primary challenge in SFUDA, which is generating reliable supervision from unlabeled target data. The method is well-justified and provides a clear advantage over existing methods.\\n2. The three-step adaptation process is clear and well-explained, and the use of GradCAM for rationale representations is an interesting innovation.\\n3. The experimental results demonstrate state-of-the-art performance in SFUDA tasks, and the method's flexibility in being integrated into existing approaches is a significant advantage.\",\n  \"Weaknesses\": \"1. The reliance on pre-trained models from the source domain may limit the applicability of the proposed method to scenarios where such models are not available.\\n2. The paper could benefit from a more detailed analysis of the effect of the number of samples used for model pre-adaptation on the overall performance.\\n3. The method may not be robust to cases where the target domain has a very different data distribution compared to the source domain.\",\n  \"Questions\": \"1"}
{"paper_id": "Fn655mJ4bv", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The method SOInter effectively incorporates correlations among output variables when interpreting structured output models, leading to higher-quality explanations.\\n2. The two-phase energy network training approach allows for efficient and scalable computation.\\n3. Experiments demonstrate that SOInter outperforms existing techniques like Lime and Kernel-Shap on both synthetic and real datasets.\\n4. The Gumbel-Softmax trick is used to select important features, which is a clever application of a well-known technique.\",\n  \"Weaknesses\": \"1. The method relies on a specific energy-based approach, which may not be universally applicable to all types of structured output models.\\n2. The pre-training phase may require careful selection of hyperparameters and training configurations.\\n3. The method's performance may degrade if the correlations among output variables are not significant.\",\n  \"Questions\": \"1. How does the method handle cases where the correlations among output variables are not significant or are difficult to capture?\\n2. Can the method be extended to handle more complex structured output models or"}
{"paper_id": "VyMW4YZfw7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a clear and well-supported idea that simpler GNNs can achieve competitive performance with state-of-the-art models.\\n2. The study provides a valuable perspective on the role of evaluation conventions in perceived model performance.\\n3. The paper is well-organized and easy to follow.\",\n  \"Weaknesses\": \"1. The authors might have taken a more nuanced view on the limitations of the proposed method, as it does not necessarily outperform all complex GNNs.\\n2. The impact of the proposed method on more complex tasks than node classification is unclear.\\n3. The method's applicability to other types of networks beyond the ones studied is uncertain.\",\n  \"Questions\": \"1. How generalizable is the proposed method to more complex GNN architectures, and what are the limitations of its applicability?\\n2. What are the implications of the study's findings on the evaluation conventions in GNNs, and how should they be addressed?\\n3. Can the authors comment on potential future directions for improving"}
{"paper_id": "09xFexjhqE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed AutoLoRa method effectively disentangles adversarial and natural objectives in Robust Fine-Tuning, achieving state-of-the-art adversarial robustness across various tasks.\\n2. The introduction of a low-rank branch and heuristic strategies for automatically scheduling learning rates and loss term scalars enhances convergence and reduces hyperparameter sensitivity.\\n3. The paper provides a practical, automated solution for converting pre-trained feature extractors to adversarially robust models, validated by empirical results showing improvements over existing methods.\",\n  \"Weaknesses\": \"1. The novelty of the proposed approach is somewhat diminished by the fact that the concept of low-rank branching is not entirely new, and its application to RFT may not be entirely unexpected.\\n2. The reliance on heuristic strategies for learning rate and loss term scalar scheduling may limit the generalizability of the approach.\\n3. The paper could benefit from a more detailed discussion of the potential trade-offs between adversarial and natural objectives in RFT.\",\n  \"Questions\": \"1. How"}
{"paper_id": "nmBjBZoySX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes an innovative and comprehensive framework for identifying Graph Lottery Tickets (GLT) that addresses existing limitations of GLT methods.\\n2. The method, AdaGLT, integrates pruning and training processes for dynamic workflows, allowing for adaptive sparse structures and automated pruning scheduling.\\n3. Theoretical proofs demonstrate that AdaGLT mitigates over-smoothing and achieves improved sparse structures in deep GNN settings.\\n4. Extensive experiments showcase AdaGLT's superiority over competitors across multiple datasets and settings, particularly in deep GNN scenarios.\",\n  \"Weaknesses\": \"1. The problem of finding sparse subnetworks within GNNs is not new, and the GLT hypothesis has been explored in previous works.\\n2. The novelty of the AdaGLT method lies in its adaptability and scalability, but it may be challenging to apply to extremely large or complex graphs.\\n3. The theoretical proofs are not explicitly stated in the paper, and more detailed explanations would be helpful.\",\n  \"Questions\": \"1. How"}
{"paper_id": "di52zR8xgf", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a substantial enhancement to the Stable Diffusion model, increasing its architecture and introducing novel conditioning schemes to improve performance.\\n2. The introduction of a refinement stage and advanced autoencoder for semantic composition and high-frequency details is a valuable addition.\\n3. The paper provides a clear and well-structured evaluation of SDXL against state-of-the-art black-box models.\",\n  \"Weaknesses\": \"1. The paper does not fully address the challenge of single-stage generation, which might be a limitation of the model.\\n2. The incorporation of additional text synthesis improvements is mentioned as future work, which could be a potential area for further research.\\n3. The paper does not delve deeply into the potential risks of increasing model complexity, which might be a trade-off for the improved performance.\",\n  \"Questions\": \"1. How does the model handle potential issues with overfitting or mode collapse due to the increased complexity?\\n2. Can the refinement stage be further optimized for improved performance and reduced computational costs?\\n3."}
{"paper_id": "0b328CMwn1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors propose a well-motivated extension of visual prompting (VP) with activation prompts (AP), which improves both efficiency and performance. \\n2. The theoretical connection to normalization tuning techniques is insightful and demonstrates a good understanding of the underlying model behavior.\\n3. The extensive experiments across 29 datasets and architectures provide a comprehensive evaluation of AP's performance and efficiency.\",\n  \"Weaknesses\": \"1. While the paper demonstrates the effectiveness of AP, it does not provide a detailed analysis of the potential limitations and edge cases where AP might not perform as well.\\n2. The choice of layer depth for AP implementation significantly affects performance, but the paper does not explore this aspect in-depth.\\n3. The comparison with other PEFT methods is thorough, but it would be beneficial to include a more detailed discussion on how AP compares to other state-of-the-art methods in terms of computational efficiency.\",\n  \"Questions\": \"1. How does AP perform on tasks with limited training data, and what are the implications for the choice of"}
{"paper_id": "AMDKqZcZbi", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant problem in machine learning, catastrophic forgetting, and proposes a biologically inspired neural architecture that achieves both rapid learning and retention of previously acquired knowledge.\\n2. The proposed MESH model is well-structured and combines a content-addressable heteroassociative memory system with a spatially invariant convolutional network architecture.\\n3. The evaluation of the model's performance on the sequential Morris Water Maze task is comprehensive and demonstrates the model's ability to learn rapidly in new environments and retain knowledge across multiple settings.\",\n  \"Weaknesses\": \"1. The paper assumes that the entorhinal-hippocampal circuit is a suitable inspiration for machine learning models, but it is unclear how well this circuit can be replicated in a computational model.\\n2. The MESH model is complex and may be difficult to train and deploy in practice.\\n3. The paper does not provide a clear comparison with other state-of-the-art models that can achieve rapid learning and memory retention.\",\n  \"Questions\": \"1."}
{"paper_id": "am7BPV3Cwo", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses an important problem in OOD detection, specifically the issue of class-aware bias in imbalanced data distributions.\\n2. The proposed ImOOD framework is a comprehensive and well-structured approach that combines post-hoc normalization and training-time regularization techniques.\\n3. The evaluation is thorough and showcases significant improvements over state-of-the-art methods on various benchmarks.\",\n  \"Weaknesses\": \"1. The paper assumes a general understanding of OOD detection and imbalanced data, which might not be familiar to readers without prior knowledge in this area.\\n2. The method relies heavily on the quality of the ID classification model, which might not be robust to varying dataset characteristics.\\n3. The evaluation could benefit from more extensive comparison with other methods and more detailed analysis of the class-aware bias.\",\n  \"Questions\": \"1. How does the ImOOD framework perform on other types of imbalanced data distributions, such as multi-modal or multi-class imbalances?\\n2. Are there any potential limitations or challenges in applying the ImOOD"}
{"paper_id": "R1crLHQ4kf", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes an efficient detection mechanism for adversarial attacks on ASR systems, which is a significant contribution to the field. The approach is well-motivated and has the potential to provide a robust defense against these attacks.\\n2. The evaluation is comprehensive and systematic, showcasing high accuracy across multiple ASR systems and datasets.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The method relies on analyzing the probability distribution of output tokens, which may not be applicable to all ASR systems or datasets.\\n2. The approach may not be effective against highly adaptive attacks that can adjust to the detection strategy.\\n3. The paper could benefit from a more in-depth analysis of the robustness of the detection mechanism against various types of attacks.\",\n  \"Questions\": \"1. How does the proposed detection mechanism perform on ASR systems with different architectures or hyperparameters?\\n2. Can the approach be adapted to detect other types of attacks, such as those targeting other machine learning models"}
{"paper_id": "i7P2mK3x3o", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel and innovative approach to computing a dynamic optimal transport map between two general distributions, addressing a significant gap in the field. This has broad implications for generative models and distribution interpolation.\\n2. The Q-flow model shows strong empirical performance in various tasks, including density ratio estimation and image-to-image translation, leveraging its continuous-time formulation and invertibility.\\n3. The paper provides a clear and well-structured presentation of the method, making it easy to follow and understand.\",\n  \"Weaknesses\": \"1. While the Q-flow model achieves impressive results, the paper could benefit from more comprehensive comparisons with existing methods, particularly those that do not rely on the normal distribution as an intermediary.\\n2. The authors mention that the model is invertible, but it would be helpful to provide more insight into how this property is utilized in the proposed framework and its implications for future work.\\n3. Some of the notation and mathematical derivations could be more detailed, making it easier for readers without a strong background in optimal"}
{"paper_id": "YIls9HEa52", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a well-motivated problem in neural activity and behavior analysis, specifically the limitations of traditional rSLDS models. The proposed irSLDS model provides a flexible and efficient framework for analyzing trial-varying neural data.\\n2. The model's introduction of a semi-Markov state process and latent geometry over states is a significant improvement over existing methods.\\n3. The study demonstrates the efficacy of the irSLDS model on both synthetic and real neural data, showcasing its ability to uncover non-stationary processes not possible with fixed-state models.\\n4. The use of variational Laplace-EM and parallel computing methods enhances computational performance.\",\n  \"Weaknesses\": \"1. The paper assumes that the irSLDS model can handle any level of non-stationarity, but it may not be clear how it performs in scenarios with very high levels of variability.\\n2. The model's reliance on a distance-dependent CRP may not be suitable for all types of neural data, particularly those with non-E"}
{"paper_id": "koYsgfEwCQ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a well-motivated problem in computer vision, specifically the unsupervised 3D dynamic scene decomposition, which is crucial for understanding spatial and temporal information.\\n2. The proposed DynaVol approach, using object-centric voxelization within a differentiable volume rendering framework, outperforms existing methods in unsupervised dynamic scene decomposition and enables scene editing.\\n3. The paper provides a clear and well-organized presentation of the method and its results.\",\n  \"Weaknesses\": \"1. The paper assumes the availability of 2D images and 3D scene annotations, which might be a limitation in real-world scenarios where such annotations are not readily available.\\n2. The model's performance is evaluated on a limited number of datasets, and it is unclear how well DynaVol generalizes to other scenarios or datasets.\\n3. The paper does not provide a thorough comparison with other 3D methods, making it difficult to fully assess the contribution of DynaVol.\",\n  \"Questions\": \"1."}
{"paper_id": "o6AN2ZoNw2", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed method, P-Seg, is a simple yet effective solution for open-vocabulary semantic segmentation, leveraging pseudo-mask generation and language supervision.\\n2. The paper demonstrates state-of-the-art results on various benchmarks, showcasing its scalability and competitiveness.\\n3. The approach avoids the need for annotated masks and large pre-trained models, making it a more efficient alternative to prior methods.\",\n  \"Weaknesses\": \"1. The reliance on pseudo-mask generation might introduce some noise or inaccuracies in the training process.\\n2. The effectiveness of P-Seg might be limited to the specific MaskFormer architecture used, and its applicability to other models is unclear.\\n3. The paper does not provide a thorough comparison with other simple yet efficient methods for open-vocabulary segmentation.\",\n  \"Questions\": \"1. How does the pseudo-mask generator handle cases where the generated masks are not accurate or do not align with the actual object boundaries?\\n2. Can P-Seg be adapted for other tasks beyond semantic segmentation, such as object detection"}
{"paper_id": "HXZK1Z8tHa", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes an innovative method to address the trade-offs between model latency and trainability in Transformer-based image restoration tasks. The authors present a clear and well-structured approach to sharing attention maps and introducing residual connections for improved optimization.\\n2. The evaluation is comprehensive and systematic, showcasing the effectiveness of the proposed method across various image restoration tasks and datasets.\\n3. The authors demonstrate a clear understanding of the current state-of-the-art methods and their limitations, providing a solid foundation for their proposed solution.\",\n  \"Weaknesses\": \"1. The evaluation primarily focuses on the proposed method's ability to reduce latency and improve trainability, but it would be beneficial to explore other aspects, such as the model's robustness to various types of image degradation.\\n2. The paper does not provide an in-depth analysis of the impact of the proposed method on the model's interpretability and explainability.\\n3. The authors mention the potential for further applications of the proposed method to high-level vision problems, but this seems like a stretch without"}
{"paper_id": "QO3yH7X8JJ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a simple yet effective noise injection methodology that leverages pre-trained DGMs for arbitrary-scale super-resolution tasks.\\n2. The proposed method, Diff-SR, achieves superior performance over existing methods across various scales, as evidenced by lower FID scores and competitive PSNR and SSIM metrics.\\n3. The work highlights the flexibility and adaptability of the method to different upsampling scenarios without retraining specific models.\",\n  \"Weaknesses\": \"1. The paper assumes the availability of pre-trained DGMs, which might not always be the case, especially for specific domains or tasks.\\n2. The method's dependence on the Perceptual Recoverable Field (PRF) might limit its applicability to other domains or tasks where PRF is not well-defined.\\n3. The paper could benefit from a more in-depth analysis of the impact of noise injection on the quality of the generated images.\",\n  \"Questions\": \"1. How does the noise injection methodology affect the computational complexity of the Diff-SR"}
{"paper_id": "sNtDKdcI1f", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The study provides a thorough analysis of the impact of output length on the effectiveness of RLHF, shedding light on a long-standing issue in the field.\\n2. The experiments are well-designed and comprehensive, covering multiple datasets and interventions to mitigate length biases.\\n3. The paper suggests a significant contribution to the field by highlighting the need for more robust reward models.\",\n  \"Weaknesses\": \"1. The authors may have oversimplified the role of length in RLHF by focusing primarily on its contribution to reward increases.\\n2. The study does not provide a clear solution to the problem of designing more robust reward models.\\n3. Some may argue that the findings are not entirely surprising, as the correlation between length and reward is well-known.\",\n  \"Questions\": \"1. How can the findings of this study be generalized to other applications of RLHF beyond question answering and dialogue?\\n2. What specific design changes or techniques could be used to create more robust reward models that capture human preferences beyond shallow correlations like length?\"\n}"}
{"paper_id": "HFtrXBfNru", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed method SMART effectively addresses the challenge of temporal generalization estimation in evolving graphs, showcasing its potential in real-world applications.\\n2. The self-supervised approach of SMART for graph reconstruction is innovative and well-executed.\\n3. The authors provide a comprehensive evaluation of SMART on both synthetic and real-world data, demonstrating its superiority over existing methods.\\n4. The study's focus on minimizing representation distortion and improving generalization estimation is timely and relevant.\",\n  \"Weaknesses\": \"1. The paper assumes a relatively simple graph evolution scenario, which might limit its applicability to more complex real-world scenarios.\\n2. The evaluation of SMART's performance on larger, more complex graphs would provide a more comprehensive understanding of its effectiveness.\\n3. While the authors discuss the importance of structure graph reconstruction, the role of feature reconstruction in the overall performance of SMART could be explored further.\",\n  \"Questions\": \"1. How does SMART perform on graphs with more complex evolution patterns, such as graphs with dynamic edge weights or node attributes?\\"}
{"paper_id": "4u0ruVk749", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach to estimating the Individual Treatment Effect (ITE) using a diffusion model, which effectively captures unobserved confounders and improves causal inference tasks.\\n2. The method's two-step diffusion process is well-motivated and well-designed, addressing the limitations of existing generative models.\\n3. The experimental comparisons with 12 state-of-the-art models demonstrate the efficacy of the proposed approach.\",\n  \"Weaknesses\": \"1. The paper assumes that the prior knowledge used in the reverse diffusion process is available and accurate, which may not be the case in real-world scenarios.\\n2. The model's performance relies heavily on the quality of the observed confounders, which may not always be available or reliable.\\n3. The paper could benefit from more discussion on the practical applications and limitations of the proposed approach.\",\n  \"Questions\": \"1. How does the proposed approach handle cases where the prior knowledge is incomplete or inaccurate?\\n2. Are there any potential biases in the observed confounders"}
{"paper_id": "Pa6SiS66p0", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The study identifies a clear gap in the exploration of multi-modal learning to mitigate forgetting in DNNs, and proposes a novel approach to address this gap.\\n2. The proposed method, SAMM, leverages relational structural similarities between data points in each modality for integration and alignment, and demonstrates improved performance in challenging continual learning settings.\\n3. The study provides a comprehensive benchmark for multi-modal continual learning using the VGGSound dataset and sets a strong baseline for further research.\\n4. The paper highlights the potential of multi-modal learning to develop robust models that can adapt to dynamic and evolving environments.\",\n  \"Weaknesses\": \"1. The paper assumes that the VGGSound dataset is a representative benchmark for multi-modal continual learning, but it is unclear whether the results can be generalized to other datasets.\\n2. The study focuses primarily on class-incremental, domain-incremental, and generalized class-incremental learning, but it is unclear how the proposed method would perform in other types of continual learning settings.\\n"}
{"paper_id": "rkplYfqUr0", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper introduces a novel approach to zero-shot text classification using generative prompting, addressing the issues of miscalibration and brittleness in discriminative prompting.\\n2. The experiments conducted across six language model families and 19 classification tasks demonstrate robustness to prompt variations and improved performance.\\n3. The use of paraphrases to reduce variance and manually crafted label descriptions is a strength.\\n4. The personalized classification enabled by GEN-Z is a valuable contribution to the field.\",\n  \"Weaknesses\": \"1. The reliance on manual crafting of label descriptions and paraphrases might be time-consuming and difficult to scale.\\n2. The paper assumes the availability of a good language model and does not address potential issues with model quality or prompt engineering.\\n3. The experiments are limited to six language model families, and the performance on other models may differ.\\n4. The comparison to traditional zero-shot and few-shot baselines may not be comprehensive enough.\",\n  \"Questions\": \"1. How does GEN-Z perform on tasks with multiple"}
{"paper_id": "pTU2X9mUBe", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. LaDe is a comprehensive and diverse dataset that addresses the lack of publicly available last-mile delivery data, which can significantly advance research in logistics.\\n2. The dataset is large-scale and was collected from a real-world logistics platform, making it suitable for a range of tasks beyond those benchmarked.\\n3. The paper clearly presents the properties and utility of the dataset, and its potential for enhancing the development of algorithms in last-mile delivery.\",\n  \"Weaknesses\": \"1. The paper primarily focuses on introducing a new dataset, which might be seen as incremental rather than groundbreaking.\\n2. The evaluation of the dataset's utility is limited to a few baseline models and tasks, and it would be beneficial to see more comprehensive evaluation or applications of the dataset.\\n3. The paper does not discuss potential issues with data quality, such as missing values or biases, which could be crucial for researchers using the dataset.\",\n  \"Questions\": \"1. How does the dataset address the potential issues of missing values or biases in the data?\\n"}
{"paper_id": "uhtQyRrTzY", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a novel and effective method for generating large-scale, real-world multi-view datasets without requiring 3D annotations, which is a significant contribution to the field of dense vision tasks.\\n2. The approach leverages computer vision techniques such as SIFT and RANSAC, demonstrating a good understanding of the state-of-the-art methods in the field.\\n3. The evaluation on various downstream tasks shows promising results, including substantial improvements in performance on dense geometric and object-related tasks with fewer training samples.\",\n  \"Weaknesses\": \"1. The paper assumes that SIFT and RANSAC can accurately detect correspondences between views, which might not always be the case, especially in scenes with low texture or high occlusion.\\n2. The method relies on the quality of the input videos and synthetic environments, which may affect the accuracy of the generated datasets.\",\n  \"Questions\": \"1. How does the method perform on datasets with varying levels of occlusion or texture? Are there any adaptations or modifications to SIFT and R"}
{"paper_id": "WZfatbNdLV", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The authors proposed a novel and effective model, TrASPr, for predicting tissue-specific RNA splicing, outperforming state-of-the-art models.\\n2. The use of transformers and Bayesian Optimization framework (BOS) is innovative and promising for designing RNA sequence alterations for specific splicing outcomes.\\n3. The paper provides a clear and well-structured explanation of the methodology and results.\",\n  \"Weaknesses\": \"1. The complexity and variability of splicing mechanisms across different cellular conditions and datasets make it challenging to design a model that accurately predicts RNA splicing outcomes in a tissue-specific manner.\\n2. The model's performance relies heavily on the quality of the training datasets, which may not be universally applicable.\",\n  \"Questions\": \"1. How does the TrASPr model handle the complexity of splicing mechanisms across different cellular conditions and datasets?\\n2. Can the model be adapted to other types of RNA splicing or regulatory mechanisms?\"\n}"}
{"paper_id": "YkRwadXWHd", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel multi-modal framework for continuous sign language recognition (CSLR) that achieves state-of-the-art performance on several datasets. This is a significant contribution to the field, especially considering the challenges of CSLR. \\n2. The hierarchical knowledge distillation (HKD) framework is an interesting idea that can help reduce computational costs while maintaining performance. \\n3. The evaluation is comprehensive, with results on multiple datasets and metrics.\",\n  \"Weaknesses\": \"1. The paper assumes that the availability of keypoints and optical flow data is not a concern, which may not be the case in real-world applications. \\n2. The impact of the HKD framework on the performance is not thoroughly analyzed, and more insights would be beneficial. \\n3. The paper does not discuss potential limitations of the multi-modal approach, such as increased complexity and potential overfitting.\",\n  \"Questions\": \"1. How does the proposed multi-modal framework perform on datasets with limited keypoints and optical flow data?\\n2. Can"}
{"paper_id": "q4Bim1dDzb", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed UniVoxel framework provides a unified and efficient approach to inverse rendering, significantly accelerating the process while maintaining high rendering quality.\\n2. The use of Spherical Gaussians for modeling local incident light radiance is innovative and effective.\\n3. The method demonstrates robustness across diverse datasets and varying lighting conditions.\",\n  \"Weaknesses\": \"1. The paper assumes a controlled environment with a fixed number of viewpoints, which may not be realistic for real-world scenarios.\\n2. The evaluation of UniVoxel is primarily focused on synthetic datasets, and its performance on real-world data remains to be seen.\\n3. The method may struggle with complex scenes that require high-frequency details or intricate geometry.\",\n  \"Questions\": \"1. How would UniVoxel perform on real-world datasets with varying lighting conditions and multiple sources?\\n2. Can the method be extended to handle dynamic scenes or scenes with moving objects?\\n3. Are there any potential limitations or trade-offs in using Spherical Gaussians for modeling local incident light radi"}
{"paper_id": "39cPKijBed", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel method for mitigating bias in diffusion models using time-dependent importance reweighting, which is both theoretically sound and practically effective.\\n2. The approach uses a time-dependent density ratio for reweighting and score correction, allowing for a more precise form of the objective function and improving model convergence towards an unbiased data distribution.\\n3. The experimental results demonstrate the superiority of the proposed method over existing baselines, showcasing its applicability and significance in enhancing the generative quality of diffusion models.\",\n  \"Weaknesses\": \"1. The method relies on estimating a time-dependent density ratio, which can be computationally expensive and may require significant resources for large-scale datasets.\\n2. The approach assumes that the time-dependent density ratio can be accurately estimated, which may not always be the case in practice, particularly for complex datasets.\",\n  \"Questions\": \"1. How does the proposed method handle cases where the time-dependent density ratio is not accurately estimated, and what are the potential consequences for model convergence?\\n2."}
{"paper_id": "aG3EARrrd1", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes an effective solution for handling large dynamic datasets with the Online Sparse Residual Tree (OSRT) model, which integrates tree decomposition and adaptive RBF exploration.\\n2. The approach is well-motivated by the limitations of existing RBF-based online learning methods and provides a feasible and efficient technique for evolving data tasks.\\n3. The evaluation on benchmark time series datasets like Mackey-Glass and Lorenz time series is comprehensive and demonstrates the model's superiority over existing online RBF networks.\",\n  \"Weaknesses\": \"1. The novelty of the approach is somewhat limited, as tree-based structures and RBF networks have been combined in other hybrid models.\\n2. The complexity of the model, which involves adaptive RBF exploration and tree decomposition, might make it challenging to implement and tune.\\n3. The paper does not provide a clear explanation of how the model handles the trade-off between model sparsity and accuracy in different scenarios.\",\n  \"Questions\": \"1. Can the OSRT model be applied to other"}
{"paper_id": "Ts95eXsPBc", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses a significant gap in current episodic memory models by incorporating spatial information, which is a crucial aspect of human cognition. \\n2. The proposed Spatially-Aware Transformers (SAT) and Adaptive Memory Allocator (AMA) are innovative and show promise in improving memory utilization efficiency and accuracy in place-centric tasks. \\n3. The experiments conducted across various environments and tasks demonstrate the effectiveness of the proposed models.\",\n  \"Weaknesses\": \"1. The paper assumes that incorporating spatial information will inherently improve memory utilization efficiency and accuracy, but this assumption may not hold across all scenarios. \\n2. The adaptive memory allocator relies on reinforcement learning, which can be computationally expensive and may not be feasible for all applications. \\n3. The experiments could benefit from a more detailed analysis of the benefits of incorporating spatial information compared to traditional approaches.\",\n  \"Questions\": \"1. How do the authors plan to address the potential computational costs associated with the adaptive memory allocator in real-world applications? \\n2. Can the proposed models"}
{"paper_id": "LfhG5znxzR", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel method for introducing sparse and discrete bottlenecks into neural networks, which enhances interpretability and controllability without degrading performance.\\n2. The method involves trainable vector quantization bottlenecks at each layer, refining the network to replace continuous activations with a sparse combination of code vectors from a codebook.\\n3. The authors apply their approach to various datasets, including a finite state machine dataset and natural language datasets, and demonstrate the effectiveness of their method.\",\n  \"Weaknesses\": \"1. The paper relies on cosine similarity to select the most representative codes for activation, which might not be the most effective or robust method for all scenarios.\\n2. The authors could have provided more detailed analysis on the trade-off between interpretability and performance, as well as the impact of codebook size on the results.\\n3. The paper assumes that the codebook features are sufficient for controlling network behavior, but it would be beneficial to explore the limitations and potential drawbacks of this approach.\",\n  \"Questions\": \""}
{"paper_id": "bUGzjiUsIq", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed PAPG framework effectively handles the challenges of partially annotated multi-label classification, demonstrating superior performance over traditional methods and baselines. \\n2. The framework shows robustness and generalizes well across different datasets and domains, substantially improving F1 scores and other metrics compared to existing methods.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The concept of integrating reinforcement learning with partial annotation is not novel, and the approach may not be generalizable to other domains beyond multi-label classification. \\n2. The paper does not provide a comprehensive comparison with state-of-the-art methods for multi-label classification, which may limit the impact of the proposed framework.\",\n  \"Questions\": \"1. How does the PAPG framework handle the trade-off between exploration and exploitation in the Markov Decision Process (MDP)?\\n2. Can the PAPG framework be extended to other types of partially annotated datasets, such as image classification or natural language processing?\"\n}"}
{"paper_id": "uMAujpVi9m", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed method, ProFSA, effectively addresses the issue of protein-ligand data scarcity by pretraining pocket representations using segmentations of high-resolution atomic protein structures and pseudo-ligand representations. This approach has the potential to improve ligand-receptor interactions and open up avenues for using diverse protein structure databases for pretraining.\\n2. The method's performance is significantly better than existing methods on tasks like pocket druggability prediction, pocket matching, and ligand binding affinity prediction.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The novelty of the proposed method, ProFSA, is not explicitly clear. While the approach of pretraining pocket representations using segmentations of high-resolution atomic protein structures and pseudo-ligand representations is a step in the right direction, it is not a new concept.\\n2. The alignment of data distribution to mimic real ligand interactions may be challenging and requires further clarification.\\n3. The paper does not discuss the scalability of the proposed"}
{"paper_id": "uqxBTcWRnj", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper makes a significant contribution to the field by proposing a transitional representation system that bridges the gap between neural and symbolic representations, leveraging unsupervised learning to create intermediate representations that maintain high-dimensional details and convey structural information about input semantics.\\n2. The proposed TDL framework, which combines Expectation Maximization and game-theoretic diffusion models, is innovative and well-suited for learning reusable and compositional predicates.\\n3. The introduction of new metrics, clustering information gain and heuristic shape score, is a welcome addition to the field, providing a more comprehensive evaluation framework.\\n4. The experiments demonstrate the effectiveness of the TDL framework in discovering compositional patterns and outperforming other unsupervised part segmentation methods.\",\n  \"Weaknesses\": \"1. The reliance on EM algorithm and game-theoretic diffusion model might limit the scalability and efficiency of the proposed framework, particularly for large and complex datasets.\\n2. The paper assumes that the transitional representation system can seamlessly transition from neural to symbolic representations, which may not be"}
{"paper_id": "o4CLLlIaaH", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper introduces a novel approach to generalizable neural radiance fields using point-based rendering, addressing existing issues with feature inconsistencies and occlusions. This work has the potential to revolutionize the field of 3D scene representation.\\n2. The proposed Generalizable neural Point Field (GPF) is well-designed, with innovations such as visibility-oriented feature fetching and a nonuniform log sampling strategy.\\n3. The experimental validation on various datasets demonstrates the effectiveness of GPF in both generalization and finetuning settings.\",\n  \"Weaknesses\": \"1. The paper could benefit from more detailed comparisons with existing state-of-the-art methods, particularly in terms of computational efficiency and rendering speed.\\n2. The potential limitations of the point-based approach, such as increased complexity and computational requirements, are not thoroughly explored.\\n3. The paper assumes a certain level of prior knowledge about neural radiance fields and their applications, which might make it challenging for non-experts to understand the context and significance of the work.\",\n  \"Questions\": \""}
{"paper_id": "KNzL1nglNB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel approach to address the limitation of ERM in scenarios with insufficient labeled data, leveraging label encodings to enhance prediction discriminability and diversity.\\n2. The theoretical analysis provides a clear link between LRM, neural class-mean collapse, and ERM, providing a strong foundation for the proposed method.\\n3. The experimental results demonstrate the effectiveness of LRM in multiple label insufficient scenarios, including SSL, UDA, and SHDA, showcasing its potential as a valuable extension to ERM.\",\n  \"Weaknesses\": \"1. While LRM shows promise, the reliance on label encodings as guidance information may still be limited in certain scenarios, particularly when the label encodings are not well-defined or are noisy.\\n2. The extension to other areas with similar challenges may be challenging, as the specific characteristics of these domains need to be taken into account.\\n3. Further investigation into the relationship between LRM and other existing methods, such as EntMin and BNM, is necessary to fully understand its advantages"}
{"paper_id": "pvhyBB86Bt", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The study extends traditional analyses by utilizing recent results from random matrix theory to propose a general stability theorem for the Jacobian that covers training. This is a significant contribution to the field.\\n2. The paper provides evidence for the stability of sparse networks when weights are suitably scaled after pruning.\\n3. The study's focus on the behavior of the Jacobian matrix during training is an important area of research.\",\n  \"Weaknesses\": \"1. The paper's novelty is somewhat limited, as it builds upon existing research by Pennington et al. (2017) and Brailovskaya and van Handel (2022).\\n2. The study's conclusions are based on a specific model (MLPs) and may not generalize to other types of neural networks.\",\n  \"Questions\": \"1. Can the stability theorem be extended to other types of neural networks beyond MLPs?\\n2. How does the scaling of weights after pruning affect the stability of the Jacobian in different types of neural networks?\"\n}"}
{"paper_id": "Mylk5iamJC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a unified approach to address both closed-set recognition (CSR) and open-set recognition (OSR) using a single generative model, which is a significant contribution.\\n2. The proposed Latent Gaussian Mixture Model (L-GMM) framework effectively combines generative and discriminative modeling, showing superior performance to discriminative models and existing specialized methods in both CSR and OSR.\\n3. The paper is well-organized and clearly presented, with a clear explanation of the methodology and results.\",\n  \"Weaknesses\": \"1. The novelty of the approach may be overstated, as the idea of using generative models for CSR and OSR is not entirely new, and some existing methods have explored similar approaches.\\n2. The paper could benefit from a more detailed analysis of the trade-offs between CSR and OSR performance, as the results seem to show improvements in both, but the discussion is somewhat limited.\\n3. The proposed framework may not be immediately applicable to other domains beyond image recognition and segmentation,"}
{"paper_id": "MsOcVFzv8D", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper fills a significant gap in the field by providing a theoretical framework for multi-domain text classification, addressing the lack of guarantees in existing methods.\\n2. The proposed method, MDAT, outperforms state-of-the-art approaches in empirical tests and offers a more comprehensive understanding of MDTC algorithms.\\n3. The paper is well-organized and clearly presented, with a logical flow from background to methodology to conclusion.\",\n  \"Weaknesses\": \"1. The theoretical analysis is based on Rademacher complexity, which might be challenging to generalize to other settings or domains.\\n2. The experimental evaluation is limited to two benchmarks, and more comprehensive testing on various datasets would strengthen the findings.\\n3. The paper assumes access to domain-specific data, which might not be feasible in real-world scenarios.\",\n  \"Questions\": \"1. How does the Rademacher complexity-based generalization bound extend to more complex domain adaptation scenarios?\\n2. Are there any plans to extend the MDAT method to other types of domain adaptation tasks"}
{"paper_id": "fBlHaSGKNg", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant gap in semi-supervised learning by proposing an efficient method for sample selection under budget constraints.\\n2. The approach, UPA, leverages the \u03b1-Maximum Mean Discrepancy criterion and the Generalized Kernel Herding without Replacement algorithm to select a diverse and representative subset for annotation.\\n3. The experimental results demonstrate the effectiveness of UPA in improving SSL performance across various setups, including constrained budgets.\\n4. The integration with popular SSL frameworks like FlexMatch and FreeMatch is a significant strength of this work.\",\n  \"Weaknesses\": \"1. While the proposed method is effective, it relies on the choice of the \u03b1 parameter, which may not be trivial to tune.\\n2. The paper could benefit from a more detailed discussion on the theoretical guarantees of the proposed method and its comparison to existing active learning and semi-supervised active learning methods.\\n3. The experimental evaluation could be strengthened by including more diverse datasets and a more comprehensive comparison with state-of-the-art methods.\",\n  \""}
{"paper_id": "HwQ8NVvdmm", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses a crucial problem in continual learning by proposing a method that maintains model accuracy while reducing computational resource consumption.\\n2. The authors' approach to Hadamard domain quantization is novel and effectively leverages existing techniques to improve efficiency.\\n3. The evaluation is comprehensive, with comparisons to several state-of-the-art CL methods.\",\n  \"Weaknesses\": \"1. While the method shows promising results, the evaluation is limited to a specific set of CL techniques and datasets, making it difficult to generalize to other scenarios.\\n2. The authors do not discuss potential issues with catastrophic forgetting in more detail, which might be a concern for certain applications.\",\n  \"Questions\": \"1. How does the proposed method handle the trade-off between accuracy and quantization levels, particularly for different types of datasets?\\n2. Can the authors provide more insights into the choice of quantization levels and stochastic rounding, and how these impact the overall performance?\"\n}"}
{"paper_id": "cZo6pDtDZr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors propose two novel algorithms that significantly improve the sample complexity of locally differentially private estimation of collision probability and enable sequential testing without knowing the privacy parameter epsilon.\\n2. The private estimation algorithm achieves near-optimal sample complexity and outperforms previous methods.\\n3. The sequential testing algorithm efficiently determines hypothesis distinctions without pre-specified sample limits.\",\n  \"Weaknesses\": \"1. The paper assumes the users and server have computational capabilities, which may not be feasible in all scenarios.\\n2. The practicality of the proposed algorithms and their scalability to large datasets are not thoroughly discussed.\",\n  \"Questions\": \"1. Can the proposed algorithms be adapted for other privacy metrics, such as differential privacy or concentration inequalities?\\n2. How do the authors envision the distributed user-server protocol being implemented in practice, and what are the potential bottlenecks in such a setup?\"\n}"}
{"paper_id": "F7QnIKlC1N", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper introduces a novel Graph Transformer-based architecture, GTMGC, with the MSRSA mechanism, which effectively predicts the 3D ground-state conformation of molecules from their 2D topological architectures.\\n2. The model is competitive with or superior to existing state-of-the-art models and open-source tools like RDkit on benchmarks from the Molecule3D and QM9 datasets.\\n3. The MSRSA module shows robustness and general applicability for molecular property predictions.\",\n  \"Weaknesses\": \"1. The paper does not thoroughly discuss the limitations of the MoleBERT Tokenizer and Laplacian Positional Encoding methods, which might be restrictive in certain cases.\\n2. It is unclear whether the MSRSA mechanism can be easily adapted to predict other molecular properties or structures.\",\n  \"Questions\": \"1. Can the MSRSA mechanism be applied to other molecular properties or structures, and if so, what are the necessary modifications?\\n2. How does the GTMGC model handle molecules with complex top"}
{"paper_id": "8JCn0kmS8W", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. WavJourney effectively addresses the limitations of existing audio generation systems by proposing a comprehensive audio creation framework that integrates multiple audio elements controlled by text instructions.\\n2. The use of Large Language Models (LLMs) to generate an audio script from text instructions is innovative and promising.\\n3. The framework is designed to be compositional and interpretable, leveraging different expert audio models without additional training.\",\n  \"Weaknesses\": \"1. The paper does not provide a detailed explanation of how the task-specific audio generation models and computational functions are invoked to produce the final audio content.\\n2. The efficiency of the framework is mentioned as a potential limitation, which may impact its practical application.\\n3. The artificial composition aspect of the generated audio remains an area for further exploration.\",\n  \"Questions\": \"1. How does the framework handle the complexity of integrating multiple audio elements, such as speech, music, and sound effects, into a coherent audio script?\\n2. Can the framework be adapted to generate audio content for other domains,"}
{"paper_id": "XbLffB0T2z", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes an effective approach to enhance the transferability of availability poisoning attacks across different learning paradigms. This work highlights a significant threat to model robustness.\\n2. The use of high-frequency perturbations in conjunction with both supervised and unsupervised contrastive learning is an innovative idea.\\n3. The experimental evaluation is thorough and demonstrates the efficacy of the proposed approach across various datasets and learning algorithms.\",\n  \"Weaknesses\": \"1. The paper assumes that the victim model uses a combination of supervised and unsupervised contrastive learning, which may not be the case in practice.\\n2. The authors do not discuss the potential limitations of using high-frequency perturbations, such as potential overfitting or increased computational cost.\",\n  \"Questions\": \"1. How does the proposed approach handle scenarios where the victim model uses a different architecture or hyperparameters?\\n2. Can the use of high-frequency perturbations be adapted for other types of attacks or vulnerabilities?\"\n}"}
{"paper_id": "4y3GDTFv70", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a comprehensive theoretical framework for emergent abilities in LLMs, attributing them to Bayesian inference on sparse joint distributions of languages.\\n2. The study offers a novel perspective on the capabilities of LLMs, shedding light on the underlying mechanisms driving their abilities.\\n3. The simulation experiments using doubly-embedded Markov chains provide a robust validation of the proposed framework.\\n4. The paper highlights the scaling behavior of LLMs' emergent qualities, suggesting that model size and data breadth enhance these abilities.\",\n  \"Weaknesses\": \"1. The theoretical framework relies on the assumption that LLMs can accurately approximate marginal distributions, which might not always hold in practice.\\n2. The study focuses primarily on synthetic data, and it is unclear how the framework generalizes to real-world language data.\\n3. The paper does not provide explicit evaluation on real-world datasets, which limits its applicability and generalizability.\",\n  \"Questions\": \"1. How does the proposed framework account for the role of"}
{"paper_id": "ODSgo2m8aE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper fills a gap in the research on fairness and stability in Graph Neural Networks (GNNs) by introducing a method to apply Lipschitz bounds to constrain biases and stabilize outputs.\\n2. The proposed method, JacoLip, is well-implemented and integrates seamlessly with existing GNN training pipelines in PyTorch.\\n3. The study provides a comprehensive theoretical analysis and empirical validation of the proposed method, demonstrating its effectiveness in enhancing fairness and stability.\",\n  \"Weaknesses\": \"1. The paper assumes that the biases in graph data are implicit, which might not always be the case in real-world scenarios where biases can be explicit.\\n2. The method may not be directly applicable to GNNs with complex node and edge features, where calculating Lipschitz bounds could be computationally expensive.\\n3. The evaluation of the proposed method is limited to node classification and link prediction tasks, and it would be interesting to see its performance on more complex tasks.\",\n  \"Questions\": \"1. Can the proposed"}
{"paper_id": "RlfD5cE1ep", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a theoretical analysis of the influence of feature normalization on non-contrastive learning dynamics, which is a significant contribution to the field.\\n2. The incorporation of cosine loss is a useful extension to existing L2-loss-based analyses, providing new insights into the learning dynamics.\\n3. The eigenmode dynamics analysis is a clear and well-presented method for understanding the behavior of non-contrastive learning.\",\n  \"Weaknesses\": \"1. The paper assumes the use of feature normalization in practice, which might not be the case for all self-supervised learning scenarios.\\n2. The analysis is limited to a high-dimensional limit and synthetic data, which may not accurately represent real-world scenarios.\\n3. The paper does not provide clear implications for the design of non-contrastive learning algorithms beyond the specific case studied.\",\n  \"Questions\": \"1. Can the authors provide more insight into how their findings can be generalized to other types of self-supervised learning methods?\\n2. How do the authors envision feature normalization"}
{"paper_id": "18TfucMNTr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant challenge in deep learning optimization, effectively proposing a solution to improve training outcomes for deep neural networks.\\n2. The continuation method with regularization is a well-executed approach that demonstrates the potential to enhance training stability and accuracy.\\n3. The paper provides thorough evaluation on benchmark problems, showcasing the effectiveness of the proposed method.\\n4. The use of Monte Carlo techniques for efficient evaluation of high-dimensional optimization is a valuable contribution.\",\n  \"Weaknesses\": \"1. The paper focuses on a specific challenge in deep learning optimization, but the generalizability of the proposed method to other optimization challenges remains unclear.\\n2. The choice of regularization term and its parameters might require further investigation to ensure its effectiveness across different problem domains.\\n3. The paper relies heavily on the effectiveness of Gaussian continuation, which may not be universally applicable.\\n4. The evaluation on benchmark problems is limited to a few datasets, and more comprehensive evaluation on a wider range of problems would be beneficial.\",\n  \"Questions\": \"1. How"}
{"paper_id": "JGTC6WgO4T", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses an important gap in existing Black-Box Domain Adaptation (BDA) research by considering multiple source domains, which is a more realistic setting in real-world scenarios.\\n2. The proposed framework, Label Space-Induced Pseudo Label Refinement (LPR), is well-designed and theoretically supported, demonstrating its potential to refine pseudo labels and improve the performance of multi-source BDA.\\n3. The experiments conducted on four benchmark datasets demonstrate competitive performance compared to state-of-the-art domain adaptation methods.\",\n  \"Weaknesses\": \"1. The paper assumes that the source and target domains have a shared label space, which might not always be the case in real-world scenarios.\\n2. The proposed framework relies heavily on the quality of the source predictions, which can be a limitation in cases where the source model is not accurate.\\n3. The paper does not provide a clear discussion on how to select the optimal number of source domains and their corresponding weights.\",\n  \"Questions\": \"1. How does the paper handle"}
{"paper_id": "fQHb1uZzl7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel architecture, UFC, which effectively unifies feature aggregation and cost aggregation for improved dense matching performance.\\n2. The method demonstrates state-of-the-art performance on various benchmarks, showcasing its robustness and efficiency.\\n3. The use of Transformers for unifying feature and cost aggregation is innovative and well-executed.\",\n  \"Weaknesses\": \"1. The paper assumes a clear understanding of the reader on Transformers and their application in computer vision, which might be a barrier for non-experts.\\n2. The comparison to existing methods is limited to a few recent works, which may not provide a comprehensive overview of the current state-of-the-art.\\n3. The evaluation metrics used do not fully capture the nuances of dense matching tasks, and more comprehensive metrics could provide a clearer understanding of the method's strengths and weaknesses.\",\n  \"Questions\": \"1. Can the proposed UFC method be extended to other dense matching tasks beyond semantic and geometric matching?\\n2. How does the UFC method handle cases where the feature and"}
{"paper_id": "YjG29CIOP7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed approach effectively addresses the Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC) issues in Data-free Meta-learning (DFML), making it a significant advancement in the field.\\n2. The introduction of TEAPOT and ROSY components is well-motivated and well-implemented, and their effectiveness is demonstrated through extensive experiments on multiple datasets.\\n3. The authors provide a clear and concise explanation of their method and results, making it easy to follow and understand.\",\n  \"Weaknesses\": \"1. While the proposed approach is effective, it may be challenging to extend it to more complex scenarios, such as tasks with multiple modalities or large-scale datasets.\\n2. The paper assumes that the pre-trained models are available, which may not always be the case in real-world scenarios.\\n3. The evaluation metrics used may not fully capture the complexity of the task, and more comprehensive evaluation methods should be considered.\",\n  \"Questions\": \"1. How does the proposed approach perform in scenarios"}
{"paper_id": "DmDpu3r8iU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a crucial concern in LLMs, their potential misalignment with human values, and proposes a framework to assess this alignment.\\n2. The Value Understanding Measurement (VUM) framework is well-developed and comprehensive, covering both 'know what' and 'know why' elements of value recognition and understanding.\\n3. The study uses a large and diverse dataset, the Schwartz Value Survey, to evaluate the performance of five LLMs.\",\n  \"Weaknesses\": \"1. The paper might benefit from a more detailed discussion on the limitations and potential biases of the VUM framework, particularly in relation to the Schwartz Value Survey.\\n2. The study's focus on GPT-4 as the primary LLM may limit the generalizability of the results to other models.\\n3. The conclusion could be more nuanced in discussing the implications of the findings, particularly in relation to the 'know why' capabilities of LLMs.\",\n  \"Questions\": \"1. How would the VUM framework perform on L"}
{"paper_id": "kXHEBK9uAY", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The Hierarchical Diffuser is a well-designed framework for combining hierarchical planning with diffusion-based modeling, addressing the limitations of current diffusion-based planning methods. The use of two diffusers is an interesting and effective approach.\\n2. The empirical results show that the Hierarchical Diffuser outperforms other methods on standard offline RL benchmarks, with improved training and planning speeds, and enhanced generalization capabilities.\\n3. The paper clearly explains the motivation and approach, and the results are presented in a well-organized manner.\",\n  \"Weaknesses\": \"1. The novelty of the Hierarchical Diffuser is somewhat limited, as it builds upon existing work in diffusion-based generative methods and hierarchical planning.\\n2. The paper could benefit from more detailed explanations of the training and planning processes, particularly for the low-level diffuser.\\n3. It would be helpful to see more extensive evaluation on a wider range of tasks and datasets to demonstrate the robustness of the Hierarchical Diffuser.\",\n  \"Questions\": \"1. How does the Hier"}
{"paper_id": "i7LCsDMcZ4", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors have successfully addressed a significant gap in the field by proposing methods for generating saliency maps and class activation maps for SNNs, which will facilitate more effective data augmentation and improve the generalization ability of SNNs.\\n2. The proposed SLTRP and SLRP methods are well-integrated into the EventRPG framework, and the data augmentation techniques, RPGDrop and RPGMix, are innovative and well-motivated.\\n3. The paper demonstrates significant improvements in the performance of SNNs on event-based object and action recognition tasks.\\n4. The authors have made significant contributions to the field of event cameras and SNNs.\",\n  \"Weaknesses\": \"1. The paper is quite dense and assumes a significant amount of prior knowledge in the field of event cameras and SNNs, making it challenging for non-experts to follow.\\n2. The evaluation is primarily focused on classification tasks, and it would be beneficial to explore the applicability of these methods to other neural network tasks beyond classification"}
{"paper_id": "jHdz0CIS2y", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a novel approach to voice conversion using entangled speech representations from self-supervised learning and speaker verification, showing significant improvements over baseline models and prior work.\\n2. The method's ability to operate without text and its applicability across languages are notable advantages.\\n3. The use of self-synthesized examples during training enhances speaker similarity and overall results in voice conversion tasks.\",\n  \"Weaknesses\": \"1. The reliance on self-supervised learning and speaker verification models might limit the generalizability of the approach to other domains or tasks.\\n2. The potential for overfitting to the training data, especially with the use of self-synthesized examples, is a concern.\\n3. The paper could benefit from a more in-depth analysis of the entangled representations and their impact on voice conversion performance.\",\n  \"Questions\": \"1. How does the entanglement of speech representations from self-supervised learning and speaker verification contribute to the improvement in voice conversion performance?\\n2. Are there any plans to"}
{"paper_id": "NkYCuGM7E2", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach to enhancing autonomous driving systems by leveraging Large Language Models (LLMs) for decision-making, which can improve safety, efficiency, and adaptability.\\n2. The cognitive pathway framework and guided parameter matrix adaptation are well-designed and effective in translating LLM decisions into actionable commands for low-level controllers.\\n3. The experiments demonstrate the system's capability in single and multi-vehicle scenarios, including complex traffic situations.\",\n  \"Weaknesses\": \"1. The reliance on LLMs may still be limited by their existing biases and lack of domain-specific knowledge.\\n2. The system's performance may degrade in scenarios where the LLMs' commonsense reasoning is insufficient or inaccurate.\\n3. The scalability and generalizability of the system to various driving environments and edge cases are not thoroughly explored.\",\n  \"Questions\": \"1. How does the system handle uncertainty or ambiguity in the LLM's decision-making process?\\n2. Are there any potential risks or challenges associated with relying on LLMs in"}
{"paper_id": "RtDok9eS3s", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a clear hypothesis that transformer blocks can be simplified without loss of performance or training speed, backed by signal propagation theory and empirical studies.\\n2. The authors propose and test various modifications to standard transformer blocks, demonstrating that they can achieve 16% faster training throughput and 15% fewer parameters.\\n3. The study is well-organized and clearly presented, with a good balance between theory and experiments.\",\n  \"Weaknesses\": \"1. The simplification of transformer blocks may not be applicable to all types of transformer-based models or tasks.\\n2. The removal of skip connections and normalization layers may have unintended effects on model behavior in certain scenarios.\\n3. The study focuses on a specific dataset and model architecture; its results may not generalize to other settings.\",\n  \"Questions\": \"1. How do the simplified transformer blocks perform on more complex tasks or models, such as those with multiple heads or layers?\\n2. Are there any potential risks or limitations to removing skip connections and normalization layers in certain scenarios?\\"}
{"paper_id": "YCPDFfmkFr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors provide a novel approach to handling infeasible QP layers in neural networks, which is a significant contribution to the field.\\n2. The proposed Extended Conservative Jacobian (ECJ) framework allows for the differentiation of QP solutions in both forward and backward modes, enabling the training of QP layers without enforcing primal feasibility.\\n3. The method is demonstrated to be numerically robust and efficient, providing improved predictive power in structured tasks.\\n4. The practicality of the approach is showcased through the implementation of QPLayer, a C++ package for differentiating feasible and infeasible convex QPs.\",\n  \"Weaknesses\": \"1. The authors assume that the QP problem is convex, which may not always be the case in real-world applications.\\n2. The approach relies on the classical least squares sense to define QP solutions as the closest feasible ones, which might not be the most suitable choice for all tasks.\\n3. The paper focuses primarily on the theoretical aspects of the method and its"}
{"paper_id": "ueTdErd5Ib", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant and timely problem in contextual stochastic optimization.\\n2. The proposed discretization framework demonstrates competitiveness in average regret and improved robustness over existing methods.\\n3. Theoretical guarantees and experimental validations provide strong evidence for the method's effectiveness.\",\n  \"Weaknesses\": \"1. The paper assumes that the feasible region can be discretized into subsets, which might not always be feasible in practice.\\n2. The proposed method may not be scalable for large-scale problems with complex distributions.\\n3. Further investigation is needed to explore the robustness of the method in real-world scenarios with multiple uncertain parameters.\",\n  \"Questions\": \"1. How does the discretization framework handle high-dimensional uncertain parameters?\\n2. Can the proposed method be extended to handle multiple uncertain parameters with different distributions?\\n3. What are the computational costs of the discretization step, and how does it impact the overall performance?\"\n}"}
{"paper_id": "sDmjlpphdB", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel Mixture-of-Prompts (MoP) framework that significantly enhances the performance of automatic prompting for complex tasks.\\n2. The two-phase process, using demo clustering and region-based joint search, is innovative and well-executed.\\n3. The MoP framework outperformed previous prompt optimization methods on benchmark NLP tasks, with improvements up to 43%.\",\n  \"Weaknesses\": \"1. The paper relies heavily on kernel regression for demo clustering, which might not generalize well to all problem domains.\\n2. The complexity of the MoP framework might make it challenging to apply to tasks with a large number of demos or complex problem spaces.\\n3. The authors do not discuss the potential scalability issues of MoP with increasing task complexity.\",\n  \"Questions\": \"1. How does the MoP framework handle cases where the demos are not semantically similar, potentially leading to fragmented expert regions?\\n2. Can the MoP framework be applied to tasks beyond NLP, such as vision"}
{"paper_id": "Mdk7YP52V3", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a comprehensive theoretical framework to explain the failure modes of overparameterized heteroskedastic regression models, filling a gap in the understanding of these phenomena.\\n2. The use of statistical physics to model the behaviors of overparameterized models is an innovative approach, providing new insights into the pathologies observed in these models.\\n3. The empirical validation against real-world data and numerical solutions to the derived equations is rigorous and supports the proposed framework.\",\n  \"Weaknesses\": \"1. The application of field theory to statistical physics and neural networks might be challenging for non-experts to follow, and the paper could benefit from a more detailed explanation of the mathematical derivations.\\n2. While the proposed framework provides a new understanding of overparameterized heteroskedastic models, it might be limited to a specific class of models or scenarios, and its generalizability to other models or domains is unclear.\",\n  \"Questions\": \"1. How does the proposed framework account for the interactions between regularization strength and model capacity in the"}
{"paper_id": "YGTSLDAPqb", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel framework that addresses the limitations of conventional fine-tuning techniques in domain adaptation.\\n2. The evaluation is thorough and demonstrates state-of-the-art results across multiple real-world datasets.\\n3. The study highlights the importance of targeted augmentations in improving out-of-distribution performance.\\n4. The methodology is well-explained and easy to follow.\",\n  \"Weaknesses\": \"1. The paper assumes that the distribution shifts between source and target domains are known, which may not be the case in many real-world scenarios.\\n2. The proposed framework may not be effective if the targeted augmentations are not well-designed or if the distribution shifts are complex.\\n3. The study only considers four real-world datasets, and it would be beneficial to evaluate the framework on a broader range of tasks and datasets.\",\n  \"Questions\": \"1. How does the proposed framework handle scenarios where the distribution shifts between source and target domains are not well-characterized?\\n2. Can the targeted augmentations be learned automatically,"}
{"paper_id": "JTcaziw7G1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors have proposed a novel method for private information retrieval in LLMs using MPC techniques, addressing a significant gap in existing methods.\\n2. The method, PRAG, demonstrates efficiency in terms of communication complexity and scalability, making it a promising solution for secure LLM applications.\\n3. The paper is well-organized, clearly presented, and provides a comprehensive evaluation of the method's efficacy.\",\n  \"Weaknesses\": \"1. The computational costs of the method suggest a need for further optimization to make it more practical for real-world applications.\\n2. The authors could have discussed the limitations of MPC techniques in more detail, such as their scalability and potential bottlenecks.\",\n  \"Questions\": \"1. How do the authors plan to address the scalability issues of MPC techniques in the future?\\n2. Can the authors provide more information about the computational costs of the method and how they were measured?\"\n}"}
{"paper_id": "lEkFq4RUCX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed Directional Distance Field (DDF) metric effectively measures the geometric differences between 3D point clouds, improving upon traditional methods like EMD and CD. This is a significant contribution to the field of computer vision.\\n2. The paper provides a clear and concise explanation of the proposed method, making it easy to understand for the reader. The experiments conducted demonstrate the efficiency and accuracy of the DDF metric across various tasks.\\n3. The paper's focus on surface differences rather than point-wise correspondences is a novel approach that has the potential to enhance 3D point cloud processing frameworks.\",\n  \"Weaknesses\": \"1. While the proposed DDF metric is an improvement over existing methods, it is unclear how it compares to other recent advancements in the field, such as the work on graph-based methods for point cloud processing.\\n2. The paper assumes that the reference points used in the DDF metric are well-distributed across the surface of the point clouds, which may not always be the case in practice.\\n"}
{"paper_id": "Fq8tKtjACC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes an effective method for leveraging high-quality data to improve model performance, potentially breaking the scale-performance relationship.\\n2. The approach of filtering data and generating synthetic datasets designed to enhance diversity and instructiveness is innovative.\\n3. The evaluation is comprehensive, comparing the model's performance with several larger models and showcasing its robustness.\",\n  \"Weaknesses\": \"1. The model's specialization in Python and sensitivity to prompt variations may limit its applicability to other domains or languages.\\n2. The reliance on GPT-4 annotations for data filtering may introduce bias in the model's training data.\\n3. The paper does not explore the potential for adapting this approach to other types of models or tasks.\",\n  \"Questions\": \"1. Can the method be adapted to other types of models or tasks beyond code generation?\\n2. How does the model's performance change when applied to different programming languages or domains?\\n3. What are the potential implications of relying on GPT-4 annotations for data filtering, and how"}
{"paper_id": "8fQlGQkj0S", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a novel theoretical framework for understanding in-context learning (ICL) and its connection to Bayesian inference and gradient descent.\\n2. The authors model both pre-training data and in-context samples using generative models, deriving risk upper bounds for task retrieval and learning.\\n3. The paper's separation between task learning and retrieval modes is clear and insightful.\\n4. The experiments on Transformers validate the findings and provide a clear understanding of the optimal number of in-context samples.\",\n  \"Weaknesses\": \"1. The paper assumes a Gaussian mixture model for pre-training data and in-context samples, which might not generalize to all scenarios.\\n2. The analysis is focused on Transformers, and it's unclear how well the findings generalize to other models.\\n3. The paper does not discuss potential limitations or challenges in applying this framework to real-world applications.\",\n  \"Questions\": \"1. How does the proposed framework handle scenarios where the pre-training data is not drawn from noisy task distributions?\\n2. Can the risk upper bounds"}
{"paper_id": "hRos9WldRK", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed method, Learning to Bootstrap (L2B), is well-designed to handle label noise and does not require explicit assumptions about the noise model or a clean validation set.\\n2. The method demonstrates improved robustness against label noise across several datasets, outperforming baseline approaches.\\n3. The method can integrate well with existing noisy label learning techniques, suggesting its potential as a practical complement.\",\n  \"Weaknesses\": \"1. The method's effectiveness in handling high levels of noise may be limited if the validation set feedback is biased or noisy.\\n2. The paper does not provide an in-depth analysis of the time and computational complexity of the method.\",\n  \"Questions\": \"1. How does the method handle cases where the validation set feedback is biased or noisy?\\n2. Can the authors provide more detailed analysis of the time and computational complexity of the method, including the overhead introduced by the meta-learning framework?\"\n}"}
{"paper_id": "ttRSBZiCiu", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed WaveFluid model shows promise in efficiently generating high-fidelity audio with reduced computational overhead, offering an effective alternative to existing methods in speech synthesis.\\n2. The authors' use of a two-stage model and reparameterization techniques to improve memory efficiency and training speed is a notable contribution.\\n3. The model's competitive performance in generating high-fidelity audio compared to previous diffusion and GAN-based vocoders is impressive.\",\n  \"Weaknesses\": \"1. The paper lacks a detailed analysis of the trade-offs between memory efficiency and model performance.\\n2. The evaluation is limited to a comparison with existing methods, and it is unclear whether the model's performance would degrade with increased complexity or larger datasets.\\n3. The paper could benefit from a more thorough discussion of the implications of the learned velocity field and its potential applications.\",\n  \"Questions\": \"1. How does the WaveFluid model handle out-of-distribution inputs or unseen mel-spectrograms during inference?\\n2. Can the model be extended to other audio"}
{"paper_id": "WYsLU5TEEo", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel framework combining interpretability and robustness, addressing a significant gap in current research.\\n2. The use of GANs for generating counterfactuals is innovative and effective, offering a new perspective on enhancing both interpretability and robustness.\\n3. The framework achieves competitive results in both IoU scores for saliency maps and resilience to PGD attacks, demonstrating its practical applications.\",\n  \"Weaknesses\": \"1. The paper relies heavily on the success of GANs, which can be challenging to train and may require careful tuning.\\n2. The framework's performance may be limited by the quality of the generated counterfactuals, which can be difficult to control and evaluate.\\n3. The discussion of the discriminator's output as a measure of prediction uncertainty is an interesting direction, but it is not fully explored in the paper.\",\n  \"Questions\": \"1. How do the authors plan to address the challenges of training GANs and ensuring the quality of the generated counterfactual"}
{"paper_id": "KKZaj2QS3G", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses a significant gap in existing time series representation learning by considering the impact of noise on task accuracy. \\n2. The proposed CoInception framework integrates a noise-resilient sampling strategy and a scalable and robust encoder design, demonstrating its effectiveness in handling noise-afflicted time series tasks. \\n3. The hierarchical triplet loss used in the CoInception framework helps to enforce noise resilience and alignment and uniformity in the embeddings for tasks like forecasting, classification, and anomaly detection.\",\n  \"Weaknesses\": \"1. The paper's contribution is somewhat incremental, as it builds upon existing research in time series representation learning and self-supervised learning. \\n2. The experiments conducted are limited to three benchmark datasets, which may not be representative of the broader applicability of the CoInception framework. \\n3. The paper does not provide a thorough comparison with other state-of-the-art methods that specifically address noise in time series data.\",\n  \"Questions\": \"1. How does the CoInception framework perform on time"}
{"paper_id": "LWDRiFzbHQ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The Vicinal Risk Proxy (VRP) approach effectively addresses the issue of spurious model responses by incorporating neighboring samples' responses into the risk proxy calculation, leading to a more robust measure of model accuracy.\\n2. The method can be applied to various generalization indicators like average confidence or effective invariance, showing consistent improvements over existing baselines across multiple datasets and test scenarios.\\n3. The paper clearly explains the motivation, methodology, and results, making it easy to follow and understand.\",\n  \"Weaknesses\": \"1. While the VRP approach is innovative and effective, it may not be suitable for all types of models or datasets, especially those with highly diverse or complex distributions.\\n2. The reliance on neighboring samples' responses may not work well in scenarios where the test data is highly imbalanced or noisy.\\n3. The paper could benefit from more discussion on the potential limitations and future directions of the VRP approach.\",\n  \"Questions\": \"1. How does the VRP approach handle cases where the neighboring"}
{"paper_id": "Yp01vcQSNl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant gap in graph transformers by explicitly incorporating edge directionality, which is crucial for directed graphs.\\n2. The proposed method, DiGT, introduces a novel attention mechanism and dual encodings, which enhance performance on learning tasks for directed graphs.\\n3. The paper explores alternative ways to incorporate directionality into existing graph transformer architectures, demonstrating the flexibility and adaptability of the approach.\\n4. The experiments on new directed graph datasets designed specifically where edge directionality is important validate the effectiveness and promise of DiGT.\",\n  \"Weaknesses\": \"1. The paper assumes that the graph is directed, which might not be the case for all real-world applications.\\n2. The reliance on dual encodings and dynamic attention mechanisms may increase computational complexity and require more memory.\\n3. The experiments are limited to directed graph datasets, and it is unclear how the approach would generalize to undirected graphs or other types of graphs.\",\n  \"Questions\": \"1. How does the DiGT approach handle cases where the direction"}
{"paper_id": "LndMyiBl3n", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The authors address a significant gap in existing studies on graph adversarial attacks by proposing a novel approach that works well under the restricted black-box attack setting.\\n2. The use of the Modified Silhouette Score (MSS) to guide the attack is an interesting and novel aspect of this work.\\n3. The Greedy Randomized Block Coordinate Descent algorithm used to optimize edge perturbations is scalable and efficient.\\n4. The paper demonstrates strong performance across homophilic and heterophilic graphs, comparable to white-box attacks.\",\n  \"Weaknesses\": \"1. The authors assume that the node features are known to the attacker, which might not be realistic in many scenarios.\\n2. The evaluation of the approach is limited to a few datasets and metrics, and it would be beneficial to see more comprehensive experiments.\\n3. The paper could benefit from a more detailed discussion on the limitations and potential weaknesses of the proposed approach.\",\n  \"Questions\": \"1. How does the Modified Silhouette Score (MSS) compare"}
