{"paper_id": "B0OwtVEejJ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The work is well-motivated and aims to bridge the gap between two sub-communities in NAS. The authors have made a significant contribution by adapting gradient-based methods for weight-entangled spaces. The code is openly accessible, which is a plus. The comparative assessment and analysis of performance are thorough and reveal the benefits of the integration.\",\n  \"Weaknesses\": \"The paper assumes that the reader is familiar with NAS and weight entanglement, which might make it difficult for some readers to follow. The proposed scheme might not be applicable to all weight-entangled spaces. The evaluation of the proposed method is limited to a single experiment.\",\n  \"Questions\": \"How generalizable is the proposed scheme to different weight-entangled spaces? What are the computational costs of adapting gradient-based methods for weight-entangled spaces? How does the proposed scheme handle the compatibility challenges of weight-entanglement?\"\n}"}
{"paper_id": "ZlEtXIxl3q", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper makes a strong case for using contrastive loss functions to extract the sparse latent function implied by global epistasis models. The fitness-epistasis uncertainty principle is a novel and well-motivated concept. The experimental results demonstrate the practical utility of contrastive losses in regimes where MSE is ineffective. The paper is well-written and well-organized, with clear and concise language.\",\n  \"Weaknesses\": \"One potential concern is that the paper assumes a monotonic nonlinearity, which may not always hold in real-world scenarios. Additionally, the paper relies on a specific type of contrastive loss function (Bradley-Terry loss), which may not be the most effective choice for all types of fitness functions. The paper could benefit from a more thorough discussion of the limitations of the proposed approach and potential avenues for future research.\",\n  \"Questions\": \"How do the authors plan to extend their approach to non-monotonic nonlinearity? What are the computational requirements for training the model with contrastive loss functions, especially for large datasets"}
{"paper_id": "6yJuDK1DsK", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors present a novel and efficient approach to lifelong test-time adaptation, FEATHER, which introduces a small number of additional parameters to a pre-trained source model. The approach is orthogonal to existing lifelong TTA methods and can be augmented with them, resulting in a significant reduction in the number of additional parameters needed. The experimental results on various benchmark datasets demonstrate that FEATHER achieves better/similar performance compared to existing SOTA lifelong TTA methods, with substantially fewer trainable parameters. The paper is well-written and clearly presents the contributions and results.\",\n  \"Weaknesses\": \"The paper assumes that the pre-trained source model is available, which might not always be the case in real-world scenarios. The authors should discuss this limitation and potential solutions. Additionally, the paper does not provide a detailed analysis of the computational cost and memory consumption of FEATHER, which might be a concern for edge devices.\",\n  \"Questions\": \"How does FEATHER handle the case where the pre-trained source model is not available? What is the computational cost and memory consumption of FEATHER"}
{"paper_id": "lt6xKGGWov", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel approach to supervised feature selection based on neural estimation of mutual information between features and targets. The ensemble-based method allows for capturing sophisticated relationships between features and targets, leading to exact selection. The examples provided demonstrate the effectiveness of the approach. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The paper assumes that the mutual information between features and targets can be estimated using neural networks, which might not always be the case. The method may not work well for high-dimensional feature spaces or when the relationships between features and targets are complex. The evaluation of the method is limited to a few examples and datasets.\",\n  \"Questions\": \"How does the proposed method handle feature interactions and correlations? Can the method be extended to handle high-dimensional feature spaces? How does the method compare to existing feature selection methods in terms of computational efficiency and scalability?\"\n}"}
{"paper_id": "Bvrc6kobWd", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a new perspective on the role of margins in contrastive learning, which is an important contribution to the field. The experimental results demonstrate the effectiveness of emphasizing positive samples and scaling gradients depending on positive sample angles and logits. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The paper assumes that the reader is familiar with contrastive learning and its variations, which may not be the case for all readers. Additionally, the paper could benefit from a more in-depth discussion of the implications of the new perspective on the design of contrastive learning algorithms.\",\n  \"Questions\": \"How does the new perspective on margins affect the choice of hyperparameters in contrastive learning algorithms? Can the proposed approach be extended to other self-supervised learning methods?\"\n}"}
{"paper_id": "FJjHQS2DyE", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel conditional adversarial support alignment (CASA) method for unsupervised domain adaptation, which aims to minimize the conditional symmetric support divergence between the source's and target domain's feature representation distributions. The method is well-motivated and theoretically justified. The empirical results demonstrate that CASA outperforms other state-of-the-art methods on different UDA benchmark tasks under different label shift conditions.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion of the limitations of the proposed method, such as the assumption of conditional independence between the source and target domains. The empirical results are mostly based on a single dataset, and it would be beneficial to include more experimental results on other datasets to demonstrate the generalizability of the proposed method.\",\n  \"Questions\": \"How does CASA handle the case when the source and target domains have different feature distributions, but the same label distribution? Is the proposed method sensitive to the choice of hyperparameters, and if so, how can they be tuned to achieve optimal performance?\"\n}"}
{"paper_id": "xelrLobW0n", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel benchmark for evaluating continual learning in aligned LLMs, which is a significant contribution to the field.\\n2. The authors demonstrate the challenge of continual learning in LLMs using their proposed benchmark, TRACE, and show that training on reasoning tasks can mitigate the loss of general abilities.\\n3. The proposed RCL approach is an effective solution to the problem of catastrophic forgetting in LLMs.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with the concept of aligned LLMs and continual learning, which may not be the case for all readers.\\n2. The paper could benefit from a more detailed explanation of the TRACE benchmark and its design choices.\\n3. The authors mention that the accuracy of llama2-chat 13B on gsm8k dataset declined precipitously from 28.8% to 2% after training on their datasets, but they do not provide a clear explanation for this significant decline.\",\n  \"Questions\": \"1. How does"}
{"paper_id": "bYQkOPvgDw", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed PRGNN framework is well-structured and effectively addresses the issue of label noise in GNNs. The use of a probabilistic graphical model based framework is a good approach to handle heterophilous graphs. The experimental results show the robustness of PRGNN on varying noise types and rates, and on graphs with different heterophilies. The authors also provide a clear description of the generative process and the prediction of labels for unlabeled nodes.\",\n  \"Weaknesses\": \"The paper assumes that the clean label set is available, which may not be the case in many real-world scenarios. The authors should provide more insights on how to deal with the scenario where the clean label set is not available. Additionally, the paper could benefit from more detailed analysis of the computational complexity of the proposed method, especially for large graphs. The authors also need to provide more information about the hyperparameters used in the experiments.\",\n  \"Questions\": \"How does the proposed method handle the case where the clean label set is not available? What is the"}
{"paper_id": "sKPzAXoylB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses the problem of catastrophic forgetting and loss of plasticity in deep representation learning methods through a novel approach called Utility-based Perturbed Gradient Descent (UPGD).\\n2. UPGD combines gradient updates with perturbations to protect useful units from forgetting and rejuvenate less useful units, which is a more comprehensive solution compared to existing methods that address these issues separately.\\n3. The paper uses a challenging streaming learning setup with hundreds of non-stationarities and unknown task boundaries, and shows that UPGD outperforms or is competitive with all existing methods in all problems.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed explanation of the intuition behind UPGD and how it combines gradient updates with perturbations.\\n2. The experimental results are based on a single task and a single learning algorithm (PPO), and it would be interesting to see how UPGD performs on other tasks and algorithms.\\n3. The paper does not provide a clear comparison with other methods"}
{"paper_id": "H8Qg1IIMaR", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper highlights an interesting and important issue in large language and vision-language models. The vulnerability to permutation sensitivity in multiple-choice question answering (MCQA) is a crucial finding that needs to be addressed. The empirical evidence provided in the paper is convincing and well-presented. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper's contribution is somewhat limited to identifying a specific vulnerability in large language and vision-language models. The proposed solution or mitigation strategy is not presented in this paper, which may leave the reader wondering about the practical implications of this finding. The paper could benefit from a more in-depth analysis of the causes and consequences of permutation sensitivity.\",\n  \"Questions\": \"What are the practical implications of permutation sensitivity in MCQA? How can this vulnerability be mitigated or addressed in large language and vision-language models? Are there any potential applications or domains where permutation sensitivity is more critical than others?\"\n}"}
{"paper_id": "tth2qXY7RU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed Super Floating-Point (SuFP) data type and quantization method is a significant innovation, addressing the limitations of conventional quantization techniques. The multi-region piecewise quantization using a tensor-wise scalable bias is a clever approach, allowing for optimized precision for each region. The tailored hardware for SuFP employing only integer arithmetic units and shifters is a notable design. The experimental results demonstrating SuFP's accuracy performance on par with or even exceeding full precision floating-point (FP32) across various benchmarks is impressive.\",\n  \"Weaknesses\": \"The paper relies heavily on comparisons with other quantization methods, but it would be beneficial to include a more comprehensive evaluation of SuFP's performance in different scenarios and datasets. The scalability of SuFP in handling large-scale models and datasets should be further explored. Additionally, the paper could benefit from a more detailed discussion on the potential trade-offs between precision and computational efficiency.\",\n  \"Questions\": \"How does SuFP handle the case where the data distribution has a large dynamic range, and the tensor-wise scalable"}
{"paper_id": "fjf3YenThE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a new insight into variance reduction and its application to zeroth-order hard-thresholding, which is a novel contribution to the field.\\n2. The proposed algorithm and theoretical results demonstrate improved convergence rates and broader applicability compared to existing methods.\\n3. The paper provides a clear and concise presentation of the algorithm and its analysis.\",\n  \"Weaknesses\": \"1. The paper assumes that the true gradient of the objective function can be approximated by zeroth-order methods, which may not always be the case in practice.\\n2. The algorithm's performance is evaluated only on a portfolio optimization problem and black-box adversarial attacks, which may not be representative of all possible applications.\\n3. The paper could benefit from more experimental results to demonstrate the practical effectiveness of the proposed algorithm.\",\n  \"Questions\": \"1. How does the proposed algorithm handle cases where the true gradient of the objective function cannot be approximated by zeroth-order methods?\\n2. What are the computational costs of the proposed algorithm compared to"}
{"paper_id": "H9DYMIpz9c", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a new approach to data distillation for auto-regressive machine learning tasks, which is a significant contribution to the field. \\n2. The method is well-motivated and the authors provide a clear explanation of the underlying ideas. \\n3. The empirical results are impressive, showing that the proposed method can achieve 98-120% of the performance of training on the full dataset with significantly less data.\",\n  \"Weaknesses\": \"1. The paper assumes that the input and output have a strict left-to-right causal structure, which may not be the case for all auto-regressive tasks. \\n2. The method relies on the Adam optimizer, which may not be the best choice for all tasks. \\n3. The paper does not provide a detailed analysis of the computational cost of the proposed method.\",\n  \"Questions\": \"1. How does the method perform on tasks with non-causal structures? \\n2. Can the method be extended to other optimizers? \\n3. How does"}
{"paper_id": "4kLVvIh8cp", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed algorithm, PNLSVI, is a well-designed and theoretically sound approach for offline RL with non-linear function approximation.\\n2. The use of variance-based weighted regression and pessimistic value iteration is innovative and effective.\\n3. The regret bound analysis is thorough and provides a tight dependency on the function class complexity.\",\n  \"Weaknesses\": \"1. The paper assumes that the behavior policy is fixed and known, which may not be the case in many real-world scenarios.\\n2. The algorithm's performance may degrade if the function class is complex and the number of samples is limited.\\n3. The paper does not provide an empirical evaluation of the algorithm's performance on real-world datasets.\",\n  \"Questions\": \"1. How does the algorithm handle the case where the behavior policy is not fixed or known?\\n2. What is the computational cost of the variance estimation subroutine?\\n3. Can the algorithm be extended to handle non-stationary environments?\"\n}"}
{"paper_id": "CSpWgKo0ID", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach to study LLM's cooperation and coordination behavior using behavioral game theory.\\n2. The results show that LLMs generally perform well in tasks and uncover persistent behavioral signatures.\\n3. The paper provides a clear and concise presentation of the research, making it easy to follow.\\n4. The experiments are well-designed and provide a good understanding of LLM's behavior in different game scenarios.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more in-depth discussion of the implications of the results and their potential applications.\\n2. The paper assumes that the other player's actions are known, which might not always be the case in real-world scenarios.\\n3. The paper could explore more game scenarios to further understand LLM's behavior in different contexts.\",\n  \"Questions\": \"1. How do the results generalize to more complex game scenarios or multiple players?\\n2. Can the proposed approach be applied to other types of agents, such as reinforcement learning agents?\\n3. How"}
{"paper_id": "gwbQ2YwLhD", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a comprehensive theoretical analysis of the impact of scale on structure learning algorithms, extending previous results to d-dimensional cases. The conditions under which square-based losses are minimal for wrong DAGs are well-defined and easy to understand. The experimental results on synthetic and real-world data confirm the theoretical findings, making the paper a significant contribution to the field.\",\n  \"Weaknesses\": \"The paper assumes a fixed scale for the variables, which might not be realistic in many real-world applications. The authors should discuss the implications of this assumption and possible ways to relax it. Additionally, the experimental results could be more comprehensive, including more datasets and a more detailed analysis of the effect of scale on structure learning.\",\n  \"Questions\": \"How do the authors plan to address the assumption of a fixed scale in future work? Can the theoretical results be extended to cases where the scale is not fixed? Are there any other implications of the scale assumption that the authors would like to discuss?\"\n}"}
{"paper_id": "eIYDKNqXuV", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The Village-Net Clustering algorithm effectively addresses the issue of determining the optimal number of clusters in a dataset, which is a challenging problem in clustering.\\nThe algorithm's ability to autonomously determine the optimal number of clusters is a significant contribution.\\nThe use of the Walk-likelihood Community Finder (WLCF) is a good choice for community detection.\\nThe algorithm's time complexity of O(N*k*d) is impressive, making it suitable for large-scale datasets.\\nThe paper provides extensive benchmarking on real datasets with known ground-truth labels, which is a good practice.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the Walk-likelihood Community Finder (WLCF) algorithm, especially its limitations and potential biases.\\nThe authors could provide more information about the pre-processing steps taken for the datasets used in the benchmarking.\\nThe paper could include more experiments to compare the performance of Village-Net Clustering with other state-of-the-art methods in different scenarios.\\nThe authors could provide more insights into the choice of the number"}
{"paper_id": "bAMPOUF227", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a simple yet effective framework for enhancing the reliability of Large Language Models (LLMs) through task-specific fine-tuning. This approach has the potential to improve the generalizability and factuality of LLMs in natural language understanding and question answering.\\n2. The empirical analysis is comprehensive and sheds light on the advantages of incorporating discriminative models into LLMs.\\n3. The authors provide a suite of resources, including datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks.\",\n  \"Weaknesses\": \"1. The paper's primary contribution is the establishment of a framework for enhancing LLMs, but the novelty of this contribution is not entirely clear.\\nThe authors mention that previous work has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, but they do not provide a clear explanation of how their approach differs from existing work.\\n\\n2. The paper could benefit from a more in-depth discussion of the limitations of their approach"}
{"paper_id": "eNoiRal5xi", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors introduce a novel objective for domain generalization, Unknown Domain Inconsistency Minimization (UDIM), which reduces the loss landscape inconsistency between source and unknown domains. Theoretically, they validate that merging SAM optimization with UDIM establishes an upper bound for the true objective of the DG task. Empirically, UDIM consistently outperforms SAM variants across multiple DG benchmark datasets. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The authors could provide more insights into the choice of perturbation methods for crafting unknown domains. The experimental results show that UDIM performs well in scenarios with more restrictive domain information, but it would be beneficial to explore its performance in scenarios with less restrictive domain information. The paper could also benefit from a more detailed analysis of the upper bound established by merging SAM optimization with UDIM.\",\n  \"Questions\": \"How do the authors select the perturbation methods for crafting unknown domains? What are the implications of using different perturbation methods on the performance of UDIM? Can the authors provide more"}
{"paper_id": "uU0Adp7Sfo", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The introduction of the Collaborative-Competitive Generative Adversarial Network (CCGAN) scheme is a good contribution to the field of GANs, as it enables data generation with additional knowledge beyond the provided dataset distribution.\\n2. The modification of the regularization term into a divergence is a clever move to preserve the equilibrium point and avoid training collapse.\\n3. The closed-form equilibrium point serves as a guidance for training and hyper-parameter tuning, resulting in consistently high-quality generated samples.\\n4. The CCGAN is well-structured and easy to follow, making it a good addition to the existing literature on GANs.\",\n  \"Weaknesses\": \"1. The paper relies heavily on game theory to justify the modifications to the GAN framework, but the connection to game theory is not well-explained.\\n2. The choice of four publicly available datasets for experimentation may not be representative of the broader applications of the CCGAN.\\n3. The comparison with baseline models is limited to four datasets, and"}
{"paper_id": "dKju7tbe6D", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a novel fully neural iterative and parallel reasoning mechanism (IPRM) that combines the benefits of iterative computation with the ability to perform distinct operations simultaneously.\\n2. The experimental results on various visual question answering and reasoning benchmarks indicate that IPRM exhibits stronger reasoning capabilities and generalization than existing recurrent as well as transformer-based reasoning and vision-language interaction mechanisms.\\n3. IPRM achieves state-of-the-art zero-shot performance on the challenging CLEVR-Humans dataset and outperforms prior task-specific methods for the NLVR and CLEVR-CoGen benchmarks.\\n4. The paper provides a clear explanation of the IPRM architecture and its advantages over existing methods.\\n5. The visualization of IPRM\u2019s computation across reasoning steps aids interpretability and diagnosis of its reasoning and outputs.\",\n  \"Weaknesses\": \"1. The paper assumes that the input queries are well-defined and can be decomposed into a sequence of steps, which may not be the case in all scenarios.\\n2. The I"}
{"paper_id": "DL7JWbdGr3", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper tackles an important problem in epidemic prediction with a novel approach of pre-training epidemic time-series models. The use of self-supervised learning tasks to learn important priors about epidemic dynamics is an interesting idea. The paper also provides a thorough evaluation on various downstream tasks with good results.\\nThe authors have made a significant contribution by tackling the problem of pre-training epidemic time-series models to learn from multiple datasets from different diseases and epidemics.\\nThe paper is well-written and the authors have clearly explained their approach and results.\",\n  \"Weaknesses\": \"The paper assumes that pre-training on a large number of datasets from different diseases and epidemics will lead to better generalization performance, which might not be the case in practice. The paper should provide more analysis on the generalization performance of the proposed method.\\nThe authors should also provide more details on how they designed the SSL tasks to learn important priors about the epidemic dynamics.\\nThe evaluation results seem to be good, but the authors should provide more analysis on the robustness of the model to different types of"}
{"paper_id": "1PaDPHDhwe", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a simple and effective class-specific scaling strategy that can be applied to existing debiasing algorithms without additional training. The proposed instance-wise adaptive scaling technique further improves performance. The unified metric for summarizing the trade-off between robust accuracy and average accuracy is a meaningful contribution. The experiments on computer vision and natural language processing domains are comprehensive and well-organized.\",\n  \"Weaknesses\": \"The paper assumes that the class-specific scaling strategy is directly applicable to existing debiasing algorithms without additional training, but it is not clear if this is always the case. The instance-wise adaptive scaling technique may not be widely applicable due to its complexity. The unified metric may not capture the entire trade-off between robust accuracy and average accuracy.\",\n  \"Questions\": \"How does the class-specific scaling strategy perform on datasets with multiple classes? Can the instance-wise adaptive scaling technique be applied to other debiasing algorithms? How does the unified metric compare to other metrics for evaluating the trade-off between robust accuracy and average accuracy?\"\n}"}
{"paper_id": "iI7hZSczxE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper introduces a novel method, Disentanglement under Independence-Of-Support via Contrastive Learning (DIOSC), to learn disentangled representations for time series electricity consumption. The proposed method effectively addresses temporal dependencies using l-variational inference layers with self-attention. The introduction of TDS (Time Disentangling Score) is a valuable contribution to the field. The code is available for public access, which is a positive aspect.\",\n  \"Weaknesses\": \"The paper assumes that the attribute correlations are learned up to a smooth bijection using contrastive learning and a single autoencoder. However, it is not clear how this assumption is validated. The evaluation of the proposed method is limited to a single dataset, and the results may not generalize to other scenarios. The method's ability to handle attribute correlations is not thoroughly explored. The paper could benefit from more detailed analysis of the TDS metric and its relation to the disentanglement performance.\",\n  \"Questions\": \"How does the proposed method handle attribute correlations in cases where they"}
{"paper_id": "qrGjFJVl3m", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a well-structured and well-motivated approach to building a large-scale vision-language model. The architecture of the Qwen-VL series is carefully designed to integrate visual and linguistic information, and the authors provide a clear explanation of each component. \\n2. The paper showcases the effectiveness of the Qwen-VL models on a range of visual-centric benchmarks, including image captioning, question answering, and visual grounding. The results are impressive, and the models set new records for generalist models under similar model scales. \\n3. The authors also demonstrate the superiority of Qwen-VL-Chat on real-world dialog benchmarks, which is a notable achievement.\\n4. The paper will facilitate future research by making the models public.\",\n  \"Weaknesses\": \"1. The paper assumes that the visual receptor and input-output interface are designed carefully, but the details of these components are not provided. More information about the design and training process of these components would be helpful. \\n2. The authors mention that"}
{"paper_id": "qup9xD8mW4", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper introduces a novel concept of behaviour distillation, which is a long-overdue extension of dataset distillation to reinforcement learning. The proposed method, HaDES, is elegant and well-designed, and the results are impressive. The paper showcases the effectiveness of HaDES in various scenarios, including out-of-distribution generalization and multi-task learning. The visualization of the synthetic datasets provides a new perspective on task insights. Overall, the paper makes a significant contribution to the field of reinforcement learning and dataset distillation.\",\n  \"Weaknesses\": \"The paper assumes that the expert policy is available, which might not be the case in many real-world scenarios. The method requires a significant amount of computational resources to train the agent and the Hallucinator. The paper does not provide a clear comparison with other methods for behaviour distillation, which makes it difficult to evaluate the novelty of the proposed approach.\",\n  \"Questions\": \"How does the method handle cases where the expert policy is not available? Can the method be applied to other types of reinforcement learning problems,"}
{"paper_id": "Zw8YxUWL4R", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel and effective method for improving text-to-image synthesis by introducing an Extended Textual Conditioning space. The proposed method, XTI, is more expressive and precise, and converges faster than the original TI space. The paper also showcases the effectiveness of the method for personalizing text-to-image models and achieving previously unattainable results in object-style mixing. The paper is well-organized and clearly presented, with a clear introduction, methodology, and results section.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that the proposed method may not be applicable to all types of text-to-image models, and the authors should explore this limitation in future work. Additionally, the paper could benefit from more detailed analysis of the properties of the new space and the effectiveness of the method for different tasks.\",\n  \"Questions\": \"1. How does the Extended Textual Conditioning space compare to other conditioning spaces used in text-to-image models? Is it more effective for certain tasks or applications? \\n2. How does the proposed method"}
{"paper_id": "3NXhwkZGjz", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel approach to source-free unsupervised domain adaptation that addresses the issue of inaccurate model predictions. The three-step adaptation process is well-defined and easy to understand. The experimental results demonstrate the effectiveness of the proposed approach and its ability to achieve state-of-the-art performance.\",\n  \"Weaknesses\": \"The paper assumes that the model's predictions can be consolidated into multiple hypotheses, which may not always be the case. The evaluation of the proposed approach is limited to a single dataset, and it would be beneficial to include results on other datasets to demonstrate its generalizability.\",\n  \"Questions\": \"How does the proposed approach handle cases where the model's predictions are not accurate, and how does it ensure that the consolidated hypotheses are indeed the most likely correct ones? What are the computational requirements of the three-step adaptation process, and how does it scale with the size of the input data?\"\n}"}
{"paper_id": "Fn655mJ4bv", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 2,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors introduce a novel interpretation technique to explain the behavior of structured output models, which can be useful in understanding the complex relationships between input and output variables. The energy-based training process for the interpreter function is an interesting approach. The use of correlations among output variables to improve explanation quality is a good idea.\",\n  \"Weaknesses\": \"The paper could benefit from a more in-depth discussion of the limitations of the proposed method, such as its scalability and applicability to large datasets. The experiments could be more comprehensive, with more datasets and evaluation metrics used to demonstrate the effectiveness of the proposed method. The authors could also provide more insight into the energy-based training process and how it incorporates structural information from the model to be explained.\",\n  \"Questions\": \"How does the proposed method handle cases where the structured model has multiple outputs with complex relationships? Can the authors provide more information on the computational complexity of the energy-based training process? How does the method perform on datasets with high-dimensional input spaces?\"\n}"}
{"paper_id": "VyMW4YZfw7", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 2,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper challenges the conventional wisdom that GNN architectures are the best choice for semi-supervised node classification. The idea of revisiting traditional methods from nonparametric estimation in the spectral domain is timely and well-motivated. The ablation study on hyperparameters associated with GNN spectral filtering techniques is also a strength of the paper.\",\n  \"Weaknesses\": \"The paper's main contribution is somewhat diminished by the fact that the authors do not provide a clear comparison with state-of-the-art GNN methods. The statement that recent performance improvements in GNN approaches may be partially attributed to shifts in evaluation conventions is not well-supported by the results presented in the paper. The experiments are limited to a few benchmarks and do not provide a comprehensive evaluation of the proposed method.\",\n  \"Questions\": \"How do the authors plan to extend their work to more complex graph types and larger datasets? What is the computational cost of the proposed method compared to traditional GNN architectures? How do the results presented in the paper relate to other recent works that have also proposed simpler"}
{"paper_id": "09xFexjhqE", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper uncovers a significant issue with existing RFT methods and proposes an elegant solution to address it.\\n2. The proposed method, AutoLoRa, achieves state-of-the-art results across various downstream tasks and has practical utility.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The authors assume that the feature extractor (FE) is pre-trained, which may not be the case in practice.\\n2. The paper does not provide a thorough analysis of the trade-off between the LoRa branch and the FE.\\n3. The proposed method may not be applicable to tasks where the FE is not well-suited for adversarial robustness.\",\n  \"Questions\": \"1. Can the authors provide more details on how the LoRa branch is trained, especially when the FE is not pre-trained?\\n2. How does the proposed method handle the case where the LoRa branch and the FE have different optimization objectives?\\n3. Are there any potential risks or limitations associated with"}
{"paper_id": "nmBjBZoySX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors introduce a novel framework, AdaGLT, that addresses the limitations of existing GLT methods. The adaptive, dynamic, and automated nature of AdaGLT allows it to tailor layer-adaptive sparse structures for various datasets and GNNs, facilitating deeper GNNs. The integration of pruning and training processes, as well as automatic capture of graph lottery tickets across diverse sparsity levels, are significant contributions. Theoretical proofs guaranteeing mitigation of over-smoothing issues and improved sparse structures in deep GNN scenarios are also noteworthy. The extensive experiments demonstrating AdaGLT's superiority over state-of-the-art competitors across multiple graph datasets of various scales and types are impressive.\",\n  \"Weaknesses\": \"While the authors provide a thorough analysis of the limitations of existing GLT methods, the paper could benefit from a more detailed comparison with these methods. The discussion on the theoretical guarantees of AdaGLT is valuable, but the connection to the problem of over-smoothing could be more explicitly stated. Additionally, the scalability of AdaGLT to even larger-scale"}
{"paper_id": "di52zR8xgf", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper presents a well-structured and clear description of the Stable Diffusion XL (SDXL) model, which is a significant improvement over previous versions of Stable Diffusion. The inclusion of a second text encoder and multiple novel conditioning schemes is a notable contribution.\\n2. The evaluation is comprehensive, with a comparison to state-of-the-art image generators such as Midjourney, and the results demonstrate a significant improvement in visual fidelity.\\n3. The refinement model is a clever approach to post-processing the generated images and achieving even higher quality results.\",\n  \"Weaknesses\": \"1. The paper could benefit from more details about the training process, such as the dataset used, the hyperparameters, and the training time.\\n2. The comparison to Midjourney is interesting, but it would be more convincing if the authors could provide more information about the specific settings used for the comparison.\\n3. The paper does not provide any discussion about the limitations of the approach or potential future work.\",\n  \"Questions\": \"1. How"}
{"paper_id": "0b328CMwn1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors propose a novel concept, activation prompt (AP), which extends the scope of input-level VP by enabling universal perturbations to be applied to activation maps within the intermediate layers of the model. This work provides a thorough performance analysis of AP, comparing it with VP and PEFT baselines, and demonstrates that AP significantly surpasses the input-level VP in terms of both accuracy and efficiency. The theoretical analysis of the rationale behind the preference of layer depth for AP and VP is also well-explored.\",\n  \"Weaknesses\": \"The authors should discuss the limitations of their work, such as the choice of datasets and model architectures used in the experiments, and the potential generalizability of the results. Additionally, the authors should provide more details on the implementation of AP, such as the specific techniques used to compute the activation maps and the perturbation design.\",\n  \"Questions\": \"How does the choice of activation maps affect the performance of AP? What is the computational cost of computing the activation maps and applying the perturbations in AP compared"}
{"paper_id": "AMDKqZcZbi", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a clear and well-structured explanation of the proposed method, and the authors demonstrate a good understanding of the biology of the Morris Water Maze task.\\nThe sequential Morris Water Maze task is a well-designed and challenging benchmark for continual learning.\\nThe authors demonstrate a good understanding of the trade-offs between rapid adaptation and retention of past knowledge.\\nThe proposed model outperforms ANN baselines in both the continual and few-shot learning contexts.\\nThe authors provide a clear and concise presentation of the results, and the figures and tables are well-designed and easy to understand.\",\n  \"Weaknesses\": \"The paper assumes that the reader is familiar with the biology of the Morris Water Maze task, which may not be the case for all readers.\\nThe authors do not provide a clear explanation of how the proposed model is related to previous work on continual learning.\\nThe paper would benefit from a more detailed discussion of the limitations of the proposed method and potential future directions.\",\n  \"Questions\": \"1. How do the authors plan to extend the proposed method to other tasks or"}
{"paper_id": "am7BPV3Cwo", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors identify a crucial issue with existing OOD detection methods, specifically the class-aware bias item, and provide a theoretical analysis to explain this phenomenon. The proposed ImOOD framework and the post-hoc normalization and training-time regularization techniques are well-motivated and effectively address the imbalanced data distribution problem. The experimental results show significant improvement over state-of-the-art methods on three benchmarks.\",\n  \"Weaknesses\": \"The authors assume that the OOD samples are drawn from the same distribution as the ID data, which may not always be the case in real-world scenarios. The proposed methods may not generalize well to scenarios with more complex or diverse OOD data distributions. The ablation study could be more comprehensive, and the authors should provide more details on the hyperparameter tuning process.\",\n  \"Questions\": \"How do the authors plan to address the potential overfitting issue when applying the proposed methods to more complex OOD data distributions? Are there any plans to evaluate the proposed methods on other datasets with varying levels of imbalanced data distributions?\"\n}"}
{"paper_id": "R1crLHQ4kf", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a clear and well-defined method for detecting adversarial attacks on ASR systems. The approach is simple and efficient, and the results are impressive, with a high AUROC for distinguishing adversarial examples. The paper also proposes adaptive attacks that are constructed with awareness of the defense mechanism, which is a good way to assess the robustness of the method. The writing is clear and concise, and the paper is well-organized.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the characteristics of the output probability distribution that are used for detection. It is not entirely clear why these specific characteristics were chosen, and a more thorough analysis of their effectiveness would be useful. Additionally, the paper could benefit from a more in-depth discussion of the limitations of the method and potential avenues for future research.\",\n  \"Questions\": \"How do the characteristics of the output probability distribution relate to the specific ASR system being used? Are there any cases where the method fails to detect adversarial attacks? What are the implications of the"}
{"paper_id": "i7P2mK3x3o", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed method for learning the dynamic optimal transport between two distributions is novel and well-motivated.\\n2. The model's ability to perform downstream tasks such as infinitesimal density ratio estimation and distribution interpolation is impressive.\\n3. The empirical demonstrations on high-dimensional data are convincing.\",\n  \"Weaknesses\": \"1. The paper assumes that the two distributions P and Q are only accessible via finite samples, which might not be a realistic assumption in some scenarios.\\n2. The model's performance on the tasks mentioned in the abstract is not thoroughly evaluated, and more experiments are needed to confirm its effectiveness.\\n3. The paper does not provide a clear comparison with existing methods for optimal transport.\",\n  \"Questions\": \"1. How does the model handle the case where the two distributions P and Q have different dimensions?\\n2. Can the model be applied to more complex tasks such as multi-modal data or data with non-Gaussian distributions?\\n3. The paper mentions that the model can be used for distribution interpolation in the latent"}
{"paper_id": "YIls9HEa52", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed irSLDS model class is a significant improvement over the existing rSLDS model, addressing its limitations and providing a more flexible and generalizable framework for analyzing neural activity and behavior. The use of semi-Markov discrete state process and PDE theory to derive efficient dynamical sufficient statistics is a clever and novel approach. The application of the model to mice electrophysiological data during decision-making is well-motivated and provides strong evidence for the model's capabilities.\",\n  \"Weaknesses\": \"One potential concern is that the model assumes a semi-Markov discrete state process, which may not be suitable for all types of neural data. Additionally, the model requires a large number of parameters to be learned, which may lead to overfitting. The authors should provide more detailed results on the model's performance on different types of neural data and investigate methods to reduce overfitting.\",\n  \"Questions\": \"How does the irSLDS model handle cases where the number of states is very large or varies significantly across trials? Are there"}
{"paper_id": "koYsgfEwCQ", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed DynaVol model effectively combines geometric structures and object-centric learning in a differentiable volume rendering framework, leading to a more comprehensive understanding of 3D scenes. The use of slot attention for global representation learning is a valuable contribution. The model's ability to freely edit geometric shapes or manipulate motion trajectories is a significant advantage over 2D scene decomposition methods.\",\n  \"Weaknesses\": \"The paper would benefit from a more detailed explanation of the canonical-space deformation function and its impact on the voxel features. Additionally, the evaluation on more diverse datasets would strengthen the claims of the model's generalizability.\",\n  \"Questions\": \"How does the model handle scenes with complex occlusions or self-occlusions? Can the model be extended to handle scenes with multiple objects of varying sizes and shapes?\"\n}"}
{"paper_id": "o6AN2ZoNw2", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a simple yet effective approach to open-vocabulary semantic segmentation, which achieves state-of-the-art results on three widely tested benchmarks.\\n2. The method can be easily trained from publicly available image-text datasets and generalizes well to multiple testing datasets without requiring fine-tuning.\\n3. The paper highlights the benefits of scalability with data and consistently improving when augmented with self-training.\\n4. The code and demo will be made publicly available soon.\",\n  \"Weaknesses\": \"1. The paper lacks a thorough analysis of the proposed method's limitations and potential drawbacks.\\n2. The evaluation metrics used are not explicitly mentioned, which makes it difficult to assess the method's performance.\\n3. The paper does not provide a clear comparison with other state-of-the-art methods, which makes it difficult to evaluate the method's novelty and impact.\",\n  \"Questions\": \"1. How does the method handle out-of-vocabulary words or words that are not present in the training dataset?\\n2. Can the method be adapted to"}
{"paper_id": "HXZK1Z8tHa", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed ShareFormer model is a well-structured and well-implemented solution to the challenges of Transformer-based networks in image restoration tasks. The addition of residual connections to the \\\"Value\\\" of self-attention is a clever approach to improve the network's trainability without compromising its performance. The experimental results demonstrate the effectiveness and efficiency of ShareFormer, and the open-sourcing of the code and pre-trained models will facilitate further research and applications. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that it relies heavily on the use of residual connections, which may not be applicable to all types of image restoration tasks. Additionally, the paper could benefit from a more thorough comparison with other state-of-the-art methods, including those that do not use residual connections. Furthermore, the paper could provide more details on the impact of the residual connections on the model's performance and trainability.\",\n  \"Questions\": \"How does the addition of residual connections to the \\\"Value\\\" of self-attention affect the"}
{"paper_id": "QO3yH7X8JJ", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method, Diff-SR, is a novel attempt to adapt pre-trained DGMs for arbitrary-scale super-resolution tasks without additional training efforts. The key insight of injecting noise into the low-resolution images before invoking the DGM's backward diffusion process is an interesting and effective approach. The Perceptual Recoverable Field (PRF) metric provides a good way to determine the suitable amount of noise to inject, which is a key component of the proposed method.\",\n  \"Weaknesses\": \"The paper assumes that the pre-trained DGMs are available, which might not be the case in all scenarios. Additionally, the proposed method relies on a specific amount of noise to be injected, which might not be generalizable to all cases. The paper does not provide a clear explanation of how the Perceptual Recoverable Field (PRF) metric is computed.\",\n  \"Questions\": \"How does the proposed method perform when the pre-trained DGMs are not available? What is the computational cost of computing the Perceptual Recoverable Field ("}
{"paper_id": "sNtDKdcI1f", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 2,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a clear and concise analysis of the relationship between reward and length in RLHF. The results show a strong correlation between the two, which is a valuable contribution to the field.\\n2. The authors explore interventions to mitigate length increases during RL and reward model learning, which is a relevant and interesting direction for future research.\\n3. The paper's findings have implications for the development of more effective reward models and RLHF algorithms.\",\n  \"Weaknesses\": \"1. The paper's main contribution is somewhat undermined by the fact that the authors find that even a reward based solely on length can reproduce most of the downstream improvements. This makes the need for RLHF less clear.\\n2. The paper could benefit from a more detailed analysis of the interventions used to mitigate length increases, including a more thorough evaluation of their effectiveness.\\n3. The paper's focus on the relationship between reward and length may not be the most significant contribution to the field, given the existing body of work on RLHF and reward models.\",\n  \""}
{"paper_id": "HFtrXBfNru", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a solid theoretical foundation for understanding the limitations of GNNs in evolving graphs, and the authors' approach to addressing this issue is well-motivated and effective. The use of an adaptive feature extractor through self-supervised graph reconstruction is a nice innovation. The experimental results on both synthetic and real-world datasets are convincing, and the ablation studies demonstrate the importance of graph reconstruction. The writing is clear and concise.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion of the limitations of the proposed approach, especially in terms of scalability and computational cost. The authors should also provide more insights into the design choices made for the adaptive feature extractor. Additionally, the paper could be improved by including more diverse and challenging real-world datasets.\",\n  \"Questions\": \"How does the proposed approach generalize to graphs with different types of evolution, such as those with changing edge weights or node attributes? What is the computational cost of the adaptive feature extractor, and how does it compare to other methods? Are there any potential applications of the"}
{"paper_id": "4u0ruVk749", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors propose a novel approach to capture unobserved confounders using a diffusion model, which is an innovative application of recent advances in latent variable modeling. This could lead to more accurate ITE estimation in real-world scenarios. \\n2. The derivation of the variational bound in closed form is a significant contribution, making the model more feasible to implement. \\n3. The experiments demonstrate consistent improvements over state-of-the-art methods on both synthetic and benchmark datasets.\",\n  \"Weaknesses\": \"1. The paper assumes that the apriori knowledge is available, which may not be the case in all real-world scenarios. \\n2. The model relies on a specific choice of diffusion process, which may not be generalizable to other types of unobserved confounders. \\n3. The paper could benefit from a more thorough analysis of the sensitivity of the results to the choice of hyperparameters.\",\n  \"Questions\": \"1. How does the model handle cases where the apriori knowledge is incomplete or uncertain?"}
{"paper_id": "Pa6SiS66p0", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper addresses an important problem in deep neural networks, i.e., catastrophic forgetting, and explores a promising direction for its mitigation by leveraging multiple modalities. The proposed benchmark for multi-modal continual learning is well-designed and provides a strong baseline for future research. The findings on the role and interactions of multiple modalities in mitigating forgetting are significant and provide valuable insights.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed analysis of the individual modalities and their contributions to the overall performance. The proposed method for integrating and aligning information from different modalities is not fully explained, and more details on the implementation and evaluation of this method would be helpful.\",\n  \"Questions\": \"How do the authors plan to extend their work to more complex scenarios, such as multiple modalities with different temporal relationships? Can the proposed benchmark be adapted to other types of data, such as images or videos? What are the computational requirements of the proposed method, and how does it compare to other approaches in terms of efficiency?\"\n}"}
{"paper_id": "rkplYfqUr0", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel and well-motivated approach to address the miscalibration and brittleness of language model prompting. The generative prompting framework is a clear and significant contribution to the field of NLP.\\n2. The experiments are thorough and well-designed, with a good variety of datasets and language models used to demonstrate the effectiveness of the approach.\\n3. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"1. The paper relies heavily on the effectiveness of the language models used, which may not generalize to all cases. The paper does not discuss this limitation.\\n2. The evaluation metrics used are not explicitly stated in the abstract, which makes it difficult to assess the quality of the results.\\n3. The paper could benefit from a more detailed discussion of the limitations of the proposed approach.\",\n  \"Questions\": \"1. How does the paper address the issue of overfitting to the specific language models used in the experiments?\\n2. Are there any plans to extend the"}
{"paper_id": "pTU2X9mUBe", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant gap in the research field by providing a large-scale, publicly available last-mile delivery dataset, which can benefit the supply chain community, data mining community, and beyond.\\n2. The dataset is comprehensive, diverse, and offers unparalleled opportunities for researchers.\\n3. The authors have made the dataset publicly available, which is a significant contribution to the research community.\\n4. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"1. The paper does not provide a detailed analysis of the dataset, such as its limitations and potential biases.\\n2. The evaluation of the dataset is limited to three tasks and classical baseline models.\\n3. The authors do not discuss the potential applications of the dataset beyond the three tasks mentioned in the paper.\\n4. The dataset homepage is not a standard academic repository, but rather an anonymous platform.\",\n  \"Questions\": \"1. How was the dataset collected, and what are the potential sources of bias?\\n2. What are the limitations of"}
{"paper_id": "uhtQyRrTzY", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel approach to generate multi-view datasets at scale without requiring additional annotations. The method is well-executed and showcases promising results on various dense geometric tasks. The experiments are comprehensive and demonstrate the effectiveness of the proposed approach. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that the quality of the generated datasets is good enough for training models. However, there is no evaluation of the quality of the generated datasets. This could be a concern, especially when dealing with real-world data. Additionally, the paper relies heavily on simulated environments and real-world videos, but it is unclear how well the method would work with other types of data sources. Finally, the paper could benefit from more detailed comparisons with existing methods and a more thorough analysis of the results.\",\n  \"Questions\": \"How do the authors ensure the quality of the generated datasets? Are there any plans to release the generated datasets or the code for the method? How does the method perform on other types of data sources, such as LiD"}
{"paper_id": "WZfatbNdLV", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel approach to predict and manipulate alternative splicing using generative models. The use of transformers and side information to predict splicing in a tissue-specific manner is an interesting and potentially impactful contribution. The application of Bayesian Optimization to design RNA splicing outcome is also a strong point of the paper. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that the generative model is able to capture the complexity of alternative splicing, which might be a limitation. The use of a costume loss function for RNA splicing outcome design might be too specific and not generalizable to other problems. The paper does not provide a clear explanation of how the TrASPr model is able to identify relevant regulatory features.\",\n  \"Questions\": \"How does the TrASPr model handle the variability of alternative splicing across different tissues? Is the use of Bayesian Optimization to design RNA splicing outcome a novel application of the algorithm, or has it been used before in a similar context?\"\n}"}
{"paper_id": "YkRwadXWHd", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a multi-modal framework that combines video, keypoints, and optical flow modalities to extract representative features for CSLR, which is a significant contribution to the field. The introduction of hierarchical knowledge distillation (HKD) framework is also a good idea to alleviate the computational burden. The experimental results demonstrate the effectiveness of the approach, achieving state-of-the-art performance on three benchmark datasets.\",\n  \"Weaknesses\": \"The paper assumes that the keypoints and optical flow information are available, which might not be the case in all scenarios. The use of HKD framework might lead to information loss during the knowledge transfer process. The paper could benefit from a more detailed analysis of the computational cost savings achieved by the HKD framework.\",\n  \"Questions\": \"How does the proposed multi-modal framework compare to other existing methods that use a single modality for CSLR? What are the implications of using HKD framework on the interpretability of the model? How does the paper ensure that the keypoints and optical flow information are accurately extracted?\"\n}"}
{"paper_id": "q4Bim1dDzb", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes an efficient framework for inverse rendering, dubbed UniVoxel, which integrates geometry, materials, and illumination into a unified representation.\\n2. The use of Spherical Gaussians to represent incident light radiance is an interesting and novel approach.\\n3. The experiments demonstrate a significant reduction in training time compared to other inverse rendering methods.\",\n  \"Weaknesses\": \"1. The paper could benefit from more extensive comparisons with other state-of-the-art methods, especially in terms of reconstruction quality.\\n2. The authors could provide more insights into the choice of Spherical Gaussians for representing incident light radiance.\\n3. The paper could be improved by providing more details on the implementation of the UniVoxel framework.\",\n  \"Questions\": \"1. How does the UniVoxel framework handle complex scenes with multiple light sources?\\n2. Can the authors provide more information on the computational complexity of the UniVoxel framework?\\n3. How does the use of Spherical Gaussians affect the reconstruction quality in scenes with"}
{"paper_id": "39cPKijBed", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed time-dependent importance reweighting method is a novel and effective approach to mitigate dataset bias in diffusion models. The connection to traditional score-matching and the establishment of convergence to an unbiased distribution are well-motivated and theoretically sound. The experimental results demonstrate the usefulness of the proposed method, outperforming baselines on various datasets and bias settings. The code availability is also a plus.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanation of the time-dependent density ratio and its derivation. Additionally, the experimental results could be more comprehensive, including more datasets and bias settings to demonstrate the robustness of the proposed method. Furthermore, the comparison with other methods, such as importance sampling, would be helpful to demonstrate the superiority of the proposed approach.\",\n  \"Questions\": \"How does the time-dependent density ratio improve upon previous approaches, and what are the implications of this improvement? Are there any potential limitations or challenges in applying this method to more complex datasets or applications? Can the proposed method be extended to other types of generative"}
{"paper_id": "aG3EARrrd1", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed Online Sparse Residual Tree (OSRT) algorithm effectively handles streaming multivariate scattered data and achieves higher efficiency and accuracy compared to existing online RBF methods. The incremental method for exploring the central node of the RBF function ensures sparsity and accuracy of the model. The evaluation on several datasets showcases the superiority of OSRT.\",\n  \"Weaknesses\": \"The paper would benefit from a more detailed explanation of the online tree decomposition process and how the sparse and appropriate RBF refinement is implemented at each child node. Additionally, the comparison with existing online RBF methods could be more comprehensive, including a discussion of the computational complexity and memory requirements of OSRT.\",\n  \"Questions\": \"How does the OSRT algorithm handle concept drift or concept shift in the data, where the underlying distribution changes over time? What is the impact of the maximum depth of the tree on the performance of the OSRT model?\"\n}"}
{"paper_id": "Ts95eXsPBc", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The idea of incorporating spatial information into episodic memory is a good approach.\\n2. The proposed Spatially-Aware Transformer model is a novel contribution to the field.\\n3. The Adaptive Memory Allocator is a useful tool for optimizing memory utilization.\\n4. The paper provides a clear explanation of the proposed method and its advantages.\\n5. The experiments demonstrate the effectiveness of the proposed model in various environments and downstream tasks.\",\n  \"Weaknesses\": \"1. The paper does not provide a clear comparison with existing transformer-based episodic memory models.\\n2. The Adaptive Memory Allocator is not fully explained, and more details are needed.\\n3. The paper assumes that the spatial dimension is always relevant, but it would be better to discuss the cases where it is not.\\n4. The paper could benefit from more experiments to demonstrate the robustness of the proposed model.\\n5. The paper does not discuss the scalability of the proposed model.\",\n  \"Questions\": \"1. How does the Spatially-Aware Transformer model"}
{"paper_id": "LfhG5znxzR", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper introduces a novel and innovative approach to understanding neural networks by using a vector quantization bottleneck to produce sparse, discrete, and more interpretable codebook features.\\n2. The authors demonstrate the effectiveness of codebook features in controlling neural network behavior and improving interpretability.\\n3. The paper provides a clear and well-organized presentation of the approach and its applications.\",\n  \"Weaknesses\": \"1. The paper assumes that the codebook features are fixed and do not change during training, which may limit the ability to generalize to different tasks.\\n2. The authors do not provide a clear explanation of how to select the codebook size and how it affects the performance of the model.\\n3. The paper does not provide a thorough comparison with other approaches to interpretability and controllability of neural networks.\",\n  \"Questions\": \"1. How does the choice of codebook size affect the performance of the model?\\n2. Can the codebook features be learned dynamically during training, rather than being fixed?\\n3"}
{"paper_id": "bUGzjiUsIq", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses a relatively understudied problem in machine learning, namely partially annotated multi-label classification, and proposes a novel framework called PAPG to tackle this problem.\\n2. The paper provides a clear and well-structured presentation of the problem, the proposed method, and the experimental results.\\n3. The authors provide a comprehensive evaluation of their method on diverse classification tasks, showcasing its effectiveness and superiority over other methods.\",\n  \"Weaknesses\": \"1. The paper assumes that the positive classes are partially annotated, but it does not provide a clear explanation of how this is handled in practice. For example, how does the model handle missing annotations for some positive classes?\\n2. The paper does not provide a thorough comparison with other state-of-the-art methods for partially annotated multi-label classification.\\n3. The authors claim that their method is effective and superior to other methods, but they do not provide a clear explanation of why this is the case.\",\n  \"Questions\": \"1. How does the model handle the case where"}
{"paper_id": "uMAujpVi9m", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors propose a novel approach to pretraining pocket representations, leveraging high-resolution atomic protein structures and pretrained small molecule representations. This approach generates a large number of pocket-ligand complex structures, allowing for effective pretraining of the pocket encoder. The method, ProFSA, achieves state-of-the-art performance across various tasks, including pocket druggability prediction, pocket matching, and ligand binding affinity prediction. The use of contrastive learning to align the pocket encoder with the representation of pseudo-ligand furnished by pretrained small molecule encoders is an interesting and effective approach.\",\n  \"Weaknesses\": \"The paper assumes that the high-quality and diverse protein structure databases are readily available, which might not be the case in all scenarios. Additionally, the generation of over 5 million complexes may be computationally expensive and require significant resources. The paper does not provide a clear evaluation of the generalizability of ProFSA to other protein-ligand complex datasets.\",\n  \"Questions\": \"How does the authors handle the potential bias in the generated complex"}
{"paper_id": "uqxBTcWRnj", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper introduces a novel approach to learning symbolic knowledge through a game-theoretic diffusion model, which is a significant contribution to the field. The use of clustering information gain and heuristic shape score as metrics to evaluate the model is also a good idea. The experiments conducted on three abstract compositional visual object datasets are comprehensive and well-designed. The results show that the proposed method outperforms state-of-the-art unsupervised part segmentation methods.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the game-theoretic diffusion model and how it is used to decompose the input into visual parts. Additionally, the authors could provide more insights into the online prototype clustering algorithm used in the EM algorithm. The human evaluation is a good addition, but it would be more convincing if more participants were involved. The paper could also benefit from a more thorough discussion of the limitations of the proposed method and potential future work.\",\n  \"Questions\": \"How does the game-theoretic diffusion model handle cases where the input image contains multiple objects"}
{"paper_id": "o4CLLlIaaH", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel paradigm for generalizable NeRF that addresses the limitations of previous methods. \\n2. The approach of constructing the generalizable neural field based on point-based rendering is innovative and shows promise. \\n3. The experiments demonstrate the effectiveness of the proposed method in delivering better geometries, view consistencies, and rendering quality than the counterparts and benchmarks.\",\n  \"Weaknesses\": \"1. The paper assumes that the geometric priors are accurate, but in real-world scenarios, these priors might not be available or accurate. \\n2. The nonuniform log sampling strategy is not well-explained, and more details about its implementation would be helpful. \\n3. The learnable kernel spatially augmented with features for feature aggregations might not be effective for all types of geometries.\",\n  \"Questions\": \"1. How does the proposed method handle cases where the geometric priors are not available or are inaccurate?\\n2. Can the nonuniform log sampling strategy be applied to other types of rendering"}
{"paper_id": "KNzL1nglNB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel approach to address the label insufficient scenario, which is a significant problem in many real-world applications. The Label-Encoding Risk Minimization (LRM) is a clever idea that draws inspiration from the phenomenon of neural collapse. 2. The paper provides a clear and concise explanation of the proposed method and its theoretical analysis. 3. The experimental results demonstrate the superiority of LRM under several label insufficient scenarios.\",\n  \"Weaknesses\": \"1. The paper assumes that the unlabeled samples are i.i.d. and drawn from the same distribution as the labeled samples. This assumption might not hold in many real-world scenarios. 2. The paper does not provide a clear explanation of how to choose the hyperparameters for the LRM. 3. The paper only provides a limited number of experimental results, and it would be better to have more results to demonstrate the robustness of the LRM.\",\n  \"Questions\": \"1. How does the LRM perform when the unlabeled samples are not i.i.d"}
{"paper_id": "pvhyBB86Bt", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper makes a significant contribution to the field by extending the analysis of Jacobian behavior beyond initialization and providing a theoretical explanation for the crucial role of initialization. The use of recent breakthrough results in random matrix theory is a strength. The analysis is thorough and well-written.\",\n  \"Weaknesses\": \"The paper assumes a high level of background knowledge in random matrix theory and neural networks, which may limit its accessibility to a broader audience. The notation and terminology used are not always clearly defined, making it difficult to follow at times.\",\n  \"Questions\": \"How do the results generalize to more complex neural network architectures, such as ResNets or Transformers? Are there any plans to extend this analysis to other types of neural networks or optimization algorithms?\"\n}"}
{"paper_id": "Mylk5iamJC", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a novel formalization of open-set recognition (OSR) based on learning theory, which is a significant contribution to the field. The proposed framework, L-GMM, is well-designed and effective in performing both CSR and OSR. The experimental results demonstrate the superiority of L-GMM over discriminative counterparts in CSR and specialized methods in OSR. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that the generative model is known, which might not be the case in real-world scenarios. The authors should provide more discussion on how to handle this limitation. Additionally, the paper focuses on image recognition and segmentation, and it would be interesting to see the results on other domains.\",\n  \"Questions\": \"How does the L-GMM framework handle the case when the generative model is unknown? What are the computational complexities of the proposed method, and how do they compare to existing methods?\"\n}"}
{"paper_id": "MsOcVFzv8D", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a rigorous theoretical analysis of multi-domain text classification (MDTC) and proposes a margin discrepancy-based adversarial training (MDAT) approach. The experimental results demonstrate the efficacy of the proposed method, surpassing state-of-the-art baselines on two MDTC benchmarks. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The paper assumes that the source and target domains have the same label space, which may not be the case in all MDTC scenarios. The Rademacher complexity-based generalization bound may not be applicable to all domains. The empirical study is limited to two benchmarks, and the authors should consider expanding the evaluation to more diverse datasets.\",\n  \"Questions\": \"How do the authors handle the case where the source and target domains have different label spaces? Is it possible to extend the theoretical analysis to the case where the domains have different label spaces?\"\n}"}
{"paper_id": "fBlHaSGKNg", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a well-structured and well-motivated approach to the problem of efficient sample selection in semi-supervised learning. The proposed method, UPA, is based on a modified Frank-Wolfe algorithm that minimizes a novel criterion \u03b1-MMD, which is a good choice for selecting a representative and diverse subset for annotation. The experiments show that UPA consistently improves the performance of several popular SSL methods, surpassing various prevailing Active Learning and Semi-Supervised Active Learning methods even under constrained annotation budgets. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that the unlabeled data is available, which may not always be the case in real-world scenarios. The paper does not discuss the scalability of the proposed method to large datasets. The experiments are limited to a few popular SSL methods, and it would be interesting to see the performance of UPA on a wider range of methods.\",\n  \"Questions\": \"How does the proposed method perform when the unlabeled data is not available? What are"}
{"paper_id": "HwQ8NVvdmm", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel technique to enable low-precision training with integer matrix multiplications using Hadamard transforms and stochastic rounding. The proposed method reduces computational precision without degrading model accuracy. The experimental results on several human activity recognition datasets and CIFAR100 show the effectiveness of the technique. The authors also provide a clear explanation of the method and its benefits.\",\n  \"Weaknesses\": \"The paper assumes that the Hadamard transforms are inexpensive, but it is not clear if this is always the case. The authors should provide more details about the cost of the Hadamard transforms and how they are implemented. Additionally, the paper could benefit from more comprehensive experimental results, such as comparisons with other low-precision training methods and a more detailed analysis of the accuracy degradation.\",\n  \"Questions\": \"How does the proposed method compare to other low-precision training methods, such as fixed-point arithmetic or tensor train decompositions? Are the Hadamard transforms computationally expensive or can they be efficiently implemented? What are the limitations of the proposed method in"}
{"paper_id": "cZo6pDtDZr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a significant improvement over previous work, achieving better sample complexity for estimating collision probability with error at most \u03b5. The introduction of a sequential testing algorithm for collision probability is also a novel contribution. The paper is well-written and easy to follow, with clear explanations of the algorithms and their analysis.\",\n  \"Weaknesses\": \"The paper relies heavily on the technical details of the algorithms, which may make it challenging for non-experts to understand. The experiments could be more comprehensive, with more datasets and scenarios considered.\",\n  \"Questions\": \"How do the authors plan to extend their work to other discrete distributions? Can the algorithms be adapted for continuous distributions? The paper mentions that the algorithms have nearly the optimal sample complexity, but what does this mean in terms of practical applications?\"\n}"}
{"paper_id": "F7QnIKlC1N", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed GTMGC network is a well-designed approach for predicting the ground-state conformation of molecules. The use of Graph-Transformer and the novel self-attention mechanism, MSRSA, is a good contribution to the field of molecular modeling. The experimental results demonstrate the effectiveness of the proposed approach. The use of Molecule3D benchmark dataset and QM9 dataset is a good choice for evaluation.\",\n  \"Weaknesses\": \"The paper could be improved by providing more details about the experimental setup and the preprocessing steps. Additionally, the paper could benefit from a more thorough analysis of the limitations of the proposed approach and a discussion of potential future work.\",\n  \"Questions\": \"How does the proposed approach compare to other molecular modeling methods, such as quantum mechanical calculations? What are the computational resources required for the proposed approach, and how does it compare to other methods in terms of scalability? How can the proposed approach be extended to more complex molecules or larger systems?\"\n}"}
{"paper_id": "8JCn0kmS8W", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper presents a novel and well-structured framework for generating harmonious audio content with controllable conditions.\\n2. The use of Large Language Models (LLMs) to connect various audio models is a unique and effective approach.\\n3. The experimental results demonstrate the potential of WavJourney in crafting engaging storytelling audio content from text.\\n4. The introduction of a new multi-genre story benchmark is a significant contribution to the field.\",\n  \"Weaknesses\": \"1. The paper assumes the availability of a text instruction, which may not always be the case in real-world scenarios.\\n2. The reliance on LLMs may limit the generative capabilities of WavJourney to the quality of the LLMs used.\\n3. The computational complexity of executing the computer program generated by WavJourney may be high, especially for long audio content.\",\n  \"Questions\": \"1. How does WavJourney handle cases where the text instruction is incomplete or ambiguous?\\n2. Can WavJ"}
{"paper_id": "XbLffB0T2z", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel approach to enhance the transferability of poisoning attacks by leveraging the gradient information with two specific algorithms from supervised and unsupervised contrastive learning paradigms. The experimental results demonstrate the effectiveness of the proposed method in generating poisoned samples with improved transferability. The paper fills a gap in the existing literature by considering the impact of different learning paradigms on the effectiveness of poisoning attacks.\",\n  \"Weaknesses\": \"The paper assumes that the adversary has knowledge of the victim's learning algorithm, which may not always be the case in real-world scenarios. Additionally, the proposed method may not be effective against more advanced defense mechanisms that can detect and mitigate poisoning attacks.\",\n  \"Questions\": \"How does the proposed method compare to other existing methods in terms of attack transferability and effectiveness? Are there any limitations or potential biases in the experimental setup or evaluation metrics used in the paper?\"\n}"}
{"paper_id": "4y3GDTFv70", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a new perspective on the relationship between languages and their underlying meanings, and explores the sparse structures in the joint distribution of languages. The author's use of LLMs to assess the marginal distribution of languages is well-motivated and provides a convenient means of exploring the sparse structures in the joint distribution for effective inferences. The quantitative results are convincing and demonstrate the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.\",\n  \"Weaknesses\": \"The paper assumes a strong association between languages and their underlying meanings, which may not always be the case. The use of LLMs to assess the marginal distribution of languages may not be applicable to all languages. The paper does not provide a clear explanation of how the Bayesian inference works in the context of LLMs.\",\n  \"Questions\": \"How does the author's framework generalize to languages with complex syntax or semantics?"}
{"paper_id": "ODSgo2m8aE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes an innovative approach to stabilize GNN outputs by using Lipschitz bounds, which is a significant contribution to the field of graph neural networks and fairness.\",\n  \"Weaknesses\": \"The paper is well-written and easy to follow, but some of the mathematical derivations could be more detailed and less concise.\",\n  \"Questions\": \"How does the proposed method handle the case when the input graph is not well-structured or contains noise? How can the Lipschitz bound be used to improve the fairness of GNNs in real-world applications?\"\n}"}
{"paper_id": "RlfD5cE1ep", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides an interesting analysis of the learning dynamics of non-contrastive learning methods, specifically the role of feature normalization in preventing collapse.\\n2. The extension of the analysis to the cosine loss is a valuable contribution to the field.\\n3. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"1. The paper builds upon previous work by Tian et al. (2021), but the connection to the original paper is not clearly established.\\n2. The analysis is limited to a specific loss function (cosine loss) and it is not clear how the results generalize to other loss functions.\\n3. The paper could benefit from more experimental results, such as visualizations of the learning dynamics.\",\n  \"Questions\": \"1. How do the results change when using other loss functions, such as the L1 loss or the mean squared error?\\n2. Can the authors provide more experimental results to support their claims, such as visualizations of the learning dynamics?\\n3. How does the"}
{"paper_id": "18TfucMNTr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides an interesting and well-motivated approach to reducing the risk of getting stuck in local minima during training of deep neural networks. The idea of using continuation as a form of regularization is novel and could have significant practical implications. The experiments demonstrate the effectiveness of the approach on several test problems.\",\n  \"Weaknesses\": \"The paper assumes that the objective function can be slowly morphed into the original non-convex function, which may not always be the case in practice. The regularization approach is not clearly motivated and its effectiveness is not thoroughly evaluated. The experiments are limited to a small number of test problems and more extensive evaluations would be needed to confirm the generalizability of the approach.\",\n  \"Questions\": \"How does the continuation optimizer handle cases where the objective function has multiple local minima with similar values? What is the computational cost of the regularization approach and how does it compare to other regularization techniques? Are there any theoretical guarantees on the convergence of the continuation optimizer?\"\n}"}
{"paper_id": "JGTC6WgO4T", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel framework for multi-source black-box domain adaptation, which is a significant contribution to the field of unsupervised domain adaptation.\\n2. The proposed method, Label Space-Induced Pseudo Label Refinement (LPR), is well-justified and provides a clear explanation of how it works.\\n3. The experimental results on four benchmark datasets show that MSBDA using LPR achieves highly competitive performance compared to state-of-the-art approaches.\",\n  \"Weaknesses\": \"1. The paper assumes that the source predictions are available, which might not be the case in practice. The authors should discuss the limitations of their approach in this regard.\\n2. The evaluation of the proposed method is limited to four benchmark datasets, and it would be beneficial to include more datasets to demonstrate its generalizability.\\n3. The paper does not provide a clear explanation of how the Pseudo label Refinery Network (PRN) is trained.\",\n  \"Questions\": \"1. How does the PRN learn the"}
{"paper_id": "fQHb1uZzl7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed architecture effectively unifies feature aggregation and cost aggregation, and the interleaved coarse-to-fine design promotes accurate correspondence estimation. The use of self- and cross-attention mechanisms is innovative and well-executed. The framework's ability to produce multi-scale predictions and compute confidence scores is a significant improvement over existing methods.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion on the choice of hyperparameters and the impact of the coarse-to-fine design on the final results. The evaluation on geometric matching is limited to a single dataset, and it would be beneficial to include more diverse benchmarks. The paper assumes a fixed number of attention layers, which might not be optimal for all scenarios.\",\n  \"Questions\": \"How does the proposed architecture handle cases where the feature and cost volumes have different resolutions? Can the authors provide more insight into the computation of confidence scores and how they affect the final prediction? Is there a clear explanation of why the coarse-to-fine design is necessary, and what would happen if it were removed?\"\n}"}
{"paper_id": "YjG29CIOP7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper addresses a significant issue in data-free meta-learning and proposes a novel approach to handle task-distribution shift. The authors provide a clear and concise introduction to the problem, and the proposed method, TEAPOT, is well-motivated. The experiments demonstrate the effectiveness of TEAPOT in handling task-distribution shift, and the defense strategy, ROSY, is a good addition to existing DFML methods.\\n\\nThe paper is well-organized and easy to follow. The authors provide a clear overview of the related work and the proposed method, and the experimental results are well-presented.\",\n  \"Weaknesses\": \"One potential issue with TEAPOT is that it may require a large memory to store the old tasks, which could be a limitation in certain scenarios. Additionally, the authors could provide more insights into the learnable reliability score used in ROSY, as it is not entirely clear how it is learned.\\n\\nThe paper assumes that the pre-trained models are available, which may not always be the case in practice."}
{"paper_id": "DmDpu3r8iU", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper makes a significant contribution to the field of Large Language Models (LLMs) by proposing a novel framework for evaluating their understanding of human values. The Value Understanding Measurement (VUM) framework is well-structured and provides a clear methodology for assessing both 'know what' and 'know why'. The use of the Schwartz Value Survey and the development of a thousand-level dialogue dataset with GPT-4 are notable strengths. The paper also raises important questions about the potential risks of LLMs crafting plausible explanations without truly understanding their inherent value.\",\n  \"Weaknesses\": \"The paper assumes that the Schwartz Value Survey is a reliable measure of human values, but it may not capture the full complexity of human values. The use of GPT-4 as a baseline may also be problematic, as it is a highly advanced language model that may not be representative of all LLMs. The paper could benefit from more discussion of the potential limitations and biases of the VUM framework and the Schwartz Value Survey.\",\n  \"Questions\": \"How does the V"}
{"paper_id": "kXHEBK9uAY", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The Hierarchical Diffuser is an effective planning method that combines the advantages of hierarchical and diffusion-based planning. The use of a 'jumpy' planning strategy at the high level is a novel approach that allows for a larger receptive field at a lower computational cost. The empirical evaluations on standard offline reinforcement learning benchmarks demonstrate the method's superior performance and efficiency compared to other methods. The exploration of generalization capability on compositional out-of-distribution tasks is also a valuable contribution.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the 'jumpy' planning strategy and how it is used to guide the low-level planner. Additionally, the evaluation on compositional out-of-distribution tasks seems limited to a single dataset, and it would be helpful to see more diverse evaluation settings to assess the method's generalizability.\",\n  \"Questions\": \"How does the 'jumpy' planning strategy improve the receptive field of the high-level planner, and what are the implications for the low-level planner? What is the computational cost savings of"}
{"paper_id": "i7LCsDMcZ4", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed method, EventRPG, leverages relevance propagation on the spiking neural network for more efficient augmentation, which is a novel approach.\\n2. The authors provide a clear and concise description of their method and its advantages.\\n3. The experimental results show that EventRPG achieves state-of-the-art performance on several SNN structures.\",\n  \"Weaknesses\": \"1. The authors do not provide a detailed explanation of the Spiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking Layer-wise Relevance Propagation rule (SLRP).\\n2. The paper assumes that the reader is familiar with event-based classification tasks and Spiking Neural Networks (SNNs), but it would be beneficial to provide a brief introduction or background information.\\n3. The paper does not discuss the limitations of the proposed method and potential future work.\",\n  \"Questions\": \"1. How does the proposed method, EventRPG, compare to other data augmentation methods for SNNs?\\n2. Can"}
{"paper_id": "jHdz0CIS2y", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed SelfVC framework is innovative and well-structured, and the use of self-supervised learning and speaker verification models is a good idea. The iterative training approach with self-synthesized examples is also a nice touch. The paper is well-written and easy to follow. The results are impressive and demonstrate the effectiveness of the proposed method.\",\n  \"Weaknesses\": \"One potential issue is that the paper assumes a fixed set of speakers and languages, which may limit its applicability. Additionally, the paper does not provide a clear explanation of how the prosodic information is derived from the audio signal and SSL representations. Finally, the paper could benefit from more detailed comparisons with other state-of-the-art methods, especially those that do not use self-supervised learning.\",\n  \"Questions\": \"How does the proposed framework handle cases where the target speaker has a significantly different accent or speaking style than the source speaker? Can the framework be extended to handle multiple speakers and languages simultaneously? What is the computational cost of generating self-synthesized examples, and how does"}
{"paper_id": "NkYCuGM7E2", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper explores a novel application of LLMs in autonomous driving, which is timely and relevant. \\n2. The proposed method combines LLMs with low-level controllers effectively, addressing the challenges of high-level information understanding, generalizability, and interpretability in AD systems. \\n3. The paper presents a clear and well-structured approach to integrating LLM decisions with low-level controllers, making it easy to follow and understand.\",\n  \"Weaknesses\": \"1. The paper does not provide a detailed explanation of how the LLMs are fine-tuned for autonomous driving tasks. It would be beneficial to provide more information on this aspect. \\n2. The evaluation metrics used in the paper seem to focus primarily on single-vehicle tasks. It would be great to see more comprehensive evaluation on multi-vehicle coordination tasks. \\n3. The paper mentions that the proposed method is inspired by human commonsense understanding, but it would be helpful to provide more insight into how this is achieved through LLMs.\","}
{"paper_id": "RtDok9eS3s", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents an excellent simplification of the standard transformer block, making it more efficient and faster to train. The authors' approach is well-motivated by signal propagation theory and empirical observations. The results on both autoregressive decoder-only and BERT encoder-only models demonstrate the effectiveness of the simplified transformers. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper relies heavily on the signal propagation theory, which might not be familiar to all readers. The simplification of the transformer block might be seen as too aggressive by some researchers, potentially leading to a loss of performance or expressiveness.\",\n  \"Questions\": \"How do the authors plan to generalize their approach to other transformer-based models, such as encoder-decoder architectures or models with different attention mechanisms? What are the potential limitations of the simplified transformers in terms of handling complex tasks or large-scale datasets?\"\n}"}
{"paper_id": "YCPDFfmkFr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The work is well-motivated and addresses a specific problem in optimization layers, which is a key component in many machine learning models.\\n2. The proposed approach to differentiate through infeasible QP solutions is novel and has the potential to improve the performance of QP layers.\\n3. The paper is well-written and easy to follow, with clear explanations of the proposed methods and their applications.\\n4. The authors provide a thorough evaluation of their approach, including comparisons with existing methods and a discussion of the limitations and future directions.\",\n  \"Weaknesses\": \"1. The paper could benefit from more extensive experimental results, including more detailed comparisons with state-of-the-art methods and a more thorough analysis of the trade-offs between different approaches.\\n2. Some of the notation and terminology used in the paper could be clarified or defined more explicitly, particularly for readers who are not familiar with convex optimization or QP layers.\\n3. The paper assumes a certain level of background knowledge in convex optimization and QP layers, which may"}
{"paper_id": "ueTdErd5Ib", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method is novel and provides a clear framework for contextual stochastic optimization problems. The approach is well-motivated and the theoretical guarantees are well-presented. The experimental results are convincing and demonstrate the competitiveness of the proposed method. The authors have done a good job in providing a comprehensive comparison with other methods.\",\n  \"Weaknesses\": \"The paper assumes that the uncertain objective function can be well approximated deterministically within each subset, which may not be the case in practice. The authors should provide more details on how to handle this assumption. Additionally, the experimental results are limited to a single real-world electricity generation problem, and it would be beneficial to include more examples to demonstrate the robustness of the proposed method.\",\n  \"Questions\": \"How do the authors plan to handle cases where the uncertain objective function cannot be well approximated deterministically within each subset? What are the computational costs associated with discretizing the feasible region into subsets, and how does this impact the overall efficiency of the proposed method? Can the authors provide more details on the real"}
{"paper_id": "sDmjlpphdB", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach to automate the instruction design process by dividing the problem space into homogeneous regions, each governed by a specialized expert. This is a good idea and shows a good understanding of the limitations of previous methods.\\n2. The two-phase process for constructing the specialized expert for each region is well-designed and effective. The use of cluster-based demo assignment and region-based joint search is a good approach to optimize the instruction complementary to the demo cluster.\\n3. The paper provides a clear and concise explanation of the method and the results, making it easy to understand and follow.\",\n  \"Weaknesses\": \"1. The paper assumes that the demos are semantically similar within a cluster, which might not always be the case. This could lead to suboptimal performance if the demos in a cluster are not well-represented.\\n2. The paper does not provide a clear explanation of how the region-based joint search is implemented, which makes it difficult to understand the optimization process.\\n3. The paper does not provide"}
{"paper_id": "Mdk7YP52V3", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a rigorous theoretical explanation for the observed instabilities in heteroskedastic neural regression models, and the derived nonparametric free energy shows excellent qualitative agreement with empirical model fits on real-world data. The insights from the theory suggest a scheme for optimizing regularization which is quadratically more efficient than the naive approach.\",\n  \"Weaknesses\": \"The paper is quite theoretical and might be difficult for non-experts to follow. The use of statistical physics concepts and mathematical derivations might make the paper less accessible to some readers.\",\n  \"Questions\": \"It would be interesting to see more experimental results to support the claims made in the paper. How do the authors plan to apply their insights to real-world problems, and what are the implications of their findings for practical applications?\"\n}"}
{"paper_id": "YGTSLDAPqb", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed approach, Connect Later, effectively leverages self-supervised pretraining to improve OOD generalization. The method's ability to fine-tune with targeted augmentations designed with knowledge of the distribution shift is a valuable contribution. The experimental results on three real-world datasets demonstrate the effectiveness of Connect Later. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The paper assumes that the generic data augmentations used during pretraining are effective in connecting the source and target domains. However, this may not always be the case, especially when the domains are far apart in the input space. The paper does not provide a comprehensive analysis of the impact of different types of augmentations on the performance of Connect Later. The authors should investigate this aspect in future work.\",\n  \"Questions\": \"What is the computational cost of fine-tuning with targeted augmentations compared to standard fine-tuning? How do the targeted augmentations designed with knowledge of the distribution shift affect the performance of Connect Later? Are there any other methods that can be used"}
{"paper_id": "JTcaziw7G1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a novel approach to private information retrieval using multi-party computation (MPC) that ensures no server observes a client's query or can see the database content. This is a significant contribution to the field of information retrieval and natural language processing.\\n2. The paper presents a new MPC-friendly protocol for inverted file approximate search (IVF) that allows for fast document search over distributed and private data in sublinear communication complexity. This is a first-of-its-kind approach to dense information retrieval.\\n3. The paper highlights the importance of private information retrieval to allow large language models (LLMs) to answer questions on information not included in pre-training data.\\n4. The paper provides a thorough explanation of the proposed approach and its benefits, including ensuring no server observes a client's query or can see the database content.\",\n  \"Weaknesses\": \"1. The paper assumes that the client and servers are honest, but in real-world scenarios, there may be malicious clients or servers. The authors should discuss potential security threats and"}
{"paper_id": "lEkFq4RUCX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed directional distance field (DDF) is an effective and efficient metric for quantifying the dissimilarity between two unstructured 3D point clouds. The authors provide a clear and concise explanation of the method, and the experimental results demonstrate its superiority over existing metrics in various tasks.\",\n  \"Weaknesses\": \"The paper assumes that the reference points are given, which may not be the case in many real-world scenarios. Additionally, the authors do not provide a thorough comparison with other state-of-the-art methods in the field.\",\n  \"Questions\": \"How does the authors plan to handle cases where the reference points are not available? Are there any potential applications of DDF in other fields, such as computer vision or robotics?\"\n}"}
{"paper_id": "Fq8tKtjACC", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The authors have introduced a novel large language model for code, phi-1, which is significantly smaller in size than competing models.\\n2. The model is trained on a selection of \\\"textbook quality\\\" data from the web and synthetically generated textbooks and exercises with GPT-3.5.\\n3. The model attains pass@1 accuracy of 50.6% on HumanEval and 55.5% on MBPP, which is impressive given its small scale.\\n4. The emergent properties of phi-1 compared to its smaller versions, phi-1-base and phi-1-small, are interesting and worth exploring.\\n5. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"1. The authors claim that phi-1 is a new model, but it seems to be a variant of the phi-1-base model, which is not clearly stated.\\n2. The evaluation metrics used are not comprehensive, and more metrics should be included to better assess the model"}
{"paper_id": "8fQlGQkj0S", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper makes a significant contribution to the understanding of in-context learning, providing a rigorous analysis of the risk bounds for both task retrieval and task learning. The authors propose a novel approach to modeling in-context samples and demonstrate the equivalence between in-context prediction and posterior mean. The analysis is thorough and well-motivated. The results are also validated with numerical computations and Transformer model implementation.\",\n  \"Weaknesses\": \"None apparent\",\n  \"Questions\": \"How do the proposed models for pretraining data and in-context samples generalize to other types of tasks or datasets? Is the assumption of mean squared error as a risk measure reasonable for all tasks, or are there other risk measures that might be more suitable?\"\n}"}
{"paper_id": "hRos9WldRK", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a novel and effective approach to learning with noisy labels, which is a significant problem in machine learning. The proposed method, L2B, is well-designed and easy to understand. \\n2. The experiments are extensive and demonstrate the effectiveness of L2B on several benchmark datasets. The comparison with other methods is thorough and fair. \\n3. The code and models are available, which is a big plus.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed discussion of the theoretical foundation of L2B. What is the underlying assumption of the joint reweighting mechanism? How does it relate to other noisy label learning methods?\\n2. The authors claim that L2B is compatible with existing noisy label learning methods, but it would be great to see some concrete evidence or experiments to support this claim.\\n3. The paper assumes that the noisy labels are independent and identically distributed, which may not be the case in many real-world scenarios.\",\n  \"Questions\": \"1"}
{"paper_id": "ttRSBZiCiu", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The WaveFluid model is an interesting approach to speech synthesis, and the use of a velocity field for adversarial training is a novel aspect. The model's performance is competitive with previous vocoders, especially within 10 inference steps. The reparameterization technique to reduce memory footprint and improve training efficiency is also a strength.\",\n  \"Weaknesses\": \"The paper lacks a clear motivation for the choice of velocity field and the use of mel-spectrogram as a condition. The division of the model into two stages and the use of reparameterization techniques may not be well-motivated. The experimental results are limited to a single task and a small number of inference steps.\",\n  \"Questions\": \"What is the theoretical justification for using a velocity field in the WaveFluid model? How does the reparameterization technique affect the model's performance in terms of sample quality and training efficiency? Are there any plans to extend the WaveFluid model to other tasks or applications?\"\n}"}
{"paper_id": "WYsLU5TEEo", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel and well-structured approach to address both interpretability and robustness in neural image classifiers. The use of image-to-image translation GANs to generate counterfactual samples is an interesting direction. The method's effectiveness is demonstrated through two distinct tasks, and the results show promising improvements in both explainability and robustness. The paper is well-written, and the authors provide clear explanations of their approach and results.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed analysis of the trade-offs between interpretability and robustness. The authors should also provide more insights into the relationship between the discriminator's 'fakeness' level and the uncertainty of the predictions. Additionally, the experiments could be improved by including more diverse and challenging datasets.\",\n  \"Questions\": \"How does the proposed method compare to other existing approaches that address only one of the issues (interpretability or robustness)? Are there any potential drawbacks or limitations of using GANs for this task? Can the authors provide more information about the computational cost"}
{"paper_id": "KKZaj2QS3G", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper addresses two significant gaps in contrastive self-supervised learning for time series analysis, namely the lack of consideration for noise and the need for efficient yet lightweight encoder architectures. The proposed sampling strategy and dilated convolution-based encoder architecture are well-motivated and demonstrate promising results. The experiments are comprehensive and show significant improvements over state-of-the-art methods.\",\n  \"Weaknesses\": \"The paper assumes that the noise in the time series is known, which may not always be the case in real-world scenarios. It would be beneficial to explore methods for handling unknown or varying noise levels. Additionally, the paper does not provide a clear comparison with other state-of-the-art methods that also consider noise in their models.\",\n  \"Questions\": \"How does the proposed sampling strategy handle different types and levels of noise in time series data? Can the authors provide more details on how they selected the number of dilated convolutions in the Inception block? What are the computational costs of the proposed method compared to other state-of-the-art methods?\"\n}"}
{"paper_id": "LWDRiFzbHQ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed Vicinal Risk Proxy (VRP) is a novel and well-motivated approach to assessing the generalization ability of classification models. The idea of integrating model responses of neighboring test samples into the correctness indicator of every test sample is intuitive and mathematically sound. The vicinal risk formulation provides a clear and concise framework for developing such a proxy. The experiments demonstrate the effectiveness of VRP in improving the correlation with model accuracy, especially on severe out-of-distribution test sets. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The paper relies heavily on the vicinal risk formulation, which may not be applicable to all types of models or tasks. The proposed approach may not be effective for models that are highly sensitive to individual test samples. The authors could provide more insight into the choice of hyperparameters for VRP, such as the size of the neighborhood and the aggregation method.\",\n  \"Questions\": \"How does the proposed VRP approach handle models that are not based on neural networks, such as decision trees"}
{"paper_id": "Yp01vcQSNl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel graph transformer architecture that explicitly takes into account edge directionality, which is a significant contribution to the field of graph neural networks. The approach is well-justified and technically sound. The experimental results are comprehensive and demonstrate the effectiveness of the proposed method on directed graph learning tasks. The authors also provide a clear and concise presentation of their work.\",\n  \"Weaknesses\": \"The paper could benefit from more discussion on the limitations of the proposed method, particularly in terms of scalability and computational efficiency. Additionally, the authors may want to consider exploring other ways to incorporate directionality into graph transformers, such as using graph convolutional networks or graph attention mechanisms.\",\n  \"Questions\": \"How does the proposed method perform on more complex directed graphs with multiple levels of hierarchy? Are there any cases where the method fails to capture directionality correctly? What are the computational requirements for the proposed method, and how does it compare to other graph neural network architectures?\"\n}"}
{"paper_id": "LndMyiBl3n", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors propose a new measure of a graph's quality, Modified Silhouette Score (MSS), which is generalizable across graphs of different homophily levels. They also present SheAttack, an efficient attack that reduces the distinguishability of nodes without prior knowledge of labels or the victim model. The experimental results show that SheAttack performs comparably to split-unknown white-box attacks.\",\n  \"Weaknesses\": \"The paper focuses on the Restrict Black-box attack, but it would be beneficial to explore other types of attacks as well. The experiments are conducted on both synthetic and real-world graphs, but it would be interesting to see the results on more diverse datasets.\",\n  \"Questions\": \"How does the Modified Silhouette Score (MSS) compare to other measures of graph quality? Can SheAttack be adapted to work with other types of graphs or networks? How does the performance of SheAttack degrade when the homophily level of the graph increases?\"\n}"}
