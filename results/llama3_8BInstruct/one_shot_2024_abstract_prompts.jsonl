{"paper_id": "B0OwtVEejJ", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a comprehensive analysis of the limitations of current NAS methods and proposes a novel solution to bridge the gap between weight sharing and weight entanglement.\\n2. The authors demonstrate the effectiveness of their approach by showing improved performance and memory efficiency.\\n3. The code for the work is openly accessible, which is a significant contribution to the research community.\",\n  \"Weaknesses\": \"1. The paper assumes that the weight-entangled space is a linear combination of individual sub-architectures, which might not be the case in practice.\\n2. The authors do not discuss the scalability of their approach to larger weight-entangled spaces.\\n3. The comparison with existing NAS methods is limited to a few baseline models.\",\n  \"Questions\": \"1. How does the proposed approach perform on more complex weight-entangled spaces, such as those with non-linear combinations of sub-architectures?\\n2. What are the computational costs associated with the proposed approach, and how does it compare to other NAS methods"}
{"paper_id": "ZlEtXIxl3q", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a clear and concise explanation of the limitations of global epistasis models and the advantages of contrastive loss functions. The author's argument is well-supported by theoretical and empirical evidence.\\n2. The proposed method is simple and flexible, and the results demonstrate its effectiveness on benchmark tasks.\\n3. The paper is well-organized and easy to follow.\",\n  \"Weaknesses\": \"1. The paper assumes a relatively narrow scope, focusing on global epistasis models and contrastive loss functions. It would be interesting to see a more comprehensive comparison with other methods.\\n2. The experimental section could benefit from more details on the specific datasets and experimental settings used.\\n3. The paper could be improved by providing more insights into the implications of the results for the broader field of protein engineering.\",\n  \"Questions\": \"1. Can the authors provide more insights into the theoretical limitations of MSE loss in global epistasis models?\\n2. How does the proposed method perform on more complex datasets or tasks?\\n3"}
{"paper_id": "6yJuDK1DsK", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper presents a novel and efficient approach to lifelong test-time adaptation, FEATHER, which disentangles source and target domain knowledge and can be applied on edge devices.\\n2. FEATHER's ability to adapt without requiring access to source data and being orthogonal to existing approaches is a significant advantage.\\n3. The extensive experiments on various datasets demonstrate the efficacy of FEATHER and its competitive performance with state-of-the-art methods.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. While FEATHER is orthogonal to existing lifelong TTA approaches, the paper could benefit from a more detailed comparison with these methods, including a discussion on how FEATHER can be combined with them.\\n2. The paper assumes that the source model is pre-trained and available, which might not be the case in all scenarios.\\n3. The claim that FEATHER can be applied on edge devices might be a stretch, as the additional computational cost of adapting the model might still be significant.\",\n  \"Questions\":"}
{"paper_id": "lt6xKGGWov", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 7,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a novel approach to supervised feature selection using neural estimation of mutual information, which allows for capturing sophisticated relationships between features and targets.\\n2. The method is demonstrated to perform exact selection, whereas other existing methods fail to do so.\\n3. The paper provides clear examples of such relationships and showcases the effectiveness of the approach.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed comparison with existing methods and a discussion of their limitations.\\n2. The authors should provide more insights into the computational complexity and scalability of the proposed approach.\\n3. The paper assumes a clear understanding of mutual information and its estimation, which may not be familiar to all readers.\",\n  \"Questions\": \"1. How does the proposed approach compare to other feature selection methods in terms of computational complexity and running time?\\n2. Can the authors provide more insights into the relationship between the number of features and the performance of the proposed approach?\\n3. Are there any plans to extend this approach to unsupervised feature"}
{"paper_id": "Bvrc6kobWd", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a new perspective to understand the role of margins in contrastive learning using gradient analysis, which is a significant contribution to the field.\\n2. The experimental results demonstrate the effectiveness of emphasizing positive samples and scaling gradients depending on positive sample angles and logits in improving the generalization performance of contrastive learning.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper does not provide a comprehensive review of existing work on contrastive learning and margins, which would have strengthened the argument.\\n2. The experimental results are limited to only two datasets, and it would have been beneficial to include more datasets to validate the findings.\\n3. The paper assumes that the improvement in generalization performance is solely due to the proposed method, without considering other factors that could contribute to the improvement.\",\n  \"Questions\": \"1. How do the authors plan to extend their method to other domains, such as natural language processing or reinforcement learning, where margins may play a different role?\\n2"}
{"paper_id": "FJjHQS2DyE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel conditional adversarial support alignment (CASA) framework that minimizes the conditional symmetric support divergence between the source's and target domain's feature representation distributions, aiming for a more discriminative representation for the classification task. This work could have a significant impact on the field of unsupervised domain adaptation.\\n2. The introduction of a novel theoretical target risk bound is a significant contribution to the field, as it justifies the merits of aligning the supports of conditional feature distributions compared to the existing marginal support alignment approach in the UDA settings. The derivation of the target risk bound is clear and well-explained.\\n3. The paper is well-organized and clearly presented, with a concise and clear writing style.\\n4. The empirical results demonstrate that CASA outperforms other state-of-the-art methods on different UDA benchmark tasks under different label shift conditions, which is a strong evidence of the method's effectiveness.\",\n  \"Weaknesses\": \"1. The paper assumes that the conditional feature distribution is"}
{"paper_id": "xelrLobW0n", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a comprehensive and challenging benchmark for continual learning in LLMs, which is a significant contribution to the field.\\n2. The introduction of TRACE and RCL approaches is innovative and timely, as it addresses a critical challenge in the development of LLMs.\\n3. The experimental results are thorough and demonstrate the effectiveness of RCL in mitigating catastrophic forgetting in LLMs.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with the concept of aligned LLMs and continual learning, which may limit its accessibility to a broader audience.\\n2. The evaluation of RCL is primarily based on a single experiment with a specific dataset, which may not be representative of all scenarios.\\n3. The paper could benefit from a more in-depth discussion of the potential limitations and challenges of RCL.\",\n  \"Questions\": \"1. How does RCL compare to other approaches for mitigating catastrophic forgetting in LLMs?"}
{"paper_id": "bYQkOPvgDw", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed framework PRGNN addresses a significant gap in the current research on robust GNNs by focusing on label noise, which is a crucial aspect of graph learning.\\n2. The authors present a comprehensive and well-structured approach to tackle label noise, incorporating both clean and noisy labels to improve the robustness of GNNs.\\n3. The experimental results demonstrate the effectiveness of PRGNN in handling high noise rates and heterophilous graphs.\\n4. The availability of the implementation code on GitHub is a significant advantage.\",\n  \"Weaknesses\": \"1. The paper primarily focuses on the theoretical formulation of PRGNN and its application to graph learning, with less emphasis on the practical implications and limitations of the approach.\\n2. The authors could provide more insight into the choice of hyperparameters and their impact on the performance of PRGNN.\\n3. While the experimental results are impressive, the paper would benefit from a more detailed analysis of the robustness of PRGNN under different noise distributions and graph structures"}
{"paper_id": "sKPzAXoylB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper introduces a novel approach for continual learning of representations, addressing both catastrophic forgetting and loss of plasticity. This is a significant contribution to the field.\\n2. The proposed method, UPGD, combines gradient updates with perturbations, which is an innovative idea.\\n3. The evaluation is comprehensive, with experiments on challenging streaming learning setups and comparisons with multiple state-of-the-art methods.\\n4. The paper clearly presents the problem, method, and results, making it easy to follow.\\n5. The use of PPO reinforcement learning experiments to demonstrate the avoidance of performance drop is a good addition.\",\n  \"Weaknesses\": \"1. The paper assumes that the useful and less useful units are known, which might not be the case in real-world scenarios.\\n2. The choice of perturbation magnitude might not be optimal and could be further explored.\\n3. The paper does not discuss the computational cost of the proposed method, which could be a concern for large-scale applications.\",\n  \"Questions\": \"1"}
{"paper_id": "H8Qg1IIMaR", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper highlights a specific vulnerability in popular models, namely permutation sensitivity in multiple-choice question answering (MCQA), which is a significant contribution to the field.\\n2. The empirical results showing the vulnerability of popular models to adversarial permutation in answer sets for multiple-choice prompting are surprising and insightful.\\n3. The paper raises an urgent need to carefully analyze the robustness of language and vision-language models.\",\n  \"Weaknesses\": \"1. The paper only discusses a specific vulnerability in popular models, which may not be representative of all models.\\n2. The paper does not provide a comprehensive solution to address the vulnerability, but rather highlights the issue.\\n3. The paper's focus is on permutation sensitivity, but the robustness of models in other aspects (e.g., in-context learning, instruction following) is not addressed.\",\n  \"Questions\": \"1. How do the results generalize to other types of models or applications beyond MCQA?\\n2. Are there any potential solutions or mitigations to address the permutation sensitivity vulnerability?\""}
{"paper_id": "tth2qXY7RU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. SuFP is a breakthrough data type and quantization method that improves both memory footprint and logic efficiency without compromising model accuracy.\\n2. The key idea of SuFP is multi-region piecewise quantization using a tensor-wise scalable bias, which can configure an optimized precision for each region to capture both dense near-zero data and outliers.\\n3. The tailored hardware for SuFP employs only integer arithmetic units and shifters, facilitating a highly compact hardware realization.\\n4. The experimental results show that SuFP quantization achieved accuracy performance on par with, and in some cases even exceeded, that of full precision floating-point (FP32) across vision, language, and generative model benchmarks.\",\n  \"Weaknesses\": \"1. The paper does not provide a clear comparison with other piecewise quantization methods, such as MSFP and BSFP, beyond the computational capability and energy efficiency.\\n2. The experimental results are based on a limited set of benchmarks, and it is unclear how SuFP would perform on more complex or diverse datasets"}
{"paper_id": "fjf3YenThE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The authors provide a novel insight into variance reduction and propose a generalized variance reduced ZO hard-thresholding algorithm, which eliminates the restrictions on the number of random directions and leads to improved convergence rates and broader applicability.\\n2. The theoretical results and analysis are well-presented and demonstrate the effectiveness of the proposed algorithm.\\n3. The utility of the method is illustrated on a portfolio optimization problem and black-box adversarial attacks.\",\n  \"Weaknesses\": \"1. The problem they are tackling is specific to certain scenarios where the true gradient of the objective function is difficult to access.\\n2. The proposed algorithm may not be applicable to all types of $\\ell_0$ constrained optimization problems.\",\n  \"Questions\": \"1. How does the proposed algorithm compare to other existing methods in terms of convergence rates and applicability?\\n2. Are there any potential challenges or limitations in applying the algorithm to more complex problems?\"\n}"}
{"paper_id": "H9DYMIpz9c", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents an innovative approach to data distillation for auto-regressive machine learning tasks, which is well-justified and backed by theoretical analysis and empirical results.\\n2. The use of Hessian-Vector Products and latent-space factorization is mathematically sound and effective, leading to efficient memory usage and better model performance.\\n3. The paper showcases impressive results on sequential recommendation and language modeling tasks, with significant reduction in dataset size.\\n4. The discussion on the implications of this work for future large auto-regressive models is well-reasoned and insightful.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed explanation of the latent-space factorization and its connection to implicit regularization.\\n2. Some more discussion on the potential limitations and challenges of extending Farzi to other tasks or domains would be valuable.\\n3. The comparison to other data distillation methods is limited, and more comprehensive evaluation against state-of-the-art methods would strengthen the paper.\",\n  \"Questions\": \"1. How"}
{"paper_id": "4kLVvIh8cp", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel algorithm, PNLSVI, for offline RL with non-linear function approximation, achieving minimax optimal instance-dependent regret.\\n2. The algorithmic design is innovative, comprising three components: variance-based weighted regression, variance estimation, and a pessimistic value iteration approach.\\n3. The paper provides a comprehensive analysis of the algorithm's performance, including regret bounds and a comparison with previous works.\\n4. The extension of instance-dependent results to a more general framework is a significant contribution to the field.\",\n  \"Weaknesses\": \"1. The paper assumes the function class is known and fixed, which might be a limitation in practice.\\n2. The algorithm's performance may degrade if the variance estimation subroutine is not accurate.\\n3. The paper could benefit from more experimental results to demonstrate the robustness of PNLSVI in various scenarios.\",\n  \"Questions\": \"1. How does PNLSVI handle cases where the function class is unknown or changes over time?\\n2. Can the algorithm be extended to"}
{"paper_id": "CSpWgKo0ID", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper explores an innovative application of behavioral game theory to study the cooperation and coordination behavior of Large Language Models (LLMs).\\n2. The results are well-organized and clearly presented, providing a comprehensive understanding of LLMs' behavior in different game scenarios.\\n3. The use of robustness checks to verify the stability of the findings adds to the paper's strength.\",\n  \"Weaknesses\": \"1. The paper focuses primarily on GPT-4 and does not explore the behavior of other LLMs, which may limit the generalizability of the findings.\\n2. The paper's results are based on a limited set of games, which may not be representative of all possible game scenarios.\\n3. The paper could benefit from a more in-depth discussion of the implications of the findings for the development of more socially aware LLMs.\",\n  \"Questions\": \"1. How do the results of this paper relate to the development of more socially aware LLMs, and what are the potential applications of these"}
{"paper_id": "gwbQ2YwLhD", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a clear and well-structured explanation of the problem of scale affecting performance in structure learning algorithms.\\n2. The theoretical findings are well-supported by extensive experiments on synthetic and real-world data.\\n3. The paper contributes to the understanding of the impact of scale on structure learning algorithms.\",\n  \"Weaknesses\": \"1. The paper focuses primarily on the theoretical aspects, and the experiments seem to be more of a proof-of-concept rather than a comprehensive evaluation.\\n2. The implications of the findings for real-world applications are not fully explored.\\n3. The paper assumes a certain level of background knowledge in structure learning and graph theory.\",\n  \"Questions\": \"1. How do the findings of this paper generalize to more complex structures and larger datasets?\\n2. Are there any practical recommendations for practitioners to mitigate the effects of scale on structure learning algorithms?\\n3. Can the authors elaborate on the potential applications of these findings in fields such as medicine and biology?\"\n}"}
{"paper_id": "eIYDKNqXuV", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed algorithm, Village-Net Clustering, effectively addresses the challenging problem of clustering complex manifold data. The two-phase approach, combining K-Means and WLCF, is novel and well-motivated.\\n2. The algorithm's ability to autonomously determine the optimal number of clusters is a significant advantage, making it well-suited for large-scale datasets.\\n3. The extensive benchmarking on real datasets with known ground-truth labels showcases the algorithm's competitive performance, particularly in terms of the NMI score.\",\n  \"Weaknesses\": \"1. While the algorithm demonstrates impressive performance, it is unclear how it compares to other state-of-the-art methods in terms of computational complexity and scalability.\\n2. The use of WLCF for community detection in the weighted network may not be the most efficient approach, potentially limiting the algorithm's scalability.\\n3. The paper could benefit from more detailed explanations of the algorithm's mathematical derivations and theoretical guarantees.\",\n  \"Questions\": \"1. How does the algorithm handle"}
{"paper_id": "bAMPOUF227", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a critical challenge in large language models, improving their generalizability and factuality. \\n2. The proposed framework is simple yet effective, enhancing the reliability of LLMs. \\n3. The empirical analysis provides a comprehensive understanding of the advantages of incorporating discriminative models into LLMs.\\n4. The paper offers a suite of resources, including datasets, prompts, and model checkpoints.\",\n  \"Weaknesses\": \"1. The novelty of the proposed framework is not entirely clear, as it builds upon existing work in in-context learning and task-specific fine-tuning. \\n2. The paper could benefit from a more in-depth analysis of the trade-offs between generalizability and factuality. \\n3. The reliance on specific models (LLama 2 and ChatGPT) may limit the generalizability of the results.\",\n  \"Questions\": \"1. How does the proposed framework compare to other methods for improving LLMs' generalizability and factuality?\\n2. Can"}
{"paper_id": "eNoiRal5xi", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel objective for domain generalization that explores the data space, which is a good direction to prevent overfitting to a specific domain.\\n2. Theoretical validation of the proposed method is provided, which adds strength to the paper.\\n3. The empirical results show that UDIM consistently outperforms SAM variants across multiple DG benchmark datasets.\\n4. The paper highlights the generalization capability of UDIM in unseen domains, which is a significant contribution.\",\n  \"Weaknesses\": \"1. The paper assumes that the unknown domains can be empirically crafted by perturbing instances from the source domain dataset, which might not be realistic in real-world scenarios.\\n2. The experimental results are based on a limited number of DG benchmark datasets, which might not fully demonstrate the generalizability of UDIM.\\n3. The paper does not provide a clear explanation of how to choose the perturbation method for crafting unknown domains.\",\n  \"Questions\": \"1. How does the perturbation method used to craft unknown"}
{"paper_id": "uU0Adp7Sfo", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a new framework, CCGAN, that addresses the limitations of traditional GANs in multi-modal generation and image super resolution.\\n2. The authors provide a clear explanation of the need for a collaborative-competitive approach and the benefits of using regularization terms as divergence.\\n3. The experimental results demonstrate the effectiveness of CCGAN in generating high-quality samples across various datasets.\\n4. The paper provides a comprehensive comparison with baseline models and showcases the advantages of CCGAN.\",\n  \"Weaknesses\": \"1. The paper's introduction of CCGAN as a solution to the limitations of traditional GANs may be seen as incremental, rather than groundbreaking, given the existing literature on multi-modal generation and image super resolution.\\n2. The regularization term as divergence is a useful concept, but its theoretical justification could be more thoroughly explored.\\n3. The training collapse issue is a common problem in GANs, and while CCGAN appears to mitigate it, more discussion on the limitations and potential failures of"}
{"paper_id": "dKju7tbe6D", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel and effective framework for iterative and parallel reasoning, combining the benefits of both approaches. The idea of using a single architecture to handle both sequential and parallel operations is innovative and promising.\\n2. The experimental results are comprehensive and demonstrate strong performance on various benchmarks, including state-of-the-art results on the CLEVR-Humans dataset.\\n3. The visualization of computation across reasoning steps enhances interpretability and diagnosis of the model's reasoning and outputs.\",\n  \"Weaknesses\": \"1. The paper assumes that the input queries are well-formed and do not require additional processing, which might limit its applicability in real-world scenarios.\\n2. The proposed method is not explicitly designed to handle cases where the input queries are ambiguous or require multiple possible answers.\\n3. The computational cost of the parallel reasoning component might be high for very large-scale models or datasets.\",\n  \"Questions\": \"1. Can the authors provide more insights into how the proposed method handles ambiguity in the input queries?\\n2. How does"}
{"paper_id": "DL7JWbdGr3", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors have developed a novel approach to pre-training epidemic time-series models by leveraging multiple datasets from different diseases, which is a significant contribution to the field. This work has the potential to improve the accuracy and reliability of predictions for future epidemics.\\n2. The paper is well-organized, and the authors have done a good job in explaining the motivation and the challenges of pre-training for epidemic time-series data.\\n3. The experimental results show that the proposed method outperforms previous state-of-the-art methods in various downstream time-series tasks.\",\n  \"Weaknesses\": \"1. The authors assume that the diverse datasets from different diseases can be used as a proxy for the underlying dynamics of a particular disease, which may not always be the case.\\n2. The method may not be suitable for diseases with very different dynamics, such as those with non-seasonal patterns.\\n3. The authors do not provide a clear explanation of how they handled the heterogeneity in the datasets.\",\n  \"Questions\": \"1. How do the"}
{"paper_id": "1PaDPHDhwe", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a simple and effective class-specific scaling strategy to control the trade-off between robust accuracy and average accuracy.\\n2. The proposed instance-wise adaptive scaling technique further improves the performance in terms of both accuracies.\\n3. The unified metric that summarizes the trade-off between the two accuracies as a scalar value is a valuable contribution to the field.\\n4. The experiments on both computer vision and natural language processing domains verify the effectiveness of the proposed frameworks.\\n5. The paper provides meaningful insights into existing robust methods beyond the robust accuracy only.\",\n  \"Weaknesses\": \"1. The paper assumes that the existing debiasing algorithms are available and can be applied with the proposed scaling techniques, which might not be the case in all scenarios.\\n2. The proposed metric might not capture all the complexities of the trade-off between robust accuracy and average accuracy.\\n3. The experiments are limited to two domains, and it would be beneficial to see the performance of the proposed frameworks on other domains as well.\",\n  \"Questions"}
{"paper_id": "iI7hZSczxE", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses a relevant and timely problem in time series analysis, focusing on disentangled representation learning for home appliance electricity usage.\\n2. The proposed method, DIOSC, is well-motivated and effectively uses contrastive learning and self-attention to address attribute correlations.\\n3. The introduction of TDS as a metric for evaluating disentanglement quality is a valuable contribution.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The problem of disentangled representation learning for time series is well-known, but the specific application to home appliance electricity usage might not be novel.\\n2. The use of contrastive learning and self-attention is not entirely new, and the combination might not be entirely novel.\\n3. The paper could benefit from more extensive experiments and comparisons with other methods, especially those that handle attribute correlations.\\n4. The evaluation of TDS might be biased towards the proposed method, as it is designed to gauge disentanglement quality.\","}
{"paper_id": "qrGjFJVl3m", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel and comprehensive vision-language model, Qwen-VL, that outperforms existing models on various benchmarks. This is a significant contribution to the field.\\n2. The paper provides a clear and detailed explanation of the model's architecture and training pipeline, making it easy to understand and replicate.\\n3. The results are impressive, with Qwen-VL setting new records on a range of visual-centric benchmarks.\",\n  \"Weaknesses\": \"1. The paper assumes a high level of background knowledge in the field, which may make it challenging for non-experts to follow. Some sections, such as the 3-stage training pipeline, could benefit from more explanation.\\n2. The evaluation metrics used are not explicitly stated, which makes it difficult to fully understand the extent of the improvements.\",\n  \"Questions\": \"1. How does the visual receptor component of Qwen-VL compare to other visual representation learning methods?\\n2. Are there any plans to extend the Qwen-VL series to other modalities"}
{"paper_id": "qup9xD8mW4", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors formalize a new setting for distillation in RL, called behaviour distillation, which is a significant contribution to the field. They also introduce a method, HaDES, that can discover and condense the information required for training an expert policy into a synthetic dataset of state-action pairs, which is a major achievement. \\n2. The method is well-designed and the results are impressive, with competitive performance levels achieved using only four state-action pairs. The generalizability of the method is also demonstrated through its application to a wide range of architectures and hyperparameters. \\n3. The authors provide a clear and well-organized presentation of their work, making it easy to follow and understand.\",\n  \"Weaknesses\": \"1. The novelty of HaDES is not entirely clear, as the method is built on top of existing techniques in distillation and RL. However, the authors do provide a clear and well-motivated explanation of their approach. \\n2. The experiments could be more comprehensive, with more detailed analysis"}
{"paper_id": "Zw8YxUWL4R", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The Extended Textual Conditioning space (P+) and Extended Textual Inversion (XTI) methods proposed in this paper significantly improve the control over the synthesis process and the expressiveness of text-to-image models.\\n2. The paper showcases the effectiveness of XTI for personalizing text-to-image models and achieving previously unattainable results in object-style mixing.\\n3. The evaluation is comprehensive and systematic, with a series of extensive experiments conducted to analyze and understand the properties of the new space.\\n4. The writing is clear and well-organized, making the paper easy to follow.\\n5. The introduction of the P+ space is well-motivated and well-explained.\",\n  \"Weaknesses\": \"1. The paper assumes a strong background in diffusion models and text-to-image synthesis, which might make it less accessible to readers without prior knowledge in the field.\\n2. The experiments could be more comprehensive if they included a comparison with other state-of-the-art methods in the field.\\n3. The"}
{"paper_id": "3NXhwkZGjz", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The approach of considering multiple prediction hypotheses and consolidating their rationales is novel and effective.\\n2. The three-step adaptation process (model pre-adaptation, hypothesis consolidation, and semi-supervised learning) is well-designed and well-explained.\\n3. The experimental results demonstrate state-of-the-art performance in the SFUDA task and provide evidence of the approach's effectiveness.\",\n  \"Weaknesses\": \"1. The paper assumes access to multiple prediction hypotheses for each sample, which may not be feasible in practice.\\n2. The approach relies heavily on the quality of the pseudo-labeled set generated from hypothesis consolidation.\",\n  \"Questions\": \"1. How does the approach handle cases where the multiple prediction hypotheses are conflicting or inconsistent?\\n2. Can the approach be extended to other types of domain adaptation tasks beyond SFUDA?\"\n}"}
{"paper_id": "Fn655mJ4bv", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper presents an interesting approach to explain the behavior of structured output models by considering correlations among output variables.\\n2. The energy-based training process for the interpreter function is a good idea to incorporate structural information.\\n3. The paper provides a clear explanation of the method and its application to various simulated and real datasets.\",\n  \"Weaknesses\": \"1. The paper assumes that the structured model is available as a black-box, which might limit its applicability in real-world scenarios.\\n2. The evaluation of the proposed method is limited to a few datasets and metrics, which might not be representative of all possible applications.\\n3. The paper could benefit from a more in-depth analysis of the relationships between the input variables and the output variables.\",\n  \"Questions\": \"1. How does the proposed method perform when the structured model is not available as a black-box, but rather as a white-box or a partially transparent model?\\n2. Can the energy-based training process be adapted to other types of structured output models, such as"}
{"paper_id": "VyMW4YZfw7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a refreshing and innovative perspective on spectral GNN approaches for semi-supervised node classification, challenging the current trend of over-engineering GNN architectures. The authors provide a clear and concise argument for the use of traditional nonparametric estimation methods in the spectral domain.\\n2. The ablative study on hyperparameters associated with GNN spectral filtering techniques is well-conducted and provides valuable insights into the effect of these parameters on performance.\\n3. The paper effectively highlights the potential issue of shifting evaluation conventions in GNN approaches.\\n4. The writing is clear, concise, and well-organized.\",\n  \"Weaknesses\": \"1. The paper does not provide a detailed comparison with the current state-of-the-art GNN approaches, which would strengthen the argument for the use of traditional methods.\\n2. The authors claim that recent performance improvements in GNN approaches may be partially attributed to shifts in evaluation conventions, but do not provide concrete evidence to support this claim.\\n3. The paper could benefit from more discussion on"}
{"paper_id": "09xFexjhqE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper identifies a critical issue with the existing Robust Fine-Tuning (RFT) approach and proposes a novel solution to disentangle the optimization process, leading to improved adversarial robustness.\\n2. The proposed AutoLoRa framework is well-designed and implemented, and the experimental results demonstrate its effectiveness across various downstream tasks.\\n3. The paper is well-written, clear, and concise, making it easy to follow and understand.\\n4. The authors provide a comprehensive evaluation and comparison with existing methods, showcasing the strength of their approach.\\n5. The availability of the source code on GitHub facilitates reproducibility and further research.\",\n  \"Weaknesses\": \"1. The paper relies heavily on experimental results to demonstrate the effectiveness of AutoLoRa, and more theoretical analysis would strengthen the argument.\\n2. The evaluation metrics used are not thoroughly explained, and more details about the choice of metrics would be helpful.\\n3. The comparison with existing methods could be more comprehensive, including more recent works in the field.\","}
{"paper_id": "nmBjBZoySX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper introduces a novel and adaptive framework for identifying graph lottery tickets, which addresses the limitations of existing GLT algorithms and provides a dynamic and automated workflow.\\n2. The proposed method is theoretically sound, and the authors provide rigorous proofs to guarantee the mitigation of over-smoothing issues and improved sparse structures in deep GNN scenarios.\\n3. The paper conducts extensive experiments on multiple graph datasets, demonstrating the effectiveness and scalability of AdaGLT.\\n4. The paper has a clear and well-organized structure, making it easy to follow and understand.\",\n  \"Weaknesses\": \"1. The novelty of the proposed method is somewhat limited, as it builds upon existing ideas in the GLT framework and does not introduce a fundamentally new concept.\\n2. The paper assumes that the reader is familiar with the GLT hypothesis and its applications, which might make it challenging for non-experts to follow the discussion.\\n3. The authors could provide more insights into the theoretical guarantees of the AdaGLT framework and its implications for future research"}
{"paper_id": "di52zR8xgf", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors provide a clear and concise description of their new model, Stable Diffusion XL (SDXL), its architecture, and its improvements over previous versions. The text is well-structured and easy to follow.\\n2. The authors demonstrate that SDXL achieves results competitive with state-of-the-art image generators such as Midjourney, showcasing its effectiveness.\\n3. The paper introduces novel conditioning schemes and a refinement model to improve visual fidelity, which is a significant contribution.\",\n  \"Weaknesses\": \"1. The paper lacks a clear explanation of the limitations of the model, such as its computational requirements and potential biases.\\n2. The authors do not provide a thorough comparison with other state-of-the-art models in terms of computational resources and training time.\\n3. The paper does not discuss potential applications of SDXL beyond image generation.\",\n  \"Questions\": \"1. How does SDXL perform on tasks other than text-to-image synthesis, such as image manipulation or image-to-image translation?\\n2. Can the authors provide more"}
{"paper_id": "0b328CMwn1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel concept, activation prompt (AP), which extends the scope of input-level visual prompting (VP) by enabling universal perturbations to be applied to activation maps within intermediate layers of the model.\\n2. The authors provide a thorough performance analysis of AP, comparing it with VP and parameter-efficient finetuning (PEFT) baselines across 29 datasets and various model architectures.\\n3. The paper provides new insights into the incapabilities of VP and the capabilities of AP, and the experimental results demonstrate that AP significantly surpasses input-level VP in terms of both accuracy and efficiency.\\n4. The paper's writing is clear and well-organized, making it easy to follow the arguments and results.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with the concept of visual prompting and its limitations, which might make it difficult for non-experts to understand the contribution of the paper.\\n2. The paper could benefit from a more detailed discussion on the theoretical implications of AP and"}
{"paper_id": "AMDKqZcZbi", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a comprehensive and well-designed experiment to address the challenges of rapid adaptation and continual learning in machine learning models, drawing inspiration from biological systems.\\n2. The proposed architecture and memory mechanism for the sequential Morris Water Maze task demonstrate good generalization and rapid learning.\\n3. The comparison with ANN baselines in both continual and few-shot learning contexts is thorough and highlights the strengths of the proposed model.\\n4. The paper's discussion of the limitations of current ML models and the potential for biological systems to inform AI development is thought-provoking.\",\n  \"Weaknesses\": \"1. The paper assumes a specific neural architecture and memory mechanism, which may not be universally applicable to other tasks or domains.\\n2. The reliance on grid cells and entorhinal-hippocampal circuitry may limit the generalizability of the proposed approach to other tasks.\\n3. The experimental setup and evaluation metrics may not be sufficient to fully capture the complexities of rapid adaptation and continual learning.\",\n  \"Questions\": \""}
{"paper_id": "am7BPV3Cwo", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a solid theoretical framework for understanding the OOD detection problem, particularly the class-aware bias item, which is a significant contribution. This framework can be applied to other problems as well.\\n2. The proposed method, ImOOD, is well-justified and effective in improving the OOD detection performance.\\n3. The experimental results demonstrate the superiority of the proposed method on various benchmarks.\\n4. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"1. The paper relies heavily on the CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks, which might not be representative of real-world scenarios.\\n2. The proposed method is mainly evaluated on imbalanced data distribution, but it's unclear how it performs on other types of data distributions.\\n3. The paper does not provide a detailed discussion on the limitations of the proposed method and potential avenues for future work.\",\n  \"Questions\": \"1. How does the class-aware bias item relate to other"}
{"paper_id": "R1crLHQ4kf", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed adversarial example detection strategy is simple and efficient, and can be applied to any ASR system that predicts a probability distribution over output tokens in each time step.\\n2. The paper demonstrates the supreme performance of this approach, achieving a mean AUROC of over 99% for distinguishing adversarial examples against clean data and 98% against noisy data.\\n3. The authors assess the robustness of their method by proposing adaptive attacks that are constructed with awareness of the defense mechanism in place.\\n4. The paper provides an extensive analysis of different state-of-the-art ASR systems and language data sets.\",\n  \"Weaknesses\": \"1. The paper assumes that the ASR system predicts a probability distribution over output tokens in each time step, which might not be the case for all ASR systems.\\n2. The authors do not provide a detailed analysis of the computational complexity of their approach.\",\n  \"Questions\": \"1. How does the proposed method handle the case where the ASR system does not predict a probability"}
{"paper_id": "i7P2mK3x3o", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The work develops a novel flow-based model that transports from an arbitrary data distribution P to an arbitrary data distribution Q, which is a significant contribution to the field of generative tasks. \\n2. The proposed model is trained to find an invertible transport map between P and Q optimally by minimizing the transport cost, which is a well-defined and well-motivated objective. \\n3. The effectiveness of the proposed model is empirically demonstrated in multiple tasks, including mutual information estimation, energy-based generative models, and image-to-image translation.\",\n  \"Weaknesses\": \"1. The paper assumes that the data distributions P and Q are only accessible via finite samples, which might not be realistic in many real-world scenarios. \\n2. The model's ability to generalize to new distributions beyond those seen during training is not thoroughly evaluated. \\n3. The paper does not provide a clear comparison with other state-of-the-art methods for generative tasks.\",\n  \"Questions\": \"1. How does the proposed model handle the"}
{"paper_id": "YIls9HEa52", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed irSLDS model addresses a significant limitation of the rSLDS model by incorporating a semi-Markov discrete state process with latent geometry, making it more flexible and capable of capturing key properties of stochastic processes over partitions.\\n2. The use of partial differential equations (PDE) theory to derive an efficient, semi-parametric formulation for dynamical sufficient statistics is innovative and effective.\\n3. The paper presents thorough validation and demonstration of the model's capabilities on synthetic data and real-world electrophysiological data from mice.\",\n  \"Weaknesses\": \"1. The paper could benefit from more detailed explanations of the theoretical background and mathematical derivations, particularly for readers without a strong background in PDE theory.\\n2. The analysis of mice electrophysiological data is limited to a single experiment and dataset, and it would be beneficial to see more extensive evaluation on various datasets and experimental settings.\\n3. While the irSLDS model shows promise, its practical deployment and scalability to more complex systems or larger datasets are"}
{"paper_id": "koYsgfEwCQ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a novel and effective approach to unsupervised learning of object-centric representations in dynamic visual scenes, leveraging 3D scene generative models and differentiable volume rendering. This is a significant contribution to the field, especially in the context of 3D scene understanding.\\n2. The proposed DynaVol model is well-motivated and demonstrates impressive performance on several benchmarks, outperforming existing approaches for unsupervised dynamic scene decomposition.\\n3. The paper is well-organized, clearly presenting the key ideas and architecture of DynaVol, and the experimental results are thorough and easy to follow.\",\n  \"Weaknesses\": \"1. While the paper presents a novel approach, some of the components, such as the slot attention mechanism, seem to be borrowed from existing literature, which might not be entirely novel.\\n2. The paper does not provide a detailed analysis of the computational cost and scalability of the proposed approach, which might be a concern for real-world applications.\\n3. The experiments are limited to a"}
{"paper_id": "o6AN2ZoNw2", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed framework, P-Seg, is conceptually simple and achieves state-of-the-art results on three widely tested benchmarks.\\n2. The method leverages pseudo-mask and language to train a MaskFormer, which is a novel approach for open-vocabulary semantic segmentation.\\n3. The paper is well-organized and clearly presented, with a concise abstract and introduction.\\n4. The authors provide a clear description of their method and its benefits, including scalability and self-training.\",\n  \"Weaknesses\": \"1. The paper lacks a detailed discussion of the limitations of their approach and potential avenues for future research.\\n2. The authors do not provide a thorough comparison with other state-of-the-art methods in the field.\\n3. The paper could benefit from a more in-depth analysis of the pseudo-mask and language components.\",\n  \"Questions\": \"1. How does the pseudo-mask component of P-Seg contribute to the overall performance of the method?\\n2. Can the authors provide more information on how they select the open-v"}
{"paper_id": "HXZK1Z8tHa", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant problem in transformer-based models, which is the trade-off between latency and trainability. The proposed solution is novel and well-motivated.\\n2. The experimental results are comprehensive and demonstrate the effectiveness of the proposed method.\\n3. The code and pre-trained models will be open-sourced, which is a big plus for reproducibility.\\n4. The addition of residual connections to the 'Value' of self-attention is a clever idea that improves the model's trainability.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with the basics of transformer-based models and their limitations. Some background information would be helpful for non-experts.\\n2. The paper focuses on image restoration tasks, but it is not clear how the proposed method would generalize to other tasks.\\n3. The impact of the proposed method on other transformer-based models is not discussed.\",\n  \"Questions\": \"1. How does the proposed method compare to other methods that aim to reduce latency in"}
{"paper_id": "QO3yH7X8JJ", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed method, Diff-SR, is novel and effective in achieving arbitrary-scale super-resolution (ASSR) without requiring distillation or fine-tuning. The use of pre-trained diffusion-based generative models (DGMs) is a clever approach to improve SR tasks.\\n2. The Perceptual Recoverable Field (PRF) metric is a well-designed method for determining the optimal trade-off between low-level fidelity and high-level signature in the diffusion process.\\n3. The paper provides extensive experiments to demonstrate the effectiveness, flexibility, and adaptability of Diff-SR under diverse ASSR environments.\",\n  \"Weaknesses\": \"1. The novelty of the idea is somewhat diminished by the fact that the authors acknowledge the existence of other methods that can also achieve ASSR, albeit with additional training efforts.\\n2. The analysis of the PRF metric is limited to a theoretical framework, which might not fully capture the complexities of real-world scenarios.\\n3. The experiments seem to focus mainly on image quality metrics, and it"}
{"paper_id": "sNtDKdcI1f", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper presents a clear and concise analysis of the relationship between reward and length in RLHF, providing valuable insights into the mechanisms behind RLHF's reported improvements.\\n2. The experiments are well-designed and thorough, exploring interventions to mitigate length increases and demonstrating the effectiveness of reward models in certain settings.\\n3. The paper raises interesting questions about the role of reward models in RLHF and the potential for optimizing for response length to drive improvements.\\n4. The writing is clear and well-organized.\",\n  \"Weaknesses\": \"1. The paper focuses primarily on a specific setting (open-source preference datasets) and may not generalize to other domains or settings.\\n2. The interventions to mitigate length increases are not uniformly effective across all settings, which raises questions about the robustness of the approach.\\n3. The paper does not explore the broader implications of optimizing for response length in RLHF, leaving some potential avenues for future work unexplored.\",\n  \"Questions\": \"1. How might the findings of this paper generalize to"}
{"paper_id": "HFtrXBfNru", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The authors provide a clear and concise introduction to the problem of representation distortion in GNNs and establish a lower bound to prove that distortion inevitably occurs over time.\\n2. The proposed method, Smart, is a well-designed and effective baseline that uses an adaptive feature extractor for graph reconstruction.\\n3. The experimental results on both synthetic and real-world datasets demonstrate the effectiveness of Smart in estimating temporal distortion.\",\n  \"Weaknesses\": \"1. The theoretical contribution is limited to a specific type of graph evolution, and it is not clear if the results generalize to more complex scenarios.\\n2. The empirical evaluation is mainly based on synthetic data and a limited number of real-world graphs, which may not be representative of all possible cases.\",\n  \"Questions\": \"1. Can the authors provide more insights into the reasons why the naive approach of pre-training a recurrent model fails to estimate temporal distortion accurately?\\n2. How does the proposed method, Smart, perform on more complex and dynamic graphs, such as those with multiple evolving subgraphs or"}
{"paper_id": "4u0ruVk749", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses an important problem in the field of causal inference by proposing a novel method to capture unobserved latent space using diffusion model.\\n2. The approach is well-motivated and the derivation of the variational bound in closed form is technically sound.\\n3. The empirical results demonstrate consistent improvements over state-of-the-art methods on both synthetic and benchmark datasets.\",\n  \"Weaknesses\": \"1. The paper assumes that the apriori knowledge is available, which might not be the case in real-world scenarios.\\n2. The impact of the unobserved confounders on the model's performance is not thoroughly explored.\\n3. The paper lacks a clear discussion on the practical implications of the proposed method and its limitations.\",\n  \"Questions\": \"1. How does the proposed method handle scenarios where the apriori knowledge is not available or incomplete?\\n2. Can the model be extended to handle multiple unobserved confounders?\\n3. What are the potential applications of the proposed method in real"}
{"paper_id": "Pa6SiS66p0", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper explores an important and understudied area in CL, namely, the role of multiple modalities in mitigating forgetting.\\n2. The introduction of a benchmark for multi-modal continual learning is a valuable contribution to the field.\\n3. The method for integrating and aligning information from different modalities is well-explained and effectively implemented.\\n4. The paper's findings and results are comprehensive and well-presented.\\n5. The conclusion ties together the various threads of the paper effectively.\",\n  \"Weaknesses\": \"1. The paper assumes that the modalities are pre-aligned and available, which may not be the case in many real-world scenarios.\\n2. The method for aligning the information from different modalities may not be suitable for all types of data or modalities.\\n3. The paper does not explore the potential limitations and challenges of using multiple modalities in CL.\\n4. The evaluation of the method is limited to a single dataset and task.\",\n  \"Questions\": \"1. How"}
{"paper_id": "rkplYfqUr0", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel and effective generative prompting framework (Gen-Z) for zero-shot text classification, addressing miscalibration and brittleness to slight prompt variations.\\n2. The framework is well-designed, with a clear and concise presentation of the approach, and a systematic evaluation on various standard classification benchmarks.\\n3. The paper shows consistent improvement over zero-shot and few-shot baselines, and demonstrates robustness to prompt variations.\\n4. The author information and subject information incorporation in the label descriptions is a nice addition for personalizing classification.\",\n  \"Weaknesses\": \"1. The paper relies heavily on the performance of the underlying language model, which might limit its applicability to other models.\\n2. The framework might not be suitable for tasks with complex label structures or multi-label classification.\\n3. The paper does not provide a clear comparison with other existing generative prompting approaches.\",\n  \"Questions\": \"1. How does Gen-Z handle out-of-vocabulary words or words with multiple possible meanings?\\n2."}
{"paper_id": "pTU2X9mUBe", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a much-needed contribution to the research community by providing a large-scale, comprehensive, and diverse last-mile delivery dataset, LaDe.\\n2. The dataset has the potential to inspire new research in logistics, supply chain management, and spatio-temporal data mining.\\n3. The authors verify LaDe on three tasks using classical baseline models, providing a solid foundation for future research.\",\n  \"Weaknesses\": \"1. The paper is primarily focused on introducing the LaDe dataset, with limited discussion on the algorithmic contributions or innovations.\\n2. The evaluation of the dataset is limited to three tasks using classical baseline models, which might not showcase the full potential of the dataset.\\n3. The dataset is not compared to other existing datasets in the field, which could provide a more comprehensive understanding of its value.\",\n  \"Questions\": \"1. How does the authors plan to maintain and update the LaDe dataset in the future?\\n2. Can the authors provide more details on the data preprocessing steps and the quality"}
{"paper_id": "uhtQyRrTzY", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel dataset-curation approach that does not require any additional annotations, making it scalable and practical for real-world applications.\\n2. The experiments demonstrate significant performance improvements over existing methods on various dense geometric tasks, showcasing the effectiveness of the proposed approach.\\n3. The paper provides a clear and concise presentation of the method and its results, making it easy to follow and understand.\\n4. The authors' ability to generate large-scale multi-view datasets from real-world videos and simulated environments is impressive and has the potential to benefit the field.\",\n  \"Weaknesses\": \"1. The paper assumes that the availability of large-scale multi-view datasets is the primary bottleneck for dense pixel-specific representation learning, which might not be the case in all scenarios.\\n2. The reliance on simulated environments for generating additional data might limit the applicability of the proposed approach in real-world scenarios.\\n3. The evaluation is mainly focused on dense geometric tasks, and it is unclear whether the proposed approach would perform well on other types of tasks.\","}
{"paper_id": "WZfatbNdLV", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a comprehensive framework for predicting and designing RNA splicing condition-specific outcomes using generative models. The use of multiple transformers and side information is an interesting approach.\\n2. The TrASPr model significantly outperforms recent published models, indicating a strong contribution to the field.\\n3. The Bayesian Optimization algorithm is effectively used to design RNA splicing outcomes.\\n4. The paper has a clear and well-organized structure, making it easy to follow.\\n5. The writing is clear and concise.\\n6. The use of a custom loss function for RNA splicing outcome design is a good approach.\\n7. The TrASPr model can identify relevant regulatory features, which is a valuable contribution to the field.\",\n  \"Weaknesses\": \"1. The paper assumes that the generative model can capture all relevant regulatory features, which might not always be the case.\\n2. The paper does not provide a clear comparison with other existing methods that use different architectures or approaches for RNA splicing prediction.\\n"}
{"paper_id": "YkRwadXWHd", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a multi-modal framework that effectively integrates video, keypoints, and optical flow modalities to improve CSLR performance.\\n2. The hierarchical knowledge distillation (HKD) framework is an innovative approach to alleviate computational burden and transfer knowledge from multiple modalities.\\n3. The extensive experiments on three benchmark datasets demonstrate state-of-the-art performance in CSLR.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper assumes that keypoints and optical flow information are readily available, which may not be the case in real-world scenarios.\\n2. The computational cost of extracting keypoints and optical flow information may still be high despite the use of HKD.\\n3. The paper does not provide a thorough analysis of the trade-offs between the different fusion techniques and the HKD framework.\",\n  \"Questions\": \"1. How does the proposed framework perform on datasets with varying video quality or lighting conditions?\\n2. Can the HKD framework be applied to other multimodal tasks beyond"}
{"paper_id": "q4Bim1dDzb", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed Unified Voxelization framework, UniVoxel, is a well-designed and efficient method for inverse rendering, which significantly accelerates the optimization process.\\n2. The use of Spherical Gaussians to represent incident light radiance is a novel and effective approach.\\n3. The paper presents thorough experiments on multiple benchmarks, demonstrating the effectiveness and efficiency of UniVoxel.\",\n  \"Weaknesses\": \"\",\n  \"Questions\": \"1. While UniVoxel achieves significant speedup in optimization, does it compromise on reconstruction quality or accuracy compared to traditional inverse rendering methods?\\n2. How does the choice of Spherical Gaussians for representing incident light radiance impact the overall performance and complexity of the UniVoxel framework?\"\n}"}
{"paper_id": "39cPKijBed", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel method for mitigating dataset bias in diffusion models, which is a significant problem in the field. The approach is well-motivated and the experimental results are convincing.\\n2. The connection to traditional score-matching is an interesting contribution.\\n3. The code is available, which is a good practice.\",\n  \"Weaknesses\": \"1. The paper assumes a specific type of bias in the dataset, which might not be the case in all scenarios.\\n2. The method is limited to diffusion models, and it is not clear if it can be applied to other types of generative models.\",\n  \"Questions\": \"1. How does the proposed method perform on more complex datasets with multiple types of bias?\\n2. Can the method be extended to other types of generative models, such as autoregressive models?\"\n}"}
{"paper_id": "aG3EARrrd1", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel predictive statistical model, OSRT, for handling streaming multivariate scattered data, which is a timely and important problem in various domains. \\n2. The design of OSRT is innovative, combining online tree decomposition and online adaptive RBF exploration to dynamically expand its network depth and incorporate sparse RBF refinement at each child node. \\n3. The evaluation on several datasets shows that OSRT achieves higher efficiency and accuracy compared to existing online RBF methods.\",\n  \"Weaknesses\": \"1. The paper assumes that the data is scattered and multivariate, but does not provide a clear explanation of how OSRT handles non-scattered or univariate data. \\n2. The evaluation is limited to a few datasets, and it would be beneficial to see more extensive evaluation on a wider range of datasets.\\n3. The paper could benefit from a more detailed explanation of the incremental method used to explore the central node of the RBF function.\",\n  \"Questions\": \"1. How does OSRT handle noisy or"}
{"paper_id": "Ts95eXsPBc", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel and innovative approach to incorporating spatial information into episodic memory models, which could lead to significant advancements in AI.\\n2. The Adaptive Memory Allocator, a reinforcement learning-based memory management method, is a significant contribution to the field.\\n3. The paper provides a clear and well-organized presentation of the research and its results.\\n4. The experiments demonstrate the advantages of the proposed model in various environments and downstream tasks.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed explanation of the spatial context representation and how it is integrated into the Spatially-Aware Transformer model.\\n2. Some of the benefits of the Adaptive Memory Allocator are not clearly explained, and more information on its training procedure would be helpful.\\n3. The paper assumes that the spatial dimension is the only missing aspect of current episodic memory models, but it would be interesting to explore other aspects that could be improved.\",\n  \"Questions\": \"1. How does the spatial context representation affect the performance of the"}
{"paper_id": "LfhG5znxzR", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel and effective approach to making neural networks more interpretable by introducing a vector quantization bottleneck, resulting in sparse, discrete, and more interpretable hidden states. This could lead to breakthroughs in understanding and controlling neural network behavior.\\n2. The experiments on different datasets show the effectiveness of the approach in overcoming the superposition problem and providing a promising unit of analysis and control for neural networks.\\n3. The authors provide open-sourced codebase and models, making it easier for the community to build upon this research.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with the concept of vector quantization and its applications, which might limit its accessibility to non-experts.\\n2. The evaluation of the approach is mainly qualitative, and it would be beneficial to have more quantitative metrics to support the claims made in the paper.\\n3. The paper does not discuss the computational cost of the approach and its potential impact"}
{"paper_id": "bUGzjiUsIq", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The authors address an understudied problem of partially annotated multi-label classification, which is relevant to many real-world tasks.\\n2. The proposed framework, PAPG, is innovative and effective in tackling the challenges of class imbalance and scarcity of positive annotations.\\n3. The paper is well-organized and clearly presented, with a clear introduction to the problem and a concise explanation of the proposed method.\",\n  \"Weaknesses\": \"1. The authors assume that the partially annotated dataset is randomly sampled, which might not be the case in real-world scenarios.\\n2. The paper could benefit from more extensive experimental evaluations on diverse datasets and tasks to demonstrate its generalizability.\\n3. The authors do not provide a clear comparison with existing methods for partially annotated multi-label classification.\",\n  \"Questions\": \"1. How does the proposed framework, PAPG, handle the case where the partially annotated dataset is not randomly sampled?\\n2. Can the authors provide more details on how to select the local and global rewards for the Policy"}
{"paper_id": "uMAujpVi9m", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant challenge in the field by leveraging high-resolution atomic protein structures to generate a large number of protein-ligand complexes.\\n2. The proposed method, ProFSA, achieves state-of-the-art performance across various tasks, including pocket druggability prediction, pocket matching, and ligand binding affinity prediction.\\n3. The paper opens up a new avenue for mitigating the scarcity of protein-ligand complex data through the utilization of high-quality and diverse protein structure databases.\",\n  \"Weaknesses\": \"1. The limited number of protein-ligand complex structures available in the PDB database might be a concern, and it is unclear how the generated complexes will generalize to real-world scenarios.\\n2. The method relies on pretrained small molecule representations, which might not be suitable for all protein-ligand interactions.\\n3. The evaluation of the method is mostly focused on specific tasks, and it is unclear how it performs on other tasks or real-world applications.\",\n  \"Questions\": \"1. How does"}
{"paper_id": "uqxBTcWRnj", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel framework, Transitional Dictionary Learning (TDL), that can implicitly learn symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations.\\n2. The proposed game-theoretic diffusion model and Expectation Maximization algorithm are effective in decomposing the input into visual parts using the dictionaries learned.\\n3. The experiments on three abstract compositional visual object datasets demonstrate the effectiveness of the proposed method in discovering compositional patterns.\\n4. The paper introduces two novel metrics, clustering information gain, and heuristic shape score, to evaluate the model.\\n5. The results show that the proposed method significantly outperforms state-of-the-art unsupervised part segmentation methods that rely on visual features from pre-trained backbones.\\n6. The human evaluation is consistent with the proposed metrics.\",\n  \"Weaknesses\": \"1. The paper assumes the availability of a pre-trained backbone, which may limit its applicability to real-world scenarios where such backbones are not available"}
{"paper_id": "o4CLLlIaaH", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel paradigm for generalizable neural radiance field (NeRF) that addresses three significant issues of previous generic NeRFs, including inconsistent feature matching, distortions, and artifacts in geometric discontinuities. This approach has the potential to improve the field of NeRFs.\\n2. The paper presents a comprehensive solution to the challenges of NeRFs, including a novel nonuniform log sampling strategy and a learnable kernel spatially augmented with features for feature aggregations.\\n3. The paper is well-organized and clearly presented, with a clear explanation of the proposed method and its advantages over previous approaches.\",\n  \"Weaknesses\": \"1. The paper assumes that the geometric priors are accurate, but in real-world scenarios, the accuracy of these priors might be limited. How does the method handle cases where the priors are inaccurate?\\n2. The paper does not provide a clear comparison with other state-of-the-art methods that do not use geometric priors.\\n3. The paper assumes that"}
{"paper_id": "KNzL1nglNB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed Label-encoding Risk Minimization (LRM) is a novel and effective approach to tackle label insufficient scenarios. This work has the potential to significantly impact semi-supervised learning, unsupervised domain adaptation, and semi-supervised heterogeneous domain adaptation.\\n2. The paper is well-written, and the authors provide a clear explanation of the problem, the proposed method, and the experimental results.\\n3. The theoretical analysis and empirical evaluation demonstrate the superiority of LRM over ERM in various scenarios.\\n4. The authors plan to release the code, which is a positive step towards reproducibility.\",\n  \"Weaknesses\": \"1. The paper assumes that the prediction means for unlabeled samples can be used as label encodings. However, this might not be the case in all scenarios, especially when the prediction means are not reliable or accurate.\\n2. The paper does not provide a detailed discussion on how to handle noisy or missing labels in the unlabeled dataset.\\n3. The experimental results show that LRM"}
{"paper_id": "pvhyBB86Bt", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper builds upon previous works and extends the analysis of Jacobian behaviour beyond initialization, providing a theoretical explanation for the crucial role of initialization.\\n2. The use of recent breakthrough results in random matrix theory is well-integrated into the framework.\\n3. The analysis of Jacobian in other scenarios such as sparse Networks and non-iid initialization demonstrates the broad applicability of the framework.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The connection to the real-world applications of the proposed framework is not explicitly mentioned.\\n2. The paper assumes a certain level of background knowledge in random matrix theory, which might make it difficult for non-experts to follow.\\n3. The empirical results are not presented in the abstract, making it hard to assess their significance.\",\n  \"Questions\": \"1. Can the proposed framework be applied to other types of neural networks, such as recurrent neural networks?\\n2. How does the proposed framework compare to other methods for ensuring Jacobian stability"}
{"paper_id": "Mylk5iamJC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel framework for open-set recognition (OSR) based on generative models, which is a significant contribution to the field.\\n2. The formalization of OSR based on learning theory is well-motivated and provides a clear understanding of the problem.\\n3. The proposed neural Latent Gaussian Mixture Model (L-GMM) is well-designed and the collaborative training algorithm is effective.\\n4. The experimental results show that L-GMM outperforms discriminative counterparts in CSR and OSR.\\n5. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The novelty of the approach is somewhat overstated, as the idea of using generative models for OSR is not entirely new.\\n2. The paper does not provide a comprehensive comparison with existing methods for OSR, which makes it difficult to evaluate the effectiveness of L-GMM.\\n3. The authors assume that the density estimator can accurately model the probability densities of unknown classes, which may not"}
{"paper_id": "MsOcVFzv8D", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a solid theoretical foundation for multi-domain text classification, filling a gap in the existing literature.\\n2. The proposed margin discrepancy-based adversarial training (MDAT) approach is well-motivated and demonstrates excellent performance on two MDTC benchmarks.\\n3. The paper is well-organized and clearly presented, with a good balance of theoretical and empirical results.\",\n  \"Weaknesses\": \"1. The MDAT approach is limited to multi-domain text classification and may not be applicable to other tasks or domains.\\n2. The paper assumes the existence of a shared-private paradigm, which might not be suitable for all multi-domain scenarios.\\n3. The authors could have provided more detailed comparisons with other state-of-the-art methods, such as more nuanced discussions of their strengths and weaknesses.\",\n  \"Questions\": \"1. How do the authors plan to extend the MDAT approach to other tasks or domains?\\n2. Can the authors provide more insights on how the shared-private paradigm assumption affects the performance of MDAT?\\n"}
{"paper_id": "fBlHaSGKNg", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed UPA methodology is well-motivated and effectively addresses the challenge of efficient sample selection under budgetary constraints.\\n2. The modified Frank-Wolfe algorithm for minimizing \\u03b1-MMD is a creative solution for selecting representative and diverse samples for annotation.\\n3. The experiments demonstrate the effectiveness of UPA in improving the performance of various SSL methods, surpassing AL and SSAL methods even under constrained annotation budgets.\",\n  \"Weaknesses\": \"1. The evaluation of UPA is primarily based on its performance comparison with existing SSL, AL, and SSAL methods. It would be beneficial to explore more comprehensive evaluations, such as a detailed analysis of the selected samples and their impact on model generalization.\\n2. The paper assumes that the unlabeled data is representative of the target task. In practice, this might not always be the case, and additional techniques to handle non-representative data would be valuable.\",\n  \"Questions\": \"1. How does UPA handle cases where the unlabeled data"}
{"paper_id": "HwQ8NVvdmm", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel technique to enable low-precision training with integer matrix multiplications, which reduces memory footprint and increases compute efficiency.\\n2. The authors demonstrate the effectiveness of their technique on several human activity recognition datasets and CIFAR100 in a class incremental learning setting.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The authors rely on the assumption that Hadamard transforms and tiled matrix multiplication are inexpensive, which might not be the case for all edge platforms.\\n2. The accuracy degradation of 0.5% and 3% might be acceptable for some applications, but it may not be sufficient for others.\\n3. The paper does not provide a detailed comparison with other existing low-precision training techniques.\",\n  \"Questions\": \"1. How does the proposed technique perform on other types of datasets and edge platforms?\\n2. Are there any potential issues with the proposed technique that could lead to accuracy degradation in certain scenarios?\\n3. What are the limitations"}
{"paper_id": "cZo6pDtDZr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents two novel algorithms for estimating and testing collision probability with improved sample complexity.\\n2. The paper provides a thorough analysis of the algorithms and their performance in experiments.\\n3. The paper is well-organized and clearly presented, with a concise abstract and introduction.\\n4. The authors provide a comparison with previous work, showing that their algorithms require significantly fewer samples.\\n5. The paper has a clear and concise conclusion, summarizing the main contributions and implications of the work.\",\n  \"Weaknesses\": \"1. The paper assumes that the collision probability is known or can be estimated, which may not be the case in all scenarios.\\n2. The paper does not provide a thorough analysis of the privacy guarantees of the algorithms.\\n3. The experimental results are limited to a few datasets and may not generalize to other domains.\",\n  \"Questions\": \"1. How do the algorithms perform when the collision probability is unknown or cannot be estimated?\\n2. Are there any other applications of these algorithms beyond estimating collision probability"}
{"paper_id": "F7QnIKlC1N", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The introduction of GTMGC and MSRSA mechanisms is novel and innovative, and the approach to directly predicting the ground-state conformation of molecules is promising.\\n2. The Graph-Transformer (GT) based architecture is well-suited for molecular structure modeling, and the self-attention mechanism is easy to implement and performs well.\\n3. The experimental results on the Molecule3D and QM9 datasets demonstrate the effectiveness of the approach and its superiority over existing methods.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The authors could provide more detailed information about the training process, including the hyperparameters and the training time.\\n2. The comparison with other methods could be more comprehensive, including a discussion of the limitations of the existing methods.\\n3. The paper could benefit from more visualizations or illustrations to help readers understand the molecular structures and the predicted conformations.\",\n  \"Questions\": \"1. How does the MSRSA mechanism handle the complexity of molecular structures,"}
{"paper_id": "8JCn0kmS8W", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed framework WavJourney is well-designed and effectively connects various audio models for audio creation, allowing users to generate harmonious audio with controllable conditions.\\n2. The paper presents a novel approach to audio generation by leveraging LLMs to generate an audio script, which is then converted into a computer program for execution.\\n3. The experimental results demonstrate state-of-the-art performance on text-to-audio generation benchmarks and showcase the potential of WavJourney in crafting engaging storytelling audio content from text.\\n4. The introduction of a new multi-genre story benchmark and the availability of code and synthesized audio foster future research.\",\n  \"Weaknesses\": \"1. The paper assumes that the text description is a good representation of the desired audio, which may not always be the case.\\n2. The framework's dependence on LLMs for generating audio scripts may limit its applicability in scenarios where high-quality text descriptions are not available.\\n3. The paper does not provide a thorough analysis of the computational complexity and efficiency"}
{"paper_id": "XbLffB0T2z", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a new and insightful perspective on the effectiveness of data poisoning attacks, specifically the issue of transferability between different learning paradigms. \\n2. The proposed Transferable Poisoning method is well-structured and easy to understand, and the use of both supervised and unsupervised contrastive learning paradigms is a nice touch.\\n3. The experimental results on benchmark image datasets are thorough and demonstrate the effectiveness of Transferable Poisoning.\\n4. The paper's focus on the importance of transferability in data poisoning attacks is timely and relevant.\",\n  \"Weaknesses\": \"1. The assumption that the victim's learning algorithm is the same as the attacker's is not entirely realistic, as real-world scenarios often involve more complex and nuanced attack-defence dynamics.\\n2. The evaluation of Transferable Poisoning is limited to image datasets, and it is unclear whether the method would work as well on other types of data or tasks.\\n3. The paper could benefit from a more in-depth analysis of the frequency"}
{"paper_id": "4y3GDTFv70", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper presents an innovative idea of categorizing languages as unambiguous or \u03b5-ambiguous and leveraging the sparse joint distribution of languages for emergent abilities of LLMs.\\n2. The paper provides quantitative results to demonstrate the effectiveness of Bayesian inference on the sparse joint distribution for various LLM abilities.\\n3. The paper is well-written and clearly presented.\",\n  \"Weaknesses\": \"1. The idea of categorizing languages as unambiguous or \u03b5-ambiguous might be too simplistic, as languages can have complex and nuanced structures that cannot be reduced to a binary classification.\\n2. The paper assumes that the marginal distribution of languages is known, which might not always be the case in real-world scenarios.\\n3. The paper does not provide a clear explanation of how the Bayesian inference is performed on the sparse joint distribution.\",\n  \"Questions\": \"1. How does the categorization of languages as unambiguous or \u03b5-ambiguous affect the performance of LLMs in different tasks and domains?\\n2. Can the sparse joint distribution"}
{"paper_id": "ODSgo2m8aE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper makes a significant contribution to the field of fairness in GNNs by providing a provable method to constrain the output perturbations induced by input biases.\\n2. The use of Lipschitz bounds is an efficient and effective way to examine the output stability of machine learning models.\\n3. The paper provides a comprehensive analysis of the theoretical and experimental results.\",\n  \"Weaknesses\": \"1. The paper's focus on GNNs might limit its applicability to other types of neural networks.\\n2. The experimental validation of the Lipschitz bound's effectiveness is limited to a specific dataset.\\n3. The paper could benefit from a more detailed discussion of the practical implications of the proposed method.\",\n  \"Questions\": \"1. How does the proposed method compare to existing fairness techniques for GNNs?\\n2. Can the Lipschitz bound be applied to other types of neural networks beyond GNNs?\\n3. What are the potential limitations of the proposed method in terms of computational cost?\"\n}"}
{"paper_id": "RlfD5cE1ep", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a clear extension of the analysis in \\cite{Tian2021ICML} by considering the cosine loss and its impact on the learning dynamics.\\n2. The findings show that feature normalization plays a crucial role in preventing the dynamics collapse.\\n3. The paper is well-organized and easy to follow.\",\n  \"Weaknesses\": \"1. The novelty of the contribution is somewhat limited, as it builds upon existing work and extends the analysis to a different loss function.\\n2. The impact of the findings on the broader field of self-supervised learning is not fully explored.\\n3. The discussion of the implications of the results could be more extensive.\",\n  \"Questions\": \"1. How do the authors envision the application of their findings to other self-supervised learning methods? Can they be used to improve the stability of other methods?\\n2. Are there any potential limitations or challenges in applying the cosine loss in practice?\\n3. How does the sixth-order dynamics induced by the cosine loss compare to the"}
{"paper_id": "18TfucMNTr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a clear and concise overview of the concept of noisy training and its relation to optimization by continuation.\\n2. The authors provide a well-structured approach to regularization in continuation optimization, which is a novel and interesting idea.\\n3. The experimental results demonstrate the effectiveness of the proposed method in reducing the risk of getting stuck in local minima.\\n4. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with the concept of optimization by continuation, which might not be the case for non-experts.\\n2. The regularization approach seems to be somewhat ad-hoc and not thoroughly justified.\\n3. The experimental results are mostly based on synthetic problems, which might not generalize to real-world problems.\",\n  \"Questions\": \"1. How does the proposed regularization approach compare to other regularization techniques in the context of continuation optimization?\\n2. Are there any potential drawbacks to using continuation optimization in real-world problems, given its reliance on the convex"}
{"paper_id": "JGTC6WgO4T", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses a significant gap in the existing research on multi-source Black-Box Domain Adaptation (BDA), and proposes a novel framework called Label Space-Induced Pseudo Label Refinement (LPR).\\n2. The theoretical supports provided for the performance of LPR demonstrate a clear understanding of the problem and its solution.\\n3. The experimental results on four benchmark datasets show competitive performance compared to state-of-the-art approaches.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The novelty of the LPR framework is somewhat limited, as the authors only provide a brief comparison with existing methods and do not discuss their differences in detail.\\n2. The paper assumes that the source domains are available, but does not discuss how to handle scenarios where the source domains are not accessible.\\n3. The experimental results are mostly focused on the performance of the LPR framework, but it would be beneficial to include more analysis on the effectiveness of the Pseudo Label Refinery Network ("}
{"paper_id": "fQHb1uZzl7", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper makes a clear contribution to the field of dense matching tasks by introducing a novel architecture that unifies feature aggregation and cost aggregation.\\n2. The proposed method is well-motivated and the architecture is clearly explained.\\n3. The evaluation on standard benchmarks for semantic matching and geometric matching shows significant improvements compared to existing methods.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with the current state-of-the-art in dense matching tasks, which may make it difficult for non-experts to follow.\\n2. The authors do not provide a clear comparison with the current state-of-the-art methods in the field.\\n3. The paper could benefit from more qualitative analysis and discussion of the results.\",\n  \"Questions\": \"1. How does the proposed method perform on other types of matching tasks, such as object matching or scene understanding?\\n2. Can the proposed method be applied to other computer vision tasks that require dense matching, such as optical flow estimation or stereo matching?\\n3. How does"}
{"paper_id": "YjG29CIOP7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel solution to the issues of Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC) in Data-Free Meta-Learning (DFML).\\n2. The proposed methods, TEAPOT and ROSY, are well-motivated and effective in addressing the identified problems.\\n3. The paper provides a clear explanation of the approach and its components.\",\n  \"Weaknesses\": \"1. The paper assumes that the pre-trained models are available, which might not always be the case in real-world scenarios.\\n2. The robustness of TEAPOT and ROSY against TDC is only demonstrated through experiments with a specific type of model poisoning attack.\\n3. The paper does not provide a thorough analysis of the computational cost of the proposed methods.\",\n  \"Questions\": \"1. How do the authors plan to handle the situation where the pre-trained models are not available or are outdated?\\n2. Can the proposed methods be applied to other types of attacks, such as advers"}
{"paper_id": "DmDpu3r8iU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel framework for evaluating LLMs' grasp of human values, considering both 'know what' and 'know why'.\\n2. The use of the Schwartz Value Survey and GPT-4 as a benchmark for evaluating the discriminator-critique gap is a strong aspect of the paper.\\n3. The study provides strong evidence that the scaling law impacts 'know what' but not 'know why', which highlights a potential risk in LLMs' value alignment.\",\n  \"Weaknesses\": \"1. The paper's reliance on GPT-4 as a benchmark might be seen as restrictive, and it is unclear how well the results would generalize to other large language models.\\n2. The evaluation of LLMs' value alignment is limited to a specific dataset and may not capture the full range of human values.\\n3. The paper does not provide clear recommendations for mitigating the risks identified in LLMs' value alignment.\",\n  \"Questions\": \"1. How do the authors plan to extend"}
{"paper_id": "kXHEBK9uAY", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The Hierarchical Diffuser combines the advantages of hierarchical and diffusion-based planning, leading to a more efficient and effective planning method.\\n2. The empirical evaluations demonstrate superior performance and efficiency compared to other methods.\\n3. The paper explores the model's generalization capability on compositional out-of-distribution tasks.\",\n  \"Weaknesses\": \"1. The paper assumes that the hierarchical Diffuser is a universal solution for all long-horizon tasks, but it may not generalize well to tasks with complex temporal abstractions.\\n2. The 'jumpy' planning strategy may not be applicable to all domains, and more exploration is needed to understand its limitations.\\n3. The fine-tuning stage may not be effective for all sub-goals, and more investigation is required to understand its impact.\",\n  \"Questions\": \"1. How does the Hierarchical Diffuser handle tasks with complex temporal abstractions, and what are the implications for its generalizability?\\n2. Are there any specific domains or tasks where the 'jumpy"}
{"paper_id": "i7LCsDMcZ4", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a critical problem in event-based classification tasks for Spiking Neural Network (SNN), which is overfitting, by proposing two new methods, SLTRP and SLRP, for extracting saliency maps and CAMs. \\n2. The proposed method, EventRPG, achieves state-of-the-art performance on several SNN structures and tasks, including object recognition and action recognition. \\n3. The paper is well-organized and clearly presented, with a clear introduction to the problem, a detailed description of the proposed method, and thorough evaluation results.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with SNNs and event cameras, which may limit its accessibility to non-experts. \\n2. The evaluation results are impressive, but it is not clear how the proposed method compares to other state-of-the-art methods in terms of computational efficiency and latency.\",\n  \"Questions\": \"1. How does the proposed method, EventRPG, compare to other sali"}
{"paper_id": "jHdz0CIS2y", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel approach to voice conversion using self-synthesized examples, which is a significant improvement over previous methods.\\n2. The framework is well-designed and the iterative training strategy is effective in improving the speaker similarity of generated speech.\\n3. The paper demonstrates state-of-the-art results in zero-shot voice conversion on various metrics.\\n4. The proposed approach is applicable to a range of tasks such as zero-shot voice conversion, voice conversion across different languages, and controllable speech synthesis with pitch and pace modifications.\",\n  \"Weaknesses\": \"1. The paper assumes that the SSL and speaker verification models are pre-trained, which might not be the case in real-world scenarios.\\n2. The evaluation metrics used are mostly focused on speaker similarity, which might not capture other important aspects of voice conversion.\\n3. The paper could benefit from more detailed analysis on the prosodic information derived from the audio signal and SSL representations.\",\n  \"Questions\": \"1. How does the proposed framework handle the issue of speaker variability in"}
{"paper_id": "NkYCuGM7E2", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper explores a promising direction of using LLMs as decision-making components for complex AD scenarios, addressing high-level information comprehension, generalization, and interpretability challenges.\\n2. The proposed cognitive pathways and parameter matrix adaptation enable seamless integration of LLM decisions with low-level controllers.\\n3. The method demonstrates consistent superiority over baseline approaches in single-vehicle tasks and handles complex multi-vehicle coordination.\\n4. The paper aspires to serve as a starting point for future research in this area, which is a valuable contribution.\",\n  \"Weaknesses\": \"1. The paper lacks a detailed analysis of the limitations and potential risks of using LLMs in AD, such as reliability and safety concerns.\\n2. The experimental results are not thoroughly compared with existing AD systems that leverage LLMs in a similar manner.\\n3. The proposed approach may be overly reliant on the quality and availability of LLMs, which could be a challenge in real-world applications.\",\n  \"Questions\": \"1. How do the authors plan"}
{"paper_id": "RtDok9eS3s", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The work presents a clear and well-motivated simplification of the transformer block, leveraging signal propagation theory and empirical observations.\\n2. The experiments demonstrate the effectiveness of the simplified transformers in terms of training speed and performance.\\n3. The paper is well-organized and clearly written.\",\n  \"Weaknesses\": \"1. The simplification might be too aggressive, potentially impacting the model's ability to generalize to more complex tasks.\\n2. The removal of skip connections, projection or value parameters, sequential sub-blocks, and normalization layers might not be universally applicable across all transformer architectures.\\n3. The paper could benefit from a more thorough analysis of the impact of these simplifications on the model's interpretability and explainability.\",\n  \"Questions\": \"1. How do the simplified transformers perform on more complex tasks, such as machine translation or question answering?\\n2. Can the simplifications be extended to other transformer architectures, such as encoder-decoder models or transformer-based models with different attention mechanisms?\"\n}"}
{"paper_id": "YCPDFfmkFr", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The work provides a clear and well-structured overview of the QP layers, which facilitates understanding and implementation.\\n2. The authors tackle the differentiability of QP solutions, a significant challenge in optimization layers, and provide a unified approach to address this issue.\\n3. The proposed QPLayer software package is a valuable contribution to the community, as it enables the differentiation of feasible and infeasible QPs and can be interfaced with modern learning frameworks.\\n4. The paper showcases the effectiveness of the proposed QP layers in various learning tasks, demonstrating superior predictive performance.\\n5. The authors provide alternative formulations to enhance numerical robustness, speed, and accuracy for training QP layers.\",\n  \"Weaknesses\": \"1. The paper focuses primarily on convex QPs, which might limit its applicability to other types of optimization problems.\\n2. While the proposed QPLayer software package is a valuable contribution, it may require significant expertise to utilize effectively.\\n3. The paper could benefit from more comprehensive comparisons"}
{"paper_id": "ueTdErd5Ib", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel method that combines the learning stage with knowledge of the downstream optimization task, which is a significant contribution to the field.\\n2. The method provides theoretical guarantees bounding the underlying regret of decisions proposed by the method.\\n3. Experimental results demonstrate that the approach is competitive in terms of average regret and yields more robust solutions than other methods.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper assumes that the feasible region can be discretized into subsets, which might not always be the case in real-world problems.\\n2. The method is limited to a specific type of optimization problem and may not be generalizable to other types of problems.\\n3. The paper does not provide a detailed comparison with other methods in terms of computational efficiency.\",\n  \"Questions\": \"1. How does the method perform on problems with non-convex feasible regions?\\n2. Can the method be extended to handle problems with multiple uncertain parameters?\\n3. How does"}
{"paper_id": "sDmjlpphdB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel approach to automating instruction design for LLMs, addressing the limitation of demo-free instructions. The use of the Mixture-of-Expert paradigm is a strong point of the paper.\\n2. The two-phase process for constructing specialized experts is well-organized and easy to follow.\\n3. The experimental results show a significant improvement over prior art, with a gain of up to 43% on benchmark NLP tasks.\\n4. The theoretical connection between in-context learning and kernel regression is an interesting idea, and the demo assignment and instruction assignment phases are well-motivated.\",\n  \"Weaknesses\": \"1. The paper assumes that the problem space can be divided into homogeneous regions, which might not always be the case in practice.\\n2. The choice of demo clustering and instruction optimization methods may not be optimal, and more exploration is needed.\\n3. The paper does not provide a thorough analysis of the computational cost of the proposed approach.\",\n  \"Questions\": \"1. How does the"}
{"paper_id": "Mdk7YP52V3", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a solid theoretical explanation for the observed instabilities in heteroskedastic neural regression models.\\n2. The work is well-organized, and the presentation is clear.\\n3. The proposed free energy framework provides a new perspective on the problem and offers insights for optimizing regularization.\\n4. The paper's theoretical results show excellent qualitative agreement with empirical model fits on real-world data.\",\n  \"Weaknesses\": \"1. The paper's focus on a specific type of instability might limit its generalizability to other types of neural networks.\\n2. The regularization scheme proposed in the paper may not be universally applicable and may require further adaptation for different problem domains.\\n3. Some parts of the derivations could be more clearly explained for non-experts in statistical physics.\",\n  \"Questions\": \"1. How would the authors respond to criticisms that their approach might be too specialized for heteroskedastic regression and not generalizable to other types of neural networks?\\n2. Can the authors provide more insight into the computational"}
{"paper_id": "YGTSLDAPqb", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors present a well-motivated approach to domain adaptation that addresses the limitations of standard fine-tuning after self-supervised pretraining. The idea of using targeted augmentations to better connect the source and target domains is a good step forward.\\n2. The evaluation is comprehensive and includes three real-world tasks, showcasing the effectiveness of Connect Later on diverse datasets.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The authors acknowledge that standard fine-tuning after pretraining does not consistently improve OOD error, but the explanation is brief and lacks a deeper analysis of why this is the case.\\n2. The paper could benefit from a more thorough discussion on the choice of targeted augmentations, as the effectiveness of these augmentations is crucial for the success of the approach.\\n3. The evaluation on multiple datasets is a good practice, but it would be helpful to provide more insights on how the performance improvement varies across different datasets.\",\n  \"Questions\": \"1. How does the"}
{"paper_id": "JTcaziw7G1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel approach to private information retrieval for large language models, ensuring both client and server privacy. This is a significant contribution to the field of natural language processing.\\n2. The use of multi-party computation (MPC) to securely transmit queries is a good choice.\\n3. The authors introduce a new MPC-friendly protocol for inverted file approximate search (IVF), which is efficient and allows for fast document search.\\n4. The work opens up new avenues for accessing and using private data in LLMs without centralizing or compromising on privacy.\\n5. The paper is well-organized and clearly presented, with a good structure and concise writing.\",\n  \"Weaknesses\": \"1. The paper assumes that the distributed servers have a similar setup, which might not be the case in real-world scenarios.\\n2. It is not clear how the system would handle large amounts of data or a large number of clients.\\n3. The evaluation of the system's performance is limited to a small-scale experiment, and"}
{"paper_id": "lEkFq4RUCX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed directional distance field (DDF) metric is a novel and effective way to quantify the dissimilarity between two 3D point clouds, with extensive experiments demonstrating its superiority over existing metrics.\\n2. The evaluation on various tasks such as shape reconstruction, rigid registration, scene flow estimation, and feature representation showcases the versatility and accuracy of DDF.\\n3. The paper is well-organized and clearly presented, with a clear introduction to the problem, a concise description of the method, and thorough experiments.\\n4. The code is made available in the supplementary material.\",\n  \"Weaknesses\": \"1. The authors do not provide a clear explanation of why the proposed metric is superior to existing metrics, particularly in terms of computational efficiency.\\n2. While the experiments are extensive, they are limited to a few tasks and datasets, which may not fully demonstrate the generalizability of DDF.\\n3. The authors assume that the reference points are known, which may not be the case in many real-world scenarios.\","}
{"paper_id": "Fq8tKtjACC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a new, smaller language model phi-1 that achieves competitive performance on code-related tasks. This is a valuable contribution to the field, as large models are typically resource-intensive and hard to train.\\n2. The emergent properties of phi-1, such as its surprising performance compared to phi-1-base and phi-1-small, are interesting and worth exploring further.\\n3. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"1. The paper may benefit from a more detailed discussion of the limitations of phi-1, such as its potential lack of generalizability to other domains or tasks.\\n2. The comparison to other models is limited to HumanEval and MBPP, and it would be helpful to see more comprehensive comparisons to other state-of-the-art models.\\n3. The use of GPT-3.5 for generating synthetic data raises some concerns about the quality and validity of the results.\",\n  \"Questions\": \"1. How does phi-1 perform on"}
{"paper_id": "8fQlGQkj0S", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a rigorous analysis of in-context learning, deriving risk upper bounds and revealing a unique phenomenon in task retrieval. The authors use a well-structured approach and provide a clear explanation of their findings.\\n2. The paper makes a significant contribution to the field by shedding light on the limitations of in-context learning.\\n3. The numerical computations and Transformer model implementation provide a clear understanding of the authors' claims.\",\n  \"Weaknesses\": \"1. The paper assumes the use of mean squared error as a risk measure, which might not be the most appropriate choice for all scenarios.\\n2. The paper could benefit from a more in-depth discussion on the implications of the phenomenon in task retrieval.\\n3. The authors could provide more insights into the practical applications of their findings.\",\n  \"Questions\": \"1. How do the results change if other risk measures are used instead of mean squared error?\\n2. Can the phenomenon in task retrieval be replicated in other models or domains?\\n3. What are the potential applications of"}
{"paper_id": "hRos9WldRK", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel method, Learning to Bootstrap (L2B), that effectively addresses the challenge of noisy labels in deep neural networks. This method is well-motivated, well-designed, and well-implemented, and it leads to significant improvements in robustness and generalization.\\n2. The evaluation is comprehensive, and the results are impressive, demonstrating the effectiveness of L2B on several benchmark datasets.\\n3. The paper is well-organized, clearly presented, and easy to follow.\\n4. The code and models are available, making it easy for others to build upon this work.\\n5. The paper highlights the importance of addressing noisy labels in machine learning and provides a practical solution that can be applied to various tasks.\",\n  \"Weaknesses\": \"1. The paper assumes that the noisy labels are available during training, which might not be the case in all scenarios. It would be great to see some discussion on how to handle the scenario where noisy labels are not available.\\n2. The paper could benefit"}
{"paper_id": "ttRSBZiCiu", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed WaveFluid model is an interesting and innovative approach to speech synthesis, leveraging adversarial training to learn a velocity field directly. \\n2. The use of reparameterization techniques to reduce memory footprint and improve training efficiency is a good idea.\\n3. The experimental results show that the model is competitive with previous vocoders in sample quality within 10 inference steps.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more in-depth analysis of the trade-offs between the two-stage architecture and the reparameterization technique. How do these changes impact the model's performance in terms of sample quality and computational efficiency?\\n2. The paper does not provide a clear comparison with other state-of-the-art speech synthesis models, making it difficult to evaluate the significance of the proposed approach.\\n3. The model's ability to generalize to other tasks beyond speech synthesis is not explored.\",\n  \"Questions\": \"1. How does the WaveFluid model perform on more complex tasks, such as multi-speaker or multi-condition speech synthesis"}
{"paper_id": "WYsLU5TEEo", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses two significant issues in neural image classifiers: interpretability and robustness. The proposed framework combines the strengths of counterfactual examples generation and adversarial training in a single model.\\n2. The evaluation of the method is comprehensive, covering both explainability and robustness aspects. The use of semantic segmentation and PGD attack as evaluation metrics is well-chosen.\\n3. The paper is well-organized, and the writing is clear and concise.\\n4. The authors provide a clear explanation of the proposed method and its benefits.\\n5. The experimental results are convincing, showing competitive IoU values and improved robustness to adversarial attacks.\",\n  \"Weaknesses\": \"1. The paper's contribution is mainly in combining existing ideas rather than introducing a new concept.\\n2. The authors do not provide a thorough comparison with other methods that address both interpretability and robustness.\\n3. The evaluation on the fruit defects detection problem may not be diverse enough, as it is a relatively simple task.\\n4"}
{"paper_id": "KKZaj2QS3G", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors address a critical gap in the literature by proposing a novel sampling strategy that promotes consistent representation learning with noise in time series data.\\n2. The proposed encoder architecture with dilated convolution is efficient and scalable, making it suitable for a wide range of downstream tasks.\\n3. The experiments demonstrate the effectiveness of the proposed method across multiple tasks, including forecasting, classification, and abnormality detection.\\n4. The method outperforms state-of-the-art approaches with a significant reduction in parameters.\",\n  \"Weaknesses\": \"1. The paper does not provide a detailed analysis of the impact of noise on the performance of the proposed method.\\n2. The comparison with other methods is limited to only a few state-of-the-art approaches.\\n3. The paper does not discuss potential applications of the proposed method beyond time series analysis.\",\n  \"Questions\": \"1. How does the proposed sampling strategy generalize to other types of noise or data distributions?\\n2. Can the dilated convolution architecture be applied to other types of data or"}
{"paper_id": "LWDRiFzbHQ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes an innovative approach to assess generalization ability of classification models without relying on test ground truths, which is a significant contribution to the field.\\n2. The vicinal risk proxy (VRP) method is mathematically sound and well-defined, and its computation does not rely on labels.\\n3. The experimental results show consistent improvements over existing generalization indicators, especially on severe out-of-distribution test sets.\",\n  \"Weaknesses\": \"1. The paper assumes that the test set is uniformly distributed, which may not always be the case in real-world scenarios.\\n2. The VRP method is computationally expensive, especially for large test sets.\\n3. The paper does not provide a clear explanation of why VRP works better than existing generalization indicators.\",\n  \"Questions\": \"1. How does the VRP method perform on test sets with non-uniform distributions?\\n2. Can VRP be applied to other types of machine learning models, such as regression models?\\n3. What are the"}
{"paper_id": "Yp01vcQSNl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses an important and timely issue in the field of graph transformers, which is the lack of consideration of edge directionality.\\n2. The proposed method is novel and effective in capturing directionality, and the experimental results on directional graph datasets are impressive.\\n3. The paper is well-organized and the writing is clear.\",\n  \"Weaknesses\": \"1. The paper assumes that edge directionality is always important, but it would be more convincing if the authors could provide more evidence or analysis to support this assumption.\\n2. The paper only considers two new datasets, which may not be enough to generalize the results.\\n3. The evaluation of the proposed method is limited to only one type of task, which may not be representative of all directed graph learning tasks.\",\n  \"Questions\": \"1. Can the proposed method be applied to other types of directed graph learning tasks, such as predicting the direction of edges in a graph?\\n2. How does the proposed method compare to other methods that incorporate directionality, such"}
{"paper_id": "LndMyiBl3n", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a specific and timely problem in the field of graph neural networks, namely the vulnerability against black-box attacks. The proposal of a novel measure of graph quality, the Modified Silhouette Score (MSS), is a significant contribution.\\n2. The theoretical analysis and experiments demonstrate the effectiveness of the proposed attack, SheAttack, in both homophilic and heterophilic settings.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The authors could further discuss the limitations and potential biases of the proposed measure, MSS, and its applicability to other domains.\\n2. While the experimental results are convincing, more comprehensive evaluations and comparisons with other state-of-the-art methods would strengthen the paper.\\n3. The paper assumes that the attacker has access to node features and the graph structure, which might not be the case in all scenarios.\",\n  \"Questions\": \"1. Can the authors provide more insights into the choice of the Modified Silhouette Score (MSS) as the"}
