{"paper_id": "B0OwtVEejJ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors have successfully integrated weight-entanglement and gradient-based NAS methods, which is a significant contribution. The proposed TangleNAS approach is efficient and accurate, and it has the potential to revolutionize the field of neural architecture search. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that the reader is familiar with the basics of NAS and weight-entanglement, which might make it difficult for non-experts to understand. Additionally, the authors could have provided more detailed experimental results and comparisons with other state-of-the-art methods.\",\n  \"Questions\": \"How does TangleNAS handle the issue of overfitting in weight-entangled spaces? Can the authors provide more insights into the impact of the zero-padding operation on the performance of the model? Are there any plans to extend TangleNAS to other types of neural networks, such as recurrent or transformer-based architectures?\"\n}"}
{"paper_id": "ZlEtXIxl3q", "review": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 9, \"Confidence\": 4, \"Strengths\": \"The Bradley-Terry loss is a good alternative to MSE for learning sparse latent fitness functions from globally epistatic data. The authors demonstrate its effectiveness on various simulated and real-world datasets. The paper's contribution to the field of protein engineering is significant, as it provides a more data-efficient and robust method for predicting fitness functions.\\nThe use of contrastive losses to learn fitness functions is an interesting direction for future research. The authors' experiments on the FLIP benchmark tasks are thorough and well-justified. The paper is well-written and easy to follow.\", \"Weaknesses\": \"The authors' claim that contrastive losses bypass the issues caused by nonlinearity is not entirely clear. The paper could benefit from a more detailed explanation of how the Bradley-Terry loss handles nonlinearity in the latent fitness functions. The authors should also provide more information about the specific simulations used to test their approach. The paper could be improved by including more results on real-world datasets, beyond the FLIP benchmark tasks.\", \"Questions\": \"How do the authors plan to extend their approach to more complex"}
{"paper_id": "6yJuDK1DsK", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed FEATHER method is well-designed and effectively addresses the challenges of lifelong test-time adaptation by introducing a lightweight adapter that requires fewer additional parameters, keeping the source model frozen and preventing forgetting. The method's performance is comparable or better than state-of-the-art approaches with significant parameter efficiency, making it suitable for real-world applications with practical hardware constraints.\\n\\nThe experimental results on various datasets demonstrate the effectiveness of FEATHER, and the authors provide a clear explanation of the method's advantages over existing approaches.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion on the choice of adapter architecture and the impact of different configurations on the performance. Additionally, the authors could provide more insights into the trade-offs between the number of adapters and the model's performance.\\n\\nThe paper assumes that the pre-trained model is already available, which might not always be the case in real-world scenarios. It would be beneficial to discuss the potential implications of using a pre-trained model and how FEATHER can be adapted to handle cases where the pre-trained model"}
{"paper_id": "lt6xKGGWov", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel feature selection method, MINERVA, which leverages neural estimation of mutual information to capture complex relationships between features and targets. The method is well-motivated and shows promising results on synthetic benchmarks.\",\n  \"Weaknesses\": \"The paper assumes that the neural network used for approximating the Donsker-Varadhan representation of Kullback-Leibler divergence is a good approximation, but does not provide any theoretical guarantees. Additionally, the regularization terms used in the loss function may not be sufficient to prevent overfitting.\",\n  \"Questions\": \"How does the proposed method scale to high-dimensional feature spaces? Are there any theoretical results or guarantees about the convergence of the optimization procedure? Can the method be extended to handle non-linear relationships between features and targets?\"\n}"}
{"paper_id": "Bvrc6kobWd", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a novel analysis of the effect of margins on gradients in contrastive learning, offering a new perspective on the topic.\\n2. The study is well-organized and clearly presented, making it easy to follow the authors' arguments and understand the results.\\n3. The experimental results are comprehensive and demonstrate the importance of the proposed gradient-based approach.\",\n  \"Weaknesses\": \"1. The paper's contribution is somewhat limited, as it only focuses on the effect of margins on gradients, which is a specific aspect of contrastive learning.\\n2. The study relies heavily on existing work in the field, and the authors do not provide a comprehensive overview of the current state of the art in contrastive learning.\\n3. The paper could benefit from a more detailed discussion of the implications of the findings, particularly in terms of potential applications and future work.\",\n  \"Questions\": \"1. How do the authors plan to extend their approach to other types of contrastive learning tasks, such as self-supervised clustering or few-shot"}
{"paper_id": "FJjHQS2DyE", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed Conditional Adversarial Support Alignment (CASA) framework is an effective approach to unsupervised domain adaptation under label shift, with a clear theoretical justification for its benefits. The paper provides a complete training process and empirically validates its performance on benchmark datasets, outperforming state-of-the-art methods in various label shift scenarios. The work highlights the importance of aligning conditional feature supports to achieve more discriminative feature representations for classification.\",\n  \"Weaknesses\": \"The paper assumes the availability of labeled data on the source domain, which might not be a realistic assumption in some real-world scenarios. The empirical evaluation only considers a limited set of benchmark datasets, and it is unclear how CASA would perform on more complex settings like domain generalization and universal domain adaptation. The paper does not discuss potential challenges or limitations of the proposed approach in these settings.\",\n  \"Questions\": \"How does CASA handle cases where the source domain is not fully labeled, or when there is a large class imbalance in the source domain? Can the authors provide more insights"}
{"paper_id": "xelrLobW0n", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The TRACE benchmark is a well-designed and comprehensive evaluation framework for continual learning in LLMs, covering diverse tasks and metrics.\\n2. The Reasoning-augmented Continual Learning (RCL) approach shows promise in retaining reasoning capabilities with less catastrophic forgetting, making it a valuable contribution to the field.\\n3. The paper presents a clear and concise overview of the problem, methodology, and results, making it easy to follow and understand.\",\n  \"Weaknesses\": \"1. The paper assumes that GPT-4 is a reliable source for reasoning annotation, which may not be the case, especially considering its limitations and potential biases.\\n2. The evaluation metrics introduced in the TRACE benchmark may not fully capture the nuances of LLMs' general abilities and instruction-following capabilities.\\n3. The paper could benefit from a more in-depth analysis of the RCL approach, including a detailed explanation of how it addresses catastrophic forgetting and a comparison with other existing methods.\",\n  \"Questions\": \"1. How does the RCL"}
{"paper_id": "bYQkOPvgDw", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed probabilistic graphical model-based framework, PRGNN, is a significant contribution to the field of robust GNNs. The framework's ability to handle label noise without relying on the label smoothness assumption is a major strength. The authors' use of variational inference and evidence lower bounds as objectives is also a notable aspect of the approach. The evaluation on various datasets with different noise levels and graph types demonstrates the effectiveness of PRGNN. The authors' consideration of both homophilous and heterophilous graphs is a notable contribution.\",\n  \"Weaknesses\": \"One potential weakness of the paper is the lack of a detailed analysis of the computational complexity of the proposed method. The authors mention that the computational cost of PRGNN is higher than that of traditional GNNs, but a more detailed analysis of the computational complexity would be beneficial. Additionally, the authors could have explored more sophisticated methods for handling label noise, such as using generative models or adversarial training. The paper could also benefit from a more in-depth discussion of the"}
{"paper_id": "sKPzAXoylB", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors address a significant problem in continual learning by proposing a novel approach that simultaneously mitigates catastrophic forgetting and loss of plasticity. UPGD's utility-based perturbations and gradient updates are a clever idea. The method's effectiveness is demonstrated across various non-stationary streaming problems and reinforcement learning tasks. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The paper assumes that the importance of neural network units can be accurately approximated using a Taylor series. While the authors provide some intuition for this choice, it is unclear how robust this assumption is to different datasets and learning scenarios. Additionally, the method's performance may rely heavily on the choice of hyperparameters, which are not thoroughly explored in the paper.\",\n  \"Questions\": \"How does the Taylor series approximation of the loss function affect the utility-based perturbations, and what are the implications of this choice for the method's performance? Can the authors provide more insight into how the importance of neural network units is determined, and whether this is a stable or robust"}
{"paper_id": "H8Qg1IIMaR", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper identifies a significant vulnerability in LLMs and VLLMs, which is a crucial finding for the field.\\n2. The experiments are thorough and well-designed, covering a range of models and datasets.\\n3. The results are clear and consistent, demonstrating a strong impact of permutation attacks on model performance.\",\n  \"Weaknesses\": \"1. The paper's conclusion may be too broad, as the findings might not generalize to all LLMs and VLLMs.\\n2. The paper could benefit from more in-depth analysis of the underlying causes of the vulnerability.\\n3. The suggested mitigation strategies are not fully effective, and more research is needed to develop robust solutions.\",\n  \"Questions\": \"1. How do the authors plan to address the identified vulnerability in future research?\\n2. Are there any potential applications of this finding beyond MCQA tasks?\\n3. Can the authors provide more insight into the specific mechanisms that make LLMs and VLLMs susceptible to permutation attacks?\"\n}"}
{"paper_id": "tth2qXY7RU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel quantization method, SuFP, which efficiently adapts to varying data distributions and represents both dense and sparse data regions. The multi-region piecewise quantization and tensor-wise scalable bias are effective in improving memory footprint and logic efficiency without compromising accuracy. The compact hardware design and integer arithmetic units enhance the method's feasibility for practical applications. The evaluation across various domains and metrics demonstrates the SuFP's advantages over traditional FP32 and other quantization methods.\",\n  \"Weaknesses\": \"The paper assumes that the data distribution can be split into multiple regions, which might not always be the case. The authors should discuss the scenarios where the method may not be effective. Additionally, the paper does not provide a comprehensive comparison with other recent works in the field, such as tensor train quantization or low-precision neural networks. It would be beneficial to include a discussion on the limitations of the method and potential future directions.\",\n  \"Questions\": \"Can the SuFP method be applied to other types of neural networks, such as recurrent neural networks"}
{"paper_id": "fjf3YenThE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed variance-reduced ZO hard-thresholding algorithm is a significant contribution to the field of zeroth-order optimization. The paper provides a clear and concise explanation of the method and its theoretical analysis. The experiments demonstrate the algorithm's competitiveness on sparse optimization tasks in adversarial attacks and portfolio optimization.\",\n  \"Weaknesses\": \"The paper assumes restricted strong smoothness and convexity, which might not be realistic in all scenarios. The authors should provide more discussion on the limitations of these assumptions and how they impact the algorithm's performance. Additionally, the experiments are limited to two applications, and more diverse use cases would strengthen the paper's claims.\",\n  \"Questions\": \"What are the potential ways to relax the assumptions of restricted strong smoothness and convexity? How would this impact the algorithm's performance and convergence rates? Are there any other applications or scenarios where the algorithm's performance can be further improved or generalized?\"\n}"}
{"paper_id": "H9DYMIpz9c", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes an efficient data distillation method for autoregressive tasks, which is a significant contribution to the field. The method, FARZI, is based on a novel combination of reverse-mode differentiation and factorization of the high-dimensional event space. The results show that FARZI can effectively summarize large autoregressive datasets into smaller, synthetic sequences while maintaining or improving model performance.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the mathematical formulation of the FARZI method, particularly the use of reverse-mode differentiation and Hessian-Vector Products. Additionally, the paper mentions that the approach involves generating FARZI DATA as 3-D tensors of sequences of token distributions, but it is not clear how this is done in practice. Finally, the paper could benefit from a more comprehensive evaluation of the method, including results on a wider range of tasks and datasets.\",\n  \"Questions\": \"How does the FARZI method handle the discrete nature of autoregressive tasks, and how does it adapt to the"}
{"paper_id": "4kLVvIh8cp", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel algorithm for non-linear function approximation in offline RL, offering statistically optimal performance and instance-dependent regret guarantees. The pessimism-based approach is well-motivated and appears to be effective. The use of variance-based weighted regression and variance estimation is a strength. The algorithm's design and analysis are clear and well-explained.\",\n  \"Weaknesses\": \"The paper assumes that the oracle-efficient algorithm is feasible, which might not be the case in practice. The algorithm's performance in complex function class scenarios is not fully explored. The comparison with existing methods is limited to linear approximation contexts.\",\n  \"Questions\": \"How does the algorithm perform in scenarios with very large state-action spaces? Is it possible to relax the oracle-efficient assumption and still achieve similar performance? Can the algorithm be applied to other types of offline RL problems, such as those with continuous state and action spaces?\"\n}"}
{"paper_id": "CSpWgKo0ID", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 3,\n  \"Strengths\": \"1. The study provides a thorough exploration of Large Language Models (LLMs) in social interactive settings, particularly in terms of cooperation and coordination.\\n2. The use of behavioral game theory to evaluate LLMs' strategic behavior is a novel approach and sheds light on their potential efficacy and trustworthiness in future applications.\\n3. The experiment's design, which includes finitely repeated two-player, two-strategy games, is comprehensive and well-structured.\",\n  \"Weaknesses\": \"1. The study focuses primarily on two specific game families (iterated Prisoner's Dilemma and the Battle of the Sexes) and may not generalize to other types of games or more complex social interactions.\\n2. The use of behavioral prompts to simulate interactions between LLMs and human-like strategies might not fully capture the nuances of human behavior.\\n3. The study does not provide a detailed analysis of the LLM's decision-making process and how it adapts to different game scenarios.\\n4. The conclusion that LLMs excel"}
{"paper_id": "gwbQ2YwLhD", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a clear and concise overview of the problem and its significance. The theoretical proofs and experiments demonstrate a significant impact of scale on the performance of structure learning algorithms. The authors have made a substantial contribution to the field by generalizing previous findings to higher-dimensional and non-linear settings.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion of the practical implications of the findings. The experimental results could be more comprehensive, including more datasets and structure types. Additionally, the paper assumes that the scale of the variables is known, which might not always be the case in real-world scenarios.\",\n  \"Questions\": \"How can the findings of this paper be applied to real-world problems where the scale of the variables is not known? What are the potential limitations of the proposed method in terms of computational complexity and scalability?\"\n}"}
{"paper_id": "eIYDKNqXuV", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 2,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed method is novel and attempts to address the challenges of clustering complex datasets efficiently.\\n2. The use of a two-phase clustering algorithm, combining the simplicity of K-Means with a novel community detection method, is an interesting approach.\\n3. The method's ability to determine the number of clusters autonomously is a significant advantage.\",\n  \"Weaknesses\": \"1. The initial K-Means step may limit the accuracy of the final clusters, as the method's performance is heavily dependent on this initial step.\\n2. The method's reliance on a weighted network to detect communities may not be scalable for very large datasets.\\n3. The experimental setup could have been more comprehensive, including more real-world datasets and metrics to evaluate the method's performance.\",\n  \"Questions\": \"1. How does the method handle noise or outliers in the data?\\n2. Can the method be adapted for use with datasets that have a varying number of clusters?\\n3. What are the computational costs of the WLCF algorithm"}
{"paper_id": "bAMPOUF227", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel framework for enhancing the performance of large language models through the integration of supervised language models. The idea of using task-specific SLMs to provide task-specific supervised knowledge to LLMs is original and has the potential to improve generalizability and reduce hallucinations. The experiments conducted on various datasets and tasks demonstrate the effectiveness of the proposed method.\",\n  \"Weaknesses\": \"The paper assumes that the task-specific SLMs are available and can be fine-tuned on specific tasks, which may not always be the case in real-world scenarios. The authors should discuss the scalability of the proposed method to large-scale datasets and the computational resources required for fine-tuning SLMs. Additionally, the paper does not provide a detailed analysis of the impact of the number of SLMs used on the performance of the LLM.\",\n  \"Questions\": \"How does the proposed method handle the case where the task-specific SLMs are not available or are not fine-tuned on the specific task? Can the authors provide more details on"}
{"paper_id": "eNoiRal5xi", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors provide a well-motivated approach to improve domain generalization by combining parameter and data space perturbation. The paper is well-organized and the experiments are thorough. The proposed UDIM framework is a robust solution to the domain generalization problem.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed discussions on the theoretical guarantees of the proposed approach. Additionally, the choice of hyperparameters and the sensitivity of the results to these hyperparameters could be explored in more detail.\",\n  \"Questions\": \"What are the implications of combining parameter and data space perturbation on the computational complexity of the proposed approach? How does the UDIM framework handle cases where the unknown domains are not simulated but are instead observed?\"\n}"}
{"paper_id": "uU0Adp7Sfo", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed CCGAN framework offers a novel approach to GANs by incorporating collaboration and competition, which enables more informed data generation and overcomes the limitations of traditional GANs. The theoretical and empirical results demonstrate its effectiveness in generating high-quality data, especially in regular-shaped object generation.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanations of the theoretical foundations and the derivation of the novel regularization term. Additionally, the experiments could be more comprehensive, including a broader range of datasets and more diverse applications.\",\n  \"Questions\": \"How does the CCGAN framework handle cases where the additional requirements are conflicting or unclear? What are the implications of the regularization term on the training process and the generated data? Can the CCGAN framework be applied to other types of GANs or generative models?\"\n}"}
{"paper_id": "dKju7tbe6D", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors propose a novel architecture that effectively integrates both iterative and parallel computation for visual reasoning, outperforming existing methods on several benchmarks. The method's ability to dynamically compose and execute multiple operations simultaneously while allowing step-by-step iterative processing is a significant contribution. The paper's clear presentation and comprehensive evaluation make it a strong candidate for publication.\",\n  \"Weaknesses\": \"While the proposed method is well-motivated and shows impressive results, the authors could provide more insight into the trade-offs between iterative and parallel computation. The paper assumes that the combination of both methods leads to better generalization, but it would be helpful to see a more thorough analysis of the effects of different ratios of iterative to parallel computation. Additionally, the authors could explore more scenarios where the proposed method might not be effective.\",\n  \"Questions\": \"How does the proposed method handle cases where the visual and language inputs are not well-aligned, potentially leading to conflicting operations? Can the authors provide more insight into the role of the Operation Composition module in the IPRM architecture?\"\n}"}
{"paper_id": "DL7JWbdGr3", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel approach to leveraging pre-trained epidemic time-series models for improved forecasting accuracy and efficiency. The use of self-supervised learning tasks and diverse epidemic datasets from different diseases is an interesting direction. The pre-training scheme allows for generalization to new diseases not seen in the pre-training phase, which is a significant advantage. The experimental results demonstrate the effectiveness of the proposed method, outperforming state-of-the-art models in various epidemic forecasting tasks.\",\n  \"Weaknesses\": \"The paper assumes that the pre-trained models can be fine-tuned for various downstream epidemic analysis tasks, but it would be beneficial to explore this assumption further, particularly in the context of limited data availability. The choice of self-supervised learning tasks (RANDMASK, LASTMASK, PEAKMASK, and SEASONDETECT) is not thoroughly justified, and the authors should provide more insight into why these tasks were selected. Additionally, the paper does not discuss the potential challenges of handling data heterogeneity and missing values in the diverse datasets used for pre-training.\",\n  \""}
{"paper_id": "1PaDPHDhwe", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method of class-specific scaling is effective in improving both robust and average accuracies without requiring additional training. The introduction of the novel evaluation metric, robust coverage, allows for a comprehensive evaluation of debiasing algorithms.\",\n  \"Weaknesses\": \"The method relies heavily on pre-trained models, which may not be optimal for all scenarios. The use of clustering for instance-wise adaptive scaling may not be effective in all cases, especially when the number of clusters is not well-defined. The trade-off between robust and average accuracies may not be universally applicable.\",\n  \"Questions\": \"How does the proposed method perform on datasets with multiple sensitive attributes? Can the class-specific scaling strategy be applied to other types of machine learning models? How does the robust coverage metric compare to other evaluation metrics, such as group accuracy and group fairness?\"\n}"}
{"paper_id": "iI7hZSczxE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method, DIOSC, addresses a significant gap in existing disentangled representation learning methods by relaxing the assumption of statistical independence to independence-of-support via contrastive learning. The use of contrastive objectives to maintain support independence is a novel approach. The Time Disentangling Score (TDS) metric is a useful addition to the field. The experiments demonstrate the effectiveness of DIOSC in handling correlated attributes in time series data.\",\n  \"Weaknesses\": \"The paper relies heavily on synthetic datasets, which may not fully represent real-world scenarios. The evaluation on real-world datasets, such as NILM, is limited to a single task (disaggregation), which may not capture the full potential of the proposed method. The paper assumes that the number of latent variables is known, which may not be the case in real-world applications.\",\n  \"Questions\": \"How does the proposed method handle cases where the number of latent variables is unknown or not fixed? Can the authors provide more details on how the contrastive objectives are computed and optimized? How does"}
{"paper_id": "qrGjFJVl3m", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed Qwen-VL series shows a well-designed architecture that effectively incorporates visual capabilities into the large language model. The three-stage training pipeline and the use of a position-aware vision-language adapter are novel and effective. The multilingual support and fine-grained visual understanding capabilities are significant contributions to the field. The authors provide extensive evaluation and comparisons with previous models, setting new benchmarks for LVLMs.\",\n  \"Weaknesses\": \"The paper would benefit from more discussion on the limitations of the proposed approach, such as the potential for increased computational cost and memory requirements due to the addition of visual capabilities. Additionally, the authors should provide more details on the pre-training and fine-tuning procedures to better understand the training process. The paper assumes a good understanding of the reader's background in vision-language models and large language models, which may not be the case for all readers.\",\n  \"Questions\": \"How do the authors plan to address the potential increase in computational cost and memory requirements for the proposed Qwen-VL models? What are the implications of the"}
{"paper_id": "qup9xD8mW4", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a novel approach to distilling reinforcement learning tasks into small synthetic datasets, enabling efficient training of policies without exploration. This is a significant contribution to the field.\\n2. The method, HaDES, is well-motivated and effectively demonstrates its ability to generalize across various tasks and architectures.\\n3. The paper provides a clear and concise explanation of the method and its application.\",\n  \"Weaknesses\": \"1. The paper relies heavily on the Brax suite and MinAtar environments, which may not be representative of all RL tasks.\\n2. The evaluation of HaDES on a wide range of tasks and architectures could be more comprehensive.\\n3. The paper could benefit from a more detailed analysis of the trade-offs between the number of state-action pairs and policy performance.\",\n  \"Questions\": \"1. How does HaDES handle the exploration-exploitation trade-off in RL tasks?\\n2. Can HaDES be applied to tasks with large action spaces?\\n3. How does HaDES compare to other"}
{"paper_id": "Zw8YxUWL4R", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The idea of using multiple per-layer tokens for conditioning in the cross-attention layers of a text-to-image diffusion model is novel and provides greater control and expressiveness in the synthesis process.\\n2. The Extended Textual Conditioning space, referred to as P+, is an interesting and well-designed approach.\\n3. The method enables novel results in object-style mixing, suggesting potential for broad applications in personalized and creative image synthesis.\",\n  \"Weaknesses\": \"1. The paper does not provide a clear comparison with other state-of-the-art methods, such as DALL-E or StyleGAN, which are also capable of generating high-quality images.\\n2. The authors do not discuss the potential limitations of the P+ space, such as the increased computational complexity or the need for more training data.\\n3. The paper could benefit from more qualitative results, such as user studies or human evaluations, to demonstrate the effectiveness of the proposed method.\",\n  \"Questions\": \"1. How does the P+ space perform when conditioned on multiple text prompts"}
{"paper_id": "3NXhwkZGjz", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 3,\n  \"Strengths\": \"1. The authors propose a novel approach to source-free unsupervised domain adaptation by leveraging multiple prediction hypotheses and their rationales, which can enhance the reliability of pseudo-labels. This is a significant contribution to the field of domain adaptation.\\n2. The proposed method is flexible and can be integrated into existing approaches, allowing for improved adaptation performance without requiring access to source data.\\n3. The authors provide a clear and well-organized presentation of their method and experiments.\",\n  \"Weaknesses\": \"1. The method relies on the accuracy of initial label predictions, which can be poor due to the domain shift. This might affect the quality of the pseudo-labels and, subsequently, the adaptation performance.\\n2. The selection of the most reliable hypothesis is based on the proximity to class-wise rationale centroids and the lack of conflicting rationales. However, this approach may not always identify the most accurate hypothesis, especially in cases where the rationales are noisy or incomplete.\\n3. The method's performance is evaluated on a limited number of datasets"}
{"paper_id": "Fn655mJ4bv", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 3,\n  \"Strengths\": \"The proposed SOInter method effectively incorporates correlations among output variables when interpreting structured output models, leading to higher-quality explanations. The two-phase energy network training and Gumbel-Softmax trick used in the interpreter function are well-designed and efficient. The experiments demonstrate the superiority of SOInter over existing techniques like Lime and Kernel-Shap, especially in cases with significant correlations between output variables.\",\n  \"Weaknesses\": \"The paper assumes a deep understanding of structured output models and their applications, which might limit its accessibility to non-experts. The method's reliance on energy-based approaches may lead to complexity in interpreting the results. The paper could benefit from more in-depth discussions on the limitations of existing interpretation methods and a more thorough comparison with other techniques.\",\n  \"Questions\": \"How does the SOInter method handle cases where the correlations between output variables are not significant? Are there any scenarios where the method's performance is negatively impacted by the structural dependencies among output variables? Can the SOInter method be extended to handle more complex structured output models, such as those with"}
{"paper_id": "VyMW4YZfw7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel and simple spectral method for semi-supervised node classification, which is easy to understand and implement. The experiments demonstrate competitive performance against state-of-the-art models on several benchmarks. The study highlights the importance of evaluating models on different datasets and splits to avoid overestimating their performance.\",\n  \"Weaknesses\": \"The paper could benefit from more in-depth analysis of the proposed method's limitations and potential applications. The comparison with existing SOTA models is not entirely fair, as some models are not used as-is, but with modifications. The paper assumes that the proposed method can replace complex GNNs, but it is unclear whether it can handle more complex tasks or larger graphs.\",\n  \"Questions\": \"Can the proposed method be extended to handle more complex tasks, such as semi-supervised graph regression or graph clustering? How does the method perform on larger graphs with millions of nodes?\"\n}"}
{"paper_id": "09xFexjhqE", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 3,\n  \"Strengths\": \"The paper effectively addresses the issue of gradient divergence in Robust Fine-Tuning (RFT) by introducing a low-rank branch. The approach, AutoLoRa, shows promising results in achieving state-of-the-art adversarial robustness across various tasks. The paper also provides heuristic strategies for automatically scheduling learning rates and loss term scalars, which is a valuable contribution.\",\n  \"Weaknesses\": \"The authors could have done a more comprehensive analysis of the empirical results, including more detailed comparisons with existing methods. Additionally, the paper assumes that the pre-trained feature extractor is fixed and does not provide a clear explanation of how to adapt the method to other pre-trained models.\",\n  \"Questions\": \"How does the low-rank branch improve the robustness of the model compared to other methods? Are there any cases where the method fails to achieve state-of-the-art results? How can the method be adapted to other pre-trained models, and what are the potential challenges in doing so?\"\n}"}
{"paper_id": "nmBjBZoySX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed AdaGLT method is well-designed and well-implemented. The authors provide a comprehensive analysis of the existing limitations of current GLT methods and convincingly argue for their approach. The theoretical proofs and experimental results demonstrate the effectiveness of AdaGLT in various scenarios, including deep GNNs. The authors' focus on adaptability, scalability, and automation is well-motivated and valuable.\",\n  \"Weaknesses\": \"One concern is that the authors do not provide a detailed comparison with other state-of-the-art pruning methods, such as Slimmable Networks, which could provide a more comprehensive understanding of the performance gap. Additionally, the evaluation on the deep GNN scenario seems to be limited to a single dataset, and it would be beneficial to include more diverse datasets to demonstrate the robustness of AdaGLT.\",\n  \"Questions\": \"How does AdaGLT handle the trade-off between sparsity and performance in deep GNNs? Is there a principled way to determine the optimal sparsity level for a given GNN architecture?"}
{"paper_id": "di52zR8xgf", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper enhances the existing Stable Diffusion model with a larger architecture, novel conditioning schemes, and a refinement stage, which improves its performance and makes it competitive with state-of-the-art black-box models while maintaining openness and transparency. The model shows improvements in image quality and adherence to text prompts. The use of advanced autoencoder and diverse dataset for training is a plus point.\",\n  \"Weaknesses\": \"The paper does not provide a clear comparison with other open models, and it is not entirely clear how the novel conditioning schemes and refinement stage contribute to the improved performance. The model still has limitations in single-stage generation and incorporation of additional text synthesis improvements.\",\n  \"Questions\": \"How does the novel conditioning scheme and refinement stage contribute to the improved performance of the model? What are the challenges in single-stage generation and how can the model be improved in this aspect? Can the model be extended to incorporate additional text synthesis improvements?\"\n}"}
{"paper_id": "0b328CMwn1", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The authors propose a novel and interesting approach to improve the performance and efficiency of visual prompting (VP). The introduction of activation prompts (AP) and its connection to normalization tuning is a valuable contribution to the field.\\n2. The extensive experiments and comparisons with existing VP and PEFT methods demonstrate the effectiveness of AP.\\n3. The study highlights the importance of layer depth for AP implementation in CNNs and ViTs.\",\n  \"Weaknesses\": \"1. The paper relies heavily on existing literature and does not provide a clear explanation of the theoretical foundation of AP.\\n2. The experiments seem to focus more on the efficiency metrics rather than the accuracy of the models.\\n3. The choice of datasets and architectures for the experiments is limited.\",\n  \"Questions\": \"1. How does AP compare to other PEFT methods in terms of accuracy and efficiency?\\n2. What are the implications of the findings on the choice of layer depth for AP implementation in different architectures?\\n3. How can the authors further improve the theoretical foundation of"}
{"paper_id": "AMDKqZcZbi", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper's attempt to draw inspiration from the entorhinal-hippocampal circuit is an interesting one. The authors propose a biologically inspired neural architecture, MESH, which combines a content-addressable heteroassociative memory system with a spatially invariant convolutional network architecture, to achieve rapid learning and retention of previously acquired knowledge.\",\n  \"Weaknesses\": \"The paper could benefit from a more thorough comparison with other state-of-the-art models that tackle the same problem. The authors also do not provide a clear explanation of how the proposed architecture scales to more complex tasks or environments.\",\n  \"Questions\": \"How does the proposed architecture generalize to more complex tasks or environments? What is the computational cost of the MESH architecture compared to other state-of-the-art models?\"\n}"}
{"paper_id": "am7BPV3Cwo", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed ImOOD framework addresses a crucial gap in OOD detection by considering imbalanced data distributions. The combination of post-hoc normalization and training-time regularization is a novel and effective approach. The experimental results demonstrate significant improvements over state-of-the-art methods on various benchmarks.\",\n  \"Weaknesses\": \"The paper assumes that the class-aware bias is the primary cause of the performance gap, which may not be universally applicable. The evaluation of ImOOD on a broader range of datasets and scenarios would be beneficial. The paper could provide more insights into the underlying mechanisms of the proposed framework.\",\n  \"Questions\": \"How does ImOOD handle cases where the class-aware bias is not the primary cause of the performance gap? What are the computational costs associated with post-hoc normalization and training-time regularization? Can ImOOD be applied to other types of imbalanced data distributions, such as those with multiple outliers or anomalies?\"\n}"}
{"paper_id": "R1crLHQ4kf", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method provides a robust means of detecting adversarial examples in ASR systems by analyzing the probability distribution of output tokens. The approach is efficient and can be applied across various ASR systems and datasets. The high accuracy of the detection approach, with AUROCs exceeding 99%, is a significant contribution. The method's ability to differentiate between benign and adversarial inputs effectively offers a new defensive layer for ASR systems.\",\n  \"Weaknesses\": \"The paper does not fully address the issue of adaptive attacks, which can reduce detection accuracy slightly. The method relies on the assumption that adversarial examples will exhibit certain statistical characteristics, which might not always hold. The approach may not be effective against highly sophisticated attacks. The paper does not provide a clear explanation of how the method can be integrated into existing ASR systems.\",\n  \"Questions\": \"How can the proposed method be adapted to handle highly sophisticated adversarial attacks? Can the method be integrated into existing ASR systems with minimal modifications? How does the method perform on real-world datasets with varying levels"}
{"paper_id": "i7P2mK3x3o", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel flow-based neural ODE model for computing dynamic optimal transport maps between arbitrary distributions P and Q in high-dimensional space, which is a significant contribution to the field. The model's ability to learn an invertible optimal transport map directly between distributions P and Q is an improvement over existing flow-based models. The paper demonstrates strong empirical performance in various high-dimensional data applications, such as mutual information estimation, energy-based generative modeling, and image-to-image translation.\",\n  \"Weaknesses\": \"The paper assumes that the distributions P and Q are accessible only through finite samples, which may not always be the case in practice. The model's performance may degrade if the sample sizes are small or if the distributions are complex. Additionally, the paper does not provide a thorough comparison with existing methods for computing optimal transport maps between distributions, which would strengthen the argument for the proposed method's novelty and effectiveness.\",\n  \"Questions\": \"How does the Q-flow model handle cases where the distributions P and Q have different support sets or are not absolutely continuous with"}
{"paper_id": "YIls9HEa52", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed irSLDS model effectively addresses the limitations of traditional rSLDS models, providing a flexible and efficient framework for analyzing trial-varying neural data. The introduction of a latent geometric structure over states facilitates better interpretability and application across various neural analysis settings. The model's ability to uncover non-stationary processes in both synthetic and real mice neural data is a significant contribution. The use of a PDE framework for efficient computational inference is also a notable aspect.\",\n  \"Weaknesses\": \"One potential weakness is the assumption of a fixed number of discrete states in the dist-CRP, which may not be suitable for all neural data. Additionally, the model's reliance on a semi-Markov state process may limit its applicability to certain types of neural data. The use of variational Laplace-EM for inference may also be computationally expensive for large datasets.\",\n  \"Questions\": \"How does the irSLDS model handle cases where the number of discrete states is not fixed or unknown? Can the model be extended to handle more"}
{"paper_id": "koYsgfEwCQ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed DynaVol method for unsupervised 3D dynamic scene decomposition is innovative and has the potential to improve scene understanding and enable scene editing. The use of object-centric voxelization and the differentiable volume rendering framework is an interesting approach.\",\n  \"Weaknesses\": \"The paper does not provide a detailed comparison with other 3D methods like uORF, and the results for the 3D-to-4D voxel expansion stage are not provided. The volumetric slot attention mechanism seems to be a key component of the method, but its design and optimization could be further explored.\",\n  \"Questions\": \"How does the DynaVol method handle cases with multiple objects and complex occlusions? Can the method be extended to handle more complex scenes with multiple cameras and varying lighting conditions?\"\n}"}
{"paper_id": "o6AN2ZoNw2", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed P-Seg method is simple yet effective in achieving state-of-the-art results in open-vocabulary semantic segmentation. It leverages pseudo-mask generation and language supervision without requiring annotated masks, making it a more efficient alternative to prior methods. The use of self-supervised ViT for image tokenization and the contrastive loss for feature alignment are well-designed approaches. The scalability of P-Seg is also demonstrated through its performance on various benchmarks.\",\n  \"Weaknesses\": \"While the method is efficient and effective, it may still be limited by the quality of the noisy image-text datasets used for language supervision. The reliance on these datasets could impact the accuracy of the semantic labels assigned to pixels. Additionally, the method's performance may degrade if the pseudo-mask generator is not well-tuned or if the language features are not effectively aligned with the mask embeddings.\",\n  \"Questions\": \"How does the pseudo-mask generator perform when dealing with images that have complex or ambiguous semantics? Are there any potential issues with the self-supervised ViT that could impact the"}
{"paper_id": "HXZK1Z8tHa", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors have successfully identified the limitations of existing Transformer-based image restoration methods and proposed a novel architecture that addresses the trade-off between latency and trainability. The proposed method, ShareFormer, achieves state-of-the-art performance across multiple benchmarks, making it a strong contribution to the field. The evaluation is comprehensive, comparing the model's performance against state-of-the-art methods in terms of latency, flops, and visual quality improvements.\",\n  \"Weaknesses\": \"The paper would benefit from a more detailed analysis of the impact of the proposed architecture on the model's interpretability. While the authors mention the introduction of residual connections on the 'Value' of self-attention, they do not explore the implications of this design choice on the model's explainability. Additionally, the paper could benefit from a more in-depth discussion on the limitations of the proposed method and potential avenues for future research.\",\n  \"Questions\": \"How does the proposed ShareFormer architecture extend to high-level vision problems, and what are the potential challenges and opportunities in applying this approach to more complex"}
{"paper_id": "QO3yH7X8JJ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method, Diff-SR, is well-structured and effectively leverages pre-trained diffusion generative models without additional fine-tuning or distillation. The concept of Perceptual Recoverable Field (PRF) is novel and well-justified. The extensive experiments demonstrate the effectiveness of the approach across multiple datasets.\",\n  \"Weaknesses\": \"The paper assumes that the pre-trained DGMs are available and suitable for the task, which might not always be the case. The choice of metrics for evaluation (FID, PSNR, and SSIM) is standard, but the paper could benefit from additional metrics to provide a more comprehensive understanding of the method's performance.\",\n  \"Questions\": \"How does the proposed method handle cases where the pre-trained DGMs are not suitable for the specific super-resolution task? Can the authors provide more insights into the Perceptual Recoverable Field (PRF) and its computation, especially for images with varying complexities?\"\n}"}
{"paper_id": "sNtDKdcI1f", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors highlight a crucial issue with RLHF and provide a clear and concise analysis of the impact of output length on reward improvements. The experiments are well-designed and the interventions are a good way to control for length biases. The study suggests that existing reward models are biased towards length and that more robust techniques are needed to capture human preferences.\",\n  \"Weaknesses\": \"The study focuses on a specific aspect of RLHF and does not provide a comprehensive evaluation of the technique. The experiments are limited to three datasets and the interventions may not fully capture the complexity of RLHF. The paper would benefit from more discussion on the implications of their findings for future work on RLHF.\",\n  \"Questions\": \"How generalizable are the findings of this study to other applications of RLHF? Are there any potential ways to design reward models that capture human preferences beyond length, without overestimating its importance? How can the results of this study be used to inform the development of more robust reward models for RLHF?\"\n}"}
{"paper_id": "HFtrXBfNru", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed SMART method effectively minimizes representation distortion and improves generalization estimation in evolving graphs. The use of self-supervised learning for temporal generalization estimation is novel and promising. The experimental results on synthetic and real-world data demonstrate the effectiveness of SMART.\",\n  \"Weaknesses\": \"The paper assumes that the graph structure remains relatively stable over time, which might not be the case in all real-world scenarios. The authors should discuss this limitation and provide more insights on how SMART can handle significant structural changes. The ablation study on the impact of structure graph reconstruction is limited to a single experiment, and it would be beneficial to explore more.\",\n  \"Questions\": \"How does SMART handle significant structural changes in the graph, and what are the implications for representation distortion and generalization estimation? Can the authors provide more insights on the theoretical lower bounds on representation distortion and how they are derived?\"\n}"}
{"paper_id": "4u0ruVk749", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel and effective approach to estimating the Individual Treatment Effect (ITE) by using a diffusion model to capture unobserved latent spaces. This approach has the potential to improve ITE estimation by reconstructing meaningful unobserved confounders using a diffusion process conditioned on prior knowledge. The paper demonstrates the effectiveness of this approach on both synthetic and benchmark datasets, and the experimental comparisons with 12 state-of-the-art models are thorough and well-conducted.\",\n  \"Weaknesses\": \"The paper assumes that the prior knowledge is available, which might not be the case in all real-world scenarios. Additionally, the diffusion process might not be able to capture all the complex relationships between the observed and unobserved confounders.\",\n  \"Questions\": \"How does the diffusion process handle high-dimensional data? Can the approach be extended to handle multiple treatment variables? How does the model handle confounding variables that are not observed but affect the outcome variable?\"\n}"}
{"paper_id": "Pa6SiS66p0", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper introduces a comprehensive benchmark for multi-modal continual learning, which is a significant contribution to the field. The proposed method, SAMM, effectively reduces catastrophic forgetting and improves adaptability in various challenging learning settings. The use of relational structural similarities between data points in each modality for integration and alignment is an innovative approach. The experiments conducted on the VGGSound dataset provide a thorough evaluation of the proposed method.\",\n  \"Weaknesses\": \"The paper could benefit from a more in-depth analysis of the proposed method's limitations and potential drawbacks. The experiments could be extended to include more diverse datasets and challenging scenarios. The use of a single benchmark dataset (VGGSound) may limit the generalizability of the results.\",\n  \"Questions\": \"1. How does the proposed method, SAMM, compare to other multi-modal continual learning approaches in terms of computational efficiency and scalability? \\n2. Can the relational structural similarities between data points in each modality be applied to other domains or applications beyond visual and audio data?\"\n}"}
{"paper_id": "rkplYfqUr0", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel approach to zero-shot text classification using a generative prompting framework, GEN-Z. This approach aggregates results from multiple paraphrases of label descriptions to reduce variance and improve performance. The use of ChatGPT to generate paraphrases is a creative and effective solution. The paper demonstrates robustness to prompt variations and better performance overall compared to traditional zero-shot and few-shot baselines.\",\n  \"Weaknesses\": \"The paper assumes that the model can generate paraphrases of label descriptions, which may not always be possible or accurate. Additionally, the use of ChatGPT to generate paraphrases may not be generalizable to other datasets or tasks. The paper does not provide a clear explanation of how the model handles out-of-vocabulary words or words with multiple possible meanings.\",\n  \"Questions\": \"How does the model handle out-of-vocabulary words or words with multiple possible meanings? Can the model be adapted to other tasks or domains with minimal modifications? How does the use of ChatGPT to generate paraphrases affect the model's performance and"}
{"paper_id": "pTU2X9mUBe", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a comprehensive dataset for last-mile delivery research, which is a critical area in logistics and supply chain management. The dataset is large-scale, diverse, and publicly available, making it a valuable resource for the research community. The paper also presents a clear hypothesis, method, and conclusion, and the authors verified the utility of the dataset by benchmarking it against several tasks.\",\n  \"Weaknesses\": \"The paper focuses primarily on the dataset and its properties, with limited discussion on the potential applications and future research directions. The authors could have provided more insights on how the dataset can be used to advance research in logistics and supply chain management.\",\n  \"Questions\": \"How does the LaDe dataset address the limitations of existing datasets, such as Amazon's dataset? What are the potential challenges and opportunities for using this dataset in real-world applications?\"\n}"}
{"paper_id": "uhtQyRrTzY", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a well-structured and well-motivated solution to the problem of generating large-scale multi-view datasets for dense vision tasks. The proposed method, MIMIC, leverages real-world data and computer vision techniques to achieve superior performance compared to existing methods relying on annotated datasets. The creation of two large-scale datasets, MIMIC-1M and MIMIC-3M, demonstrates the scalability of the approach. The evaluation on downstream tasks shows promising results, and the method's potential to create even larger datasets for better model training is a significant contribution.\",\n  \"Weaknesses\": \"The paper assumes the availability of a large corpus of unannotated real-world videos and synthetic environments, which might not be feasible for all applications. The method's reliance on SIFT and RANSAC might not be robust to challenging cases where keypoints are difficult to detect or correspondences are hard to establish. The evaluation is limited to a few downstream tasks, and it would be beneficial to assess the method's performance on a broader range of tasks and"}
{"paper_id": "WZfatbNdLV", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The TrASPr model shows significant improvement in predicting and designing RNA splicing outcomes, specifically for tissue-specific splicing, suggesting its potential in uncovering novel regulatory mechanisms and therapeutic targets.\\n2. The combination of multiple transformers and side information in TrASPr architecture appears to be effective in handling the complexity and variability of splicing mechanisms across different cellular conditions and datasets.\\n3. The use of Bayesian Optimization framework (BOS) leveraging TrASPr for proposing sequence alterations for specific splicing outcomes is a valuable contribution to experimental and therapeutic applications.\",\n  \"Weaknesses\": \"1. The paper relies heavily on the pre-trained transformer model, and it is not clear how much of the improvement is due to the novel architecture and how much is due to the pre-training.\\n2. The evaluation on test datasets for PSI and dPSI prediction accuracy could be more comprehensive, including a broader range of tissues and conditions.\\n3. The regulatory elements tested using ENCODE RBP knockout data could be more thoroughly explored to establish the biological"}
{"paper_id": "YkRwadXWHd", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed SignKD approach combines video, keypoints, and optical flow effectively to improve CSLR performance. The hierarchical knowledge distillation framework is an interesting contribution. The experimental results show significant improvements in WER compared to previous methods.\",\n  \"Weaknesses\": \"The paper assumes the availability of keypoints and optical flow data, which may not be feasible in all scenarios. The computational cost of the multi-modal feature extractor could be a concern for real-time applications. The evaluation of the proposed method is limited to three datasets, and it would be beneficial to explore its performance on a broader range of datasets.\",\n  \"Questions\": \"How does the proposed method handle cases where keypoints or optical flow data are missing or noisy? Can the hierarchical knowledge distillation framework be applied to other computer vision tasks? What are the computational costs of the multi-modal feature extractor and the hierarchical knowledge distillation framework in real-time applications?\"\n}"}
{"paper_id": "q4Bim1dDzb", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 3,\n  \"Strengths\": \"1. The proposed unified voxelization framework, UniVoxel, effectively integrates geometry, materials, and illumination into a single representation, significantly accelerating inverse rendering.\\n2. The use of lightweight MLP networks and Spherical Gaussians for modeling local incident light radiance is a novel and efficient approach.\\n3. The method demonstrates superior optimization efficiency and favorable reconstruction quality on various benchmarks.\",\n  \"Weaknesses\": \"1. The paper assumes that the input images are multi-view, which may not always be the case in real-world scenarios.\\n2. The method's performance on complex scenes with varying lighting conditions could be more extensively evaluated.\\n3. The authors should provide a more detailed analysis of the computational complexity of the proposed method compared to existing inverse rendering methods.\",\n  \"Questions\": \"1. How does the unified voxelization framework handle scenes with complex geometry and varying materials?\\n2. Are there any plans to extend the method to handle more general lighting conditions, such as non-Lambertian materials or complex environments?\\n3. The"}
{"paper_id": "39cPKijBed", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed time-dependent importance reweighting method effectively reduces bias in diffusion model training, outperforming existing baselines. The theoretical and experimental results show that the method converges to an unbiased distribution more reliably, addressing the corruption of generated outputs due to dataset bias. The approach is substantiated by results on diverse datasets with varying bias intensities, highlighting its practical applicability and significance in enhancing the generative quality of diffusion models.\",\n  \"Weaknesses\": \"The authors do not discuss the computational complexity of their method and its potential impact on the training time of diffusion models. They also do not provide a clear explanation of how to choose the optimal time-dependent density ratio for different datasets and tasks.\",\n  \"Questions\": \"How does the time-dependent importance reweighting method handle the case when the biased and unbiased distributions have different supports? Can the authors provide more details on the choice of the time-dependent discriminator and its impact on the performance of the proposed method?\"\n}"}
{"paper_id": "aG3EARrrd1", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes an efficient and accurate method for handling streaming multivariate scattered data, which is a significant problem in real-time data analysis. The model's ability to adaptively expand and refine network components is a notable contribution. The experiments on benchmark time series datasets demonstrate the model's efficacy.\",\n  \"Weaknesses\": \"The model's performance is compared to existing online RBF approaches, but the comparison could be more comprehensive, including other state-of-the-art methods. The paper assumes that the model's adaptability is a significant advantage, but it would be helpful to provide a more detailed analysis of its limitations and potential drawbacks.\",\n  \"Questions\": \"How does the model handle high-dimensional data, and what is its performance in such scenarios? What are the computational costs of the adaptive RBF exploration, and are there any potential optimizations to be made?\"\n}"}
{"paper_id": "Ts95eXsPBc", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors propose a novel approach to incorporating spatial information into episodic memory models, which is a significant step forward in the field. The introduction of the Adaptive Memory Allocator (AMA) based on reinforcement learning is also an interesting contribution. The paper demonstrates the effectiveness of SAT across multiple machine learning tasks.\",\n  \"Weaknesses\": \"The paper assumes that spatial information is essential for cognitive processes, which is a claim that needs to be further supported. The experiments conducted are limited to a few environments and tasks, which may not be representative of all possible use cases. The paper could benefit from a more comprehensive evaluation.\",\n  \"Questions\": \"How does the AMA learn to optimize memory utilization in different environments and tasks? What is the theoretical foundation for incorporating spatial information into episodic memory models? Are there any potential limitations or pitfalls in using reinforcement learning for memory allocation?\"\n}"}
{"paper_id": "LfhG5znxzR", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed method of introducing sparse and discrete bottlenecks to neural networks is well-motivated and innovative, addressing a significant gap in the field of neural network interpretability.\\n2. The method's ability to enhance interpretability and controllability without degrading performance is impressive and opens up new avenues for understanding and manipulating neural networks.\\n3. The approach of using vector quantization to represent activations as discrete codes is sound and well-implemented.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more thorough discussion on the trade-offs between the size of the codebook and the performance of the network.\\n2. It would be beneficial to explore the potential applications of this method in more complex neural network architectures and real-world tasks.\\n3. Some of the experiments could be improved by including more baselines and comparing the performance of the proposed method with other state-of-the-art methods.\",\n  \"Questions\": \"1. How does the size of the codebook affect the performance of the network, and is there"}
{"paper_id": "bUGzjiUsIq", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The PAPG framework effectively addresses the challenges of partially annotated multi-label classification. The proposed approach shows promise in handling the scarcity of positive annotations and label imbalance. The integration of reinforcement learning with partial annotation using a Policy Gradient (PAPG) approach is innovative and addresses a significant gap in existing methods.\",\n  \"Weaknesses\": \"The paper could benefit from more extensive experimentation on a wider range of datasets and domains. Additionally, the authors could provide more insight into the impact of the adaptive threshold and data enhancement on the performance of the PAPG framework.\",\n  \"Questions\": \"How does the PAPG framework handle the case where the partially annotated data contains incorrect or noisy labels? What is the computational cost of the iterative updates between the policy network and the value network? How does the PAPG framework compare to other existing methods that also use reinforcement learning for multi-label classification?\"\n}"}
{"paper_id": "uMAujpVi9m", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 3,\n  \"Strengths\": \"The paper addresses a significant problem in the field by introducing a novel approach to mitigate the protein-ligand data scarcity issue. The proposed method, ProFSA, is well-designed and demonstrates promising results on various tasks. The use of segmentations of high-resolution atomic protein structures and pseudo-ligand representations is a good idea. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The paper assumes that the pretrained small molecule encoder is fixed, which might limit the generality of the proposed method. The alignment of data distributions is also a critical aspect, and the authors should provide more details on how they ensured that the generated pseudo-complexes are representative of real ligand interactions. Additionally, the paper does not discuss the potential limitations of the method in terms of scalability and computational resources required.\",\n  \"Questions\": \"How does the authors handle the case where the protein structure has multiple pockets or binding sites? Is the proposed method applicable to other types of protein-ligand interactions, such as protein-protein interactions? The"}
{"paper_id": "uqxBTcWRnj", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes an innovative framework, TDL, for learning symbolic features from visual inputs without relying on pre-defined visual features. This approach has the potential to bridge the gap between neural and symbolic representations.\\n2. The authors provide a clear and well-organized presentation of their method, making it easy to follow.\\n3. The experiments demonstrate the effectiveness of TDL in discovering compositional patterns and outperforming other unsupervised part segmentation methods.\",\n  \"Weaknesses\": \"1. The paper assumes that the proposed framework can learn symbolic knowledge from visual observations without providing a clear explanation of how this is achieved.\\n2. The authors do not provide a detailed comparison with other symbolic reasoning methods that do not rely on neural networks.\\n3. The evaluation metrics, clustering information gain and heuristic shape score, may not be comprehensive enough to fully capture the effectiveness of the proposed method.\\n4. The paper lacks a clear discussion on how the proposed framework can be applied to real-world tasks and problems.\",\n  \"Questions\": \"1"}
{"paper_id": "o4CLLlIaaH", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed Generalizable neural Point Field (GPF) is a well-structured and well-implemented solution to the existing issues with image-based neural radiance fields. The method's innovations in visibility-oriented feature fetching, nonuniform log sampling, and feature-augmented learnable kernel are effective and improve rendering quality and geometry correctness. The experimental validation on NeRF synthetic, DTU, and BlendedMVS datasets demonstrates the GPF's potential in enabling scene interaction and achieving significant enhancements over benchmarks in both generalization and finetuning scenarios.\",\n  \"Weaknesses\": \"While the GPF shows promising results, its reliance on geometric priors and learnable kernels may not be suitable for all scenes, especially those with complex or dynamic geometries. Additionally, the method's scalability and efficiency in handling large scenes or high-resolution images are not thoroughly explored.\",\n  \"Questions\": \"How does the GPF handle scenes with complex or dynamic geometries, and what modifications would be necessary to adapt to such cases? What are the computational costs associated with the"}
{"paper_id": "KNzL1nglNB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors provide a novel approach to address the challenge of insufficient labeling in machine learning, leveraging label encoding to enhance prediction discriminability and diversity. The LRM method is theoretically sound, and the empirical evaluation demonstrates its effectiveness in various scenarios. The paper is well-organized, and the presentation is clear.\",\n  \"Weaknesses\": \"The paper relies heavily on existing work in semi-supervised learning and unsupervised domain adaptation, and the novelty of LRM might be limited. The experimental setup could be improved by including more datasets and baseline methods. The theoretical analysis, while informative, could be more comprehensive.\",\n  \"Questions\": \"How does LRM handle the case where the label encoding is ambiguous or uncertain? Can the authors provide more insights into the relationship between LRM and neural collapse? Are there any potential risks or limitations of using LRM in real-world applications?\"\n}"}
{"paper_id": "pvhyBB86Bt", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 2,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The study provides a comprehensive analysis of the Jacobian's behavior during training, leveraging recent results from random matrix theory. The proposed general stability theorem is a significant contribution to the field. The use of Multi-Layer Perceptrons (MLPs) as a model and the investigation of sparse networks and non-iid weight initialization are well-suited for the study's goals.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanations of the Jacobian's behavior in different scenarios, such as deeper networks or more complex architectures. The study's focus on MLPs might limit its generalizability to other network types. The conclusions drawn from the analysis might be too specific to the chosen models and initializations.\",\n  \"Questions\": \"How do the results generalize to more complex architectures, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs)? What are the implications of the study's findings for other areas of machine learning, such as transfer learning or meta-learning?\"\n}"}
{"paper_id": "Mylk5iamJC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel approach that effectively addresses both closed-set and open-set recognition problems using a single latent generative model, which is a significant contribution to the field. The L-GMM framework is well-defined and the experimental results demonstrate its superiority over discriminative models and existing specialized methods. The paper is well-organized and the writing is clear.\",\n  \"Weaknesses\": \"The paper assumes that the generative model can accurately represent the underlying distribution of the data, which may not always be the case. The authors should provide more discussion on the limitations of this assumption and how it affects the performance of the proposed method. Additionally, the paper could benefit from more extensive experiments on different datasets and settings to further evaluate the robustness of the proposed approach.\",\n  \"Questions\": \"How does the proposed method handle cases where the unknown classes are not well-represented in the training data? Can the authors provide more insights on how the L-GMM framework can be extended to handle multi-class open-set recognition problems? What are the computational requirements for training"}
{"paper_id": "MsOcVFzv8D", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a clear and concise explanation of the theoretical foundation of MDTC and its limitations. \\n2. The proposed MDAT method is well-defined and easy to understand. \\n3. The empirical validation of MDAT on two MDTC benchmarks demonstrates its effectiveness in improving the average accuracy and performance across individual domains.\\n4. The paper addresses the noted gap between theory and practice in MDTC by providing a new theoretical framework with guarantees.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed discussion of the theoretical framework and its implications on the MDAT method.\\n2. The empirical evaluation could be more comprehensive, including more experiments and baselines.\\n3. The authors could provide more insights into the Rademacher complexity and its role in the MDAT method.\",\n  \"Questions\": \"1. How does the proposed theoretical framework compare to existing approaches, and what are the key differences?\\n2. Can the authors provide more details about the Rademacher complexity and its relation to the MD"}
{"paper_id": "fBlHaSGKNg", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed UPA method provides a novel approach to sample selection for annotation, which can efficiently select a diverse and representative subset for annotation under budget constraints. This can lead to improved performance in SSL. The use of \u03b1-MMD as a criterion for sample selection is also a strength.\",\n  \"Weaknesses\": \"The paper assumes that the labeled and unlabeled data are independent and identically distributed, which might not hold in real-world scenarios. The selection of the Gaussian kernel for sample selection might not be the best choice for all datasets. The evaluation of UPA is limited to a few popular SSL frameworks and datasets.\",\n  \"Questions\": \"Can the UPA method be adapted for other types of SSL or active learning methods? How does the choice of kernel affect the performance of UPA? Are there any scalability issues with the GKHR algorithm for large datasets?\"\n}"}
{"paper_id": "HwQ8NVvdmm", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed method, HDQT, effectively integrates Hadamard transforms with quantization and stochastic rounding to achieve accurate and efficient continual learning.\\n2. The study demonstrates significant improvements in computational efficiency and energy consumption, making it suitable for edge applications.\\n3. The authors provide a clear and well-structured presentation of their method and results.\",\n  \"Weaknesses\": \"1. The evaluation of HDQT is limited to a few datasets and continual learning settings, which may not fully represent the diversity of real-world applications.\\n2. The comparison with other CL techniques, such as LwF, iCaRL, and BiC, could be more comprehensive, including additional metrics or more detailed analysis.\\n3. The discussion on the choice of quantization levels and the impact of stochastic rounding on accuracy could be more explicit.\",\n  \"Questions\": \"1. How does the choice of Hadamard transform size and quantization levels affect the trade-off between accuracy and efficiency?\\n2. Can the authors provide more insights into the computational"}
{"paper_id": "cZo6pDtDZr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents two novel LDP algorithms that address the open problem of locally differentially private estimation of collision probability, achieving near-optimal sample complexity and sequential testing without pre-specified sample limits. The authors provide a thorough analysis of the algorithms' performance, demonstrating their efficiency and accuracy.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that it may not be immediately clear to readers without a strong background in differential privacy and locally private algorithms. Additionally, the paper assumes a somewhat specific setting, focusing on collision probability estimation, which might limit its applicability to other domains.\",\n  \"Questions\": \"Can the authors provide more insight into how their algorithms can be adapted to other types of privacy-sensitive applications? How do they see their work being integrated into existing frameworks for differential privacy?\"\n}"}
{"paper_id": "F7QnIKlC1N", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed Graph Transformer with Molecule Structural Residual Self-Attention (MSRSA) mechanism is well-designed and effective in predicting the 3D ground-state conformation of molecules from their 2D topological architectures. The model's performance on the Molecule3D and QM9 datasets is impressive, and its robustness and general applicability for other molecular property predictions are notable. The use of Laplacian Positional Encoding and the MoleBERT Tokenizer also adds to the model's strengths.\",\n  \"Weaknesses\": \"While the model's performance is impressive, it would be beneficial to have a more comprehensive comparison with other state-of-the-art models and open-source tools like RDkit. Additionally, the model's reliance on pre-trained language models like MoleBERT might be a limitation in certain scenarios where domain-specific knowledge is required. Furthermore, the model's performance on larger molecules or more complex systems should be investigated to assess its scalability.\",\n  \"Questions\": \"1. How does the MSRSA mechanism improve the self-attention process"}
{"paper_id": "8JCn0kmS8W", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a comprehensive framework for generating audio content from textual descriptions, showcasing potential in storytelling and human-machine co-creation. The use of Large Language Models (LLMs) to generate an audio script is an interesting approach. The evaluation on text-to-audio benchmarks, including AudioCaps and Clotho, demonstrates state-of-the-art results.\",\n  \"Weaknesses\": \"The paper's reliance on existing audio generation models and LLMs may limit its novelty. The efficiency and artificial composition aspects of the system remain as potential limitations. The paper could benefit from more detailed discussions on how the proposed framework handles edge cases, such as when the text input is ambiguous or incomplete.\",\n  \"Questions\": \"How does the proposed framework handle cases where the text input is ambiguous or incomplete, leading to inconsistent or missing audio elements? Can the system be fine-tuned to accommodate specific genres or styles of audio content, and if so, how?\"\n}"}
{"paper_id": "XbLffB0T2z", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 2,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed Transferable Poisoning approach achieves significantly improved transferability of poisoning perturbations across different learning paradigms.\\n2. The use of high-frequency perturbations ensures efficacy regardless of the training paradigm, demonstrating a more severe threat of availability attacks than previously thought.\\n3. The authors provide a clear and concise explanation of the proposed method and its effectiveness.\",\n  \"Weaknesses\": \"1. The paper focuses on the transferability of poisoning attacks across different learning paradigms, but it would be beneficial to explore the potential defenses against such attacks.\\n2. The experimental results are mostly based on image datasets, and it would be interesting to see the authors' approach applied to other domains, such as text or speech recognition.\\n3. The authors assume that the adversary and the victim use different learning algorithms, but in real-world scenarios, the adversary may have more information about the victim's algorithm.\",\n  \"Questions\": \"1. How do the authors plan to extend their approach to handle multiple adversaries with different learning algorithms"}
{"paper_id": "4y3GDTFv70", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a comprehensive theoretical framework for explaining the emergent abilities of LLMs, which is a significant contribution to the field of NLP. The use of Bayesian inference on sparse joint distributions of languages is an interesting and novel approach. The simulation experiments using synthetic data provide a clear validation of the proposed theory. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that LLMs can approximate the marginal distribution of languages accurately, which might not be the case in all scenarios. The use of doubly-embedded Markov chains as a simulation experiment might not capture the complexity of real-world language data. The paper does not provide empirical evidence from real-world datasets to support the proposed theory.\",\n  \"Questions\": \"How does the proposed theory account for the differences in emergent abilities between various LLMs, such as BERT and RoBERTa? Can the theory be extended to other types of language models, such as transformer-based models? How does the theory handle the issue of overfitting in L"}
{"paper_id": "ODSgo2m8aE", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors have made a novel contribution to the field of graph neural networks by introducing Lipschitz bounds to constrain biases and stabilize outputs. The proposed method, JacoLip, is a practical and efficient approach that can be easily integrated with existing GNN training pipelines. The experimental results demonstrate the effectiveness of JacoLip in enhancing fairness and stability of GNN predictions without significant computational overhead.\",\n  \"Weaknesses\": \"The paper assumes that the input graph data is already preprocessed and does not address the issue of preprocessing itself. It would be beneficial to discuss the implications of this assumption and potential methods to address it. Additionally, the paper focuses on node classification and link prediction tasks, but it would be interesting to explore the applicability of JacoLip to other tasks, such as graph regression or clustering.\",\n  \"Questions\": \"How does JacoLip perform on more complex graph structures, such as those with multiple types of edges or nodes with varying attributes? Are there any potential limitations or edge cases where JacoLip may"}
{"paper_id": "RlfD5cE1ep", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors provide a clear and concise analysis of the dynamics of non-contrastive learning, highlighting the importance of feature normalization in preventing collapse. The theoretical framework and eigenmode dynamics analysis are well-explained and easy to follow. The numerical simulations using the CIFAR-10 dataset and SimSiam framework demonstrate the practical relevance of the findings.\",\n  \"Weaknesses\": \"The paper assumes a high-dimensional limit to model the learning dynamics, which may not accurately represent real-world scenarios. The authors could have explored more real-world datasets and architectures to validate their findings. Additionally, the paper could benefit from more discussion on the implications of the stable equilibriums and how they relate to the robust performance of self-supervised non-contrastive learning systems.\",\n  \"Questions\": \"How do the authors envision their findings being applied to real-world problems, where the data distribution may not be well-represented by the high-dimensional limit? Can the authors provide more insight into the role of cosine loss in stabilizing the representation space, and how it compares"}
{"paper_id": "18TfucMNTr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a well-motivated approach to addressing the challenge of saddle points in deep learning optimization. The proposed method, using a continuation approach with regularization, is theoretically sound and computationally efficient. The experimental results demonstrate the effectiveness of the method in improving training outcomes for deep networks. The paper is well-written and clearly presented.\",\n  \"Weaknesses\": \"One potential weakness is that the method may not be broadly applicable to all deep learning tasks, particularly those with non-convex loss functions that are not well-suited to continuation methods. Additionally, the regularization term used in the continuation optimizer may require careful tuning for different problems.\",\n  \"Questions\": \"How does the regularization term in the continuation optimizer interact with the underlying non-convex loss function? Does the method's performance depend on the specific choice of regularization term, and if so, how? Are there any theoretical guarantees on the convergence of the method, and if so, what are they?\"\n}"}
{"paper_id": "JGTC6WgO4T", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel framework for multi-source black-box domain adaptation, which is a relevant and timely problem. The framework is well-structured and easy to follow. The experimental results demonstrate competitive performance compared to state-of-the-art methods.\",\n  \"Weaknesses\": \"The paper assumes that the source and target domains have the same label space, which might not be the case in real-world scenarios. The pseudo-label refinement process relies on the quality of the source predictions, which might be biased. The authors did not provide a detailed analysis of the computational complexity of the proposed framework.\",\n  \"Questions\": \"How does the framework handle cases where the source and target domains have different label spaces? What are the implications of the pseudo-label refinement process on the overall performance of the framework?\"\n}"}
{"paper_id": "fQHb1uZzl7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method effectively integrates feature and cost aggregation, leading to improved performance in dense matching tasks.\\nThe Transformer-based architecture capitalizes on self- and cross-attention mechanisms, allowing for robust and efficient estimation of correspondences.\\nThe method is well-evaluated on various benchmarks, demonstrating significant improvements over existing techniques.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed analysis of the trade-offs between feature and cost aggregation.\\nThe method's performance in extreme cases, such as large intra-class variations and geometric deformations, is impressive, but more experiments would strengthen the results.\\nThe authors could also discuss potential applications of the proposed method beyond dense matching tasks.\",\n  \"Questions\": \"What are the computational requirements for the proposed method, especially for large-scale datasets?\\nHow does the method handle cases with multiple objects or occlusions?\\nCan the authors provide more insights into the choice of hyperparameters for the Transformer-based architecture?\"\n}"}
{"paper_id": "YjG29CIOP7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method, TEAPOT, effectively addresses the task-distribution shift (TDS) and task-distribution corruption (TDC) issues in data-free meta-learning. The combination of TEAPOT with ROSY, a model selection policy, demonstrates significant improvements in stability and robustness. The experiments on various datasets show superior performance compared to previous baselines. The authors' approach has the potential to make data-free meta-learning more applicable in real-world scenarios.\",\n  \"Weaknesses\": \"One potential concern is that the proposed method relies heavily on the quality of the pre-trained models, which may not always be available or reliable. Additionally, the complexity of the ROSY policy may make it challenging to adapt to new scenarios. The authors should provide more insights into how TEAPOT and ROSY can be adapted to handle diverse task distributions and corrupted models.\",\n  \"Questions\": \"How does the authors' approach handle the scenario where the pre-trained models are not available or are corrupted? Can TEAPOT and ROSY be extended to handle more"}
{"paper_id": "DmDpu3r8iU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper addresses a critical concern in the field of LLMs, which is the alignment with human values. The Value Understanding Measurement (VUM) framework is a novel approach to evaluate LLMs' understanding of values, including both 'know what' and 'know why' elements. The use of the discriminator-critique gap (DCG) approach is a good idea. The study provides a clear and well-organized presentation of the methodology and results.\",\n  \"Weaknesses\": \"The paper relies on a limited number of LLMs (five) for evaluation, which might not be representative of the entire range of LLMs. The study also focuses on a specific set of values (ten) and their categorization, which may not capture the full spectrum of human values. The authors assume that the Schwartz Value Survey is a reliable source for value annotation, but this assumption may not hold for all values or LLMs. Additionally, the study does not provide a clear explanation of how the critique scores are calculated, which makes it"}
{"paper_id": "kXHEBK9uAY", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes an innovative hierarchical approach to improve the efficiency and effectiveness of diffusion-based planning methods for long-horizon tasks. The proposed Hierarchical Diffuser outperforms non-hierarchical methods and other hierarchical planning methods on standard offline RL benchmarks, with better training and planning speeds. The method also enhances generalization capabilities for compositional out-of-distribution tasks.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanations of the training process and the choice of hyperparameters for the Sparse Diffuser and the low-level planner. Additionally, the paper could provide more insight into the effectiveness of the hierarchical approach in different scenarios and environments.\",\n  \"Questions\": \"How does the Hierarchical Diffuser handle scenarios where the subgoals are not well-defined or are subject to change during the planning process? What is the computational cost of training and planning with the Hierarchical Diffuser compared to the Diffuser and other hierarchical planning methods?\"\n}"}
{"paper_id": "i7LCsDMcZ4", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel method for generating saliency maps and class activation maps for SNNs, which is a significant contribution to the field.\\n2. The proposed methods, SLTRP and SLRP, are well-motivated and provide a clear improvement over existing methods.\\n3. The paper demonstrates the effectiveness of the proposed methods on several event-based object and action recognition tasks.\",\n  \"Weaknesses\": \"1. The paper assumes that the SNNs used are already trained, which might not be the case in practice.\\n2. The paper does not discuss the computational cost of the proposed methods, which might be high due to the complexity of the relevance propagation techniques.\\n3. The paper does not provide a clear explanation of how the proposed methods can be extended to other neural network tasks beyond classification.\",\n  \"Questions\": \"1. How does the proposed method handle the case where the SNNs are not pre-trained, and what are the implications for the performance of the proposed method?\\n2."}
{"paper_id": "jHdz0CIS2y", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a novel approach to voice conversion by leveraging entangled speech representations and self-synthesized examples, which shows significant improvements over baseline models and prior work.\\n2. The method is competent across various languages and applicable to diverse tasks such as zero-shot conversion and cross-lingual voice conversion.\\n3. The paper is well-organized and clearly presented, with a good balance of technical and non-technical details.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more thorough discussion of the limitations of the self-supervised learning model and speaker verification model used in the approach.\\n2. The iterative refinement training strategy using self-synthesized examples may require more detailed explanation and evaluation.\\n3. The paper could provide more analysis on the impact of the entangled speech representations on the overall performance of the voice conversion system.\",\n  \"Questions\": \"1. How does the entangled speech representation affect the voice conversion performance, and what are the implications for future work?\\n2. Can the self-s"}
{"paper_id": "NkYCuGM7E2", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes an innovative approach to integrating Large Language Models (LLMs) into autonomous driving (AD) systems, leveraging commonsense reasoning and interpretable decision-making. The use of a cognitive pathway framework and guided parameter matrix adaptation shows promise in improving safety, efficiency, and adaptability in complex driving scenarios. The system's ability to outperform traditional methods in single and multi-vehicle tasks is a significant contribution.\",\n  \"Weaknesses\": \"One potential concern is the reliance on LLMs, which may not be robust to edge cases or rare events. The paper assumes that LLMs can generalize well to various driving scenarios, but this may not always be the case. The use of Model Predictive Control (MPC) systems for translating LLM decisions into actionable commands may also introduce additional complexity. The paper could benefit from more detailed analysis of the system's robustness and scalability.\",\n  \"Questions\": \"How do the authors plan to address the potential issues with LLMs in edge cases or rare events? Can the system be adapted to"}
{"paper_id": "RtDok9eS3s", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a significant simplification of the transformer block, which has been a cornerstone of various advancements in deep learning. The authors demonstrate that removing several components like skip connections, value and projection parameters, and normalization layers does not compromise training speed or performance. This work has the potential to lead to better resource efficiency in training and inference without sacrificing model capability. The experimental results are comprehensive, and the authors provide a clear comparison with standard transformer architectures.\",\n  \"Weaknesses\": \"While the authors demonstrate a significant simplification of the transformer block, the proposed modifications may not be universally applicable, and the impact of these changes on more complex models or tasks is not fully explored. The paper could benefit from a more detailed analysis of the theoretical foundations of signal propagation theory and its application to transformer architectures.\",\n  \"Questions\": \"How do the authors envision the application of these simplified transformer blocks to more complex models or tasks, such as vision transformer models or transformer-based language models? What are the potential limitations or challenges in scaling up these simplified blocks to larger models or"}
{"paper_id": "YCPDFfmkFr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel method for differentiating through infeasible QP layers in neural networks, which is a significant contribution to the field. The Extended Conservative Jacobian (ECJ) framework is a well-defined and elegant solution to the problem. The authors provide a clear and concise presentation of the method, and the code implementation is accessible through the QPLayer C++ package.\",\n  \"Weaknesses\": \"The paper assumes that the QP layers are convex, which might not always be the case in practice. The authors should provide more discussion on how their method handles non-convex QP layers. Additionally, the experimental results are mostly focused on a specific task (Sudoku solving), and it would be beneficial to see more diverse applications of the proposed method.\",\n  \"Questions\": \"How does the ECJ framework handle the case where the QP layer is non-convex? Can the authors provide more details on the computational efficiency of their method compared to existing approaches? What are the potential applications of this method beyond Sudoku solving?\""}
{"paper_id": "ueTdErd5Ib", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The discretization framework for contextual stochastic optimization is well-presented and has clear theoretical guarantees. The method demonstrates competitiveness in average regret and significantly improved robustness over existing methods. The experimental validation on a real-world electricity generation problem is a significant advantage.\",\n  \"Weaknesses\": \"The paper's main contribution is the discretization framework, but the framework itself may not be entirely novel. The authors could have explored other methods for discretization. Additionally, the assumption of a user-defined threshold for costs may be restrictive in real-world applications.\",\n  \"Questions\": \"How does the discretization framework handle cases where the uncertain objective function is non-convex? What is the computational complexity of the secondary optimization problem? Are there any potential issues with the assumption of a user-defined threshold, and how might this be addressed in future work?\"\n}"}
{"paper_id": "sDmjlpphdB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The Mixture-of-Prompts (MoP) framework is a well-structured approach to automatic prompting. It effectively leverages the strengths of both instructions and demos to enhance task performance. The division of the problem space into homogeneous regions managed by specialized experts is a clever idea. The two-phase process for assigning demos and instructions is well-explained and easy to follow. The experiments conducted on benchmark NLP tasks demonstrate the effectiveness of MoP, with improvements up to 43% compared to previous methods.\",\n  \"Weaknesses\": \"One potential limitation of MoP is its reliance on a two-phase process, which may be computationally expensive for large problem spaces. The division of the problem space into regions managed by different experts may not always be straightforward, especially for tasks with complex relationships between instructions and demos. The paper assumes that the clusters of demos are homogeneous, but in practice, this may not always be the case.\",\n  \"Questions\": \"How does MoP handle cases where the problem space is not well-divided into homogeneous regions?"}
{"paper_id": "Mdk7YP52V3", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a comprehensive theoretical framework for understanding the pathologies in overparameterized heteroskedastic regression models, which is novel and impactful. The authors' use of a field-theoretical approach derived from statistical physics is a strength of the paper. The empirical validation of the framework against real-world data using neural networks and numerical solutions to the derived equations is also a significant strength. The study's findings on phase transitions and the necessity for appropriate regularization are well-supported and have practical implications for model development.\",\n  \"Weaknesses\": \"The paper assumes a Gaussian likelihood model, which might not be suitable for all real-world scenarios. The authors should consider more general likelihood models or discuss the limitations of their assumptions. Additionally, the paper could benefit from more detailed explanations of the numerical methods used to solve the derived differential equations and the implications of the results for model selection and tuning.\",\n  \"Questions\": \"How do the authors think their framework can be extended to handle non-Gaussian likelihood models? Can the authors provide more details on the numerical methods used to solve"}
{"paper_id": "YGTSLDAPqb", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel approach to addressing the issue of out-of-distribution (OOD) performance in machine learning models by using targeted augmentations after self-supervised pretraining. The proposed 'Connect Later' framework demonstrates state-of-the-art results in multiple benchmarks. The study highlights the importance of accounting for known distribution shifts between source and target domains to improve OOD performance. The framework's ability to reduce OOD error metrics by 12% in one task and 11% in another is impressive and shows potential for real-world applications.\",\n  \"Weaknesses\": \"One potential limitation of the paper is that it relies on the assumption that the distribution shifts between source and target domains are known. In real-world scenarios, this may not always be the case. Additionally, the paper does not provide a clear explanation of how the targeted augmentations were crafted and validated. It would be beneficial to have more details on the design and evaluation of these augmentations.\",\n  \"Questions\": \"1. How were the targeted augmentations designed and validated in the paper?"}
{"paper_id": "JTcaziw7G1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel approach to private retrieval augmented generation (PRAG) for large language models (LLMs) using multi-party computation (MPC) techniques. The method is designed to ensure secure query transmission and retrieval from distributed servers without exposing sensitive information. The authors demonstrate the efficacy of PRAG through experiments on synthetic and real data, showcasing its potential for secure LLM applications.\",\n  \"Weaknesses\": \"The computational costs of the proposed method suggest a need for further optimization. The authors could provide more insights into the trade-off between privacy and efficiency. Additionally, the paper does not discuss the potential risks associated with the use of MPC techniques in real-world scenarios.\",\n  \"Questions\": \"How does the proposed method handle the scenario where multiple clients query the same database simultaneously? Does the method have any limitations in terms of scalability when dealing with large datasets or a large number of clients? The paper mentions that the method achieves sublinear communication complexity, but it would be helpful to see a more detailed analysis of this aspect.\"\n}"}
{"paper_id": "lEkFq4RUCX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed DDF metric effectively measures the difference between 3D surfaces of point clouds, improving accuracy and efficiency compared to existing metrics. This work has the potential to enhance 3D point cloud processing frameworks. The method is well-explained, and the experiments demonstrate its effectiveness across various tasks.\",\n  \"Weaknesses\": \"The paper could benefit from a more in-depth discussion of the limitations of the proposed metric, such as its sensitivity to noise or outliers in the point clouds. Additionally, the authors should provide more insight into the computational complexity of the DDF metric compared to existing methods.\",\n  \"Questions\": \"How does the DDF metric perform in scenarios with varying levels of noise or outliers in the point clouds? Are there any plans to extend the DDF metric to handle more complex 3D tasks, such as dynamic scenes or non-rigid transformations?\"\n}"}
{"paper_id": "Fq8tKtjACC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 3,\n  \"Strengths\": \"1. The paper provides a clear and well-structured hypothesis and supporting evidence for leveraging high-quality data to improve model performance. 2. The proposed method, phi-1, achieves state-of-the-art performance on HumanEval and MBPP benchmarks with a smaller model size. 3. The paper highlights the importance of data quality in model training and provides insights for future research.\",\n  \"Weaknesses\": \"1. The paper relies heavily on GPT-4 and GPT-3.5 for data filtering and generation, which might limit its generalizability. 2. The model's specialization in Python and sensitivity to prompt variations may limit its applicability to other programming languages and tasks. 3. The paper does not provide a thorough comparison with other high-quality data-based approaches.\",\n  \"Questions\": \"1. How does the phi-1 model perform on other programming languages and tasks? 2. What are the potential limitations of using GPT-4 and GPT-3.5 for data filtering and generation? 3"}
{"paper_id": "8fQlGQkj0S", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a clear and well-structured framework for understanding in-context learning (ICL) and task retrieval. The use of generative models to model pretraining data and in-context samples is an innovative approach. The numerical computations and experiments on Transformers validate the findings, providing a new theoretical framework for distinguishing between ICL and task retrieval modes.\",\n  \"Weaknesses\": \"The paper assumes a Gaussian mixture model for pretraining data, which may not capture the complexity of real-world data distributions. The risk upper bounds derived for ICL and task retrieval may not be tight in all cases. The paper could benefit from more thorough experiments on a wider range of tasks and models to validate the findings.\",\n  \"Questions\": \"How do the authors plan to extend their framework to handle out-of-distribution tasks or data with complex distributions? Can the authors provide more insights into the relationship between the number of in-context samples and the task learning risk? Are there any potential applications of this framework in real-world settings?\"\n}"}
{"paper_id": "hRos9WldRK", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method, L2B, is a well-designed and effective approach to learning with noisy labels. The use of a joint reweighting mechanism to adjust the importance between real and pseudo-labels is a novel and interesting idea. The empirical results on various datasets demonstrate the method's robustness against label noise. The method's ability to eliminate the dependency on a clean validation set is a significant advantage.\",\n  \"Weaknesses\": \"The paper relies heavily on existing techniques like meta-learning and pseudo-labeling, which might limit the novelty of the contribution. The experimental results are mostly based on a few well-known datasets, and it is unclear how the method performs on more diverse and challenging datasets. The method's sensitivity to the choice of hyperparameters and the validation set size is not thoroughly investigated.\",\n  \"Questions\": \"How does the method perform on datasets with varying levels of noise and label skew? What are the implications of using a meta-learning framework for learning with noisy labels? How does the method compare to other state-of-the-art methods for learning"}
{"paper_id": "ttRSBZiCiu", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed WaveFluid model presents a new direction in speech synthesis by leveraging a velocity field learned through adversarial training. This approach appears to efficiently synthesize high-fidelity audio with reduced computational overhead. The model's modular design and reparameterization techniques are effective in improving training speed and memory efficiency.\",\n  \"Weaknesses\": \"The paper lacks a detailed comparison with other state-of-the-art models in terms of computational complexity and memory requirements. The authors should provide more insights into the specific advantages of WaveFluid over existing methods in terms of computational efficiency. Additionally, the paper could benefit from a more thorough discussion on the limitations of the proposed approach, such as its potential vulnerability to mode collapse or the need for careful tuning of hyperparameters.\",\n  \"Questions\": \"How does the proposed velocity field learning approach compare to other methods for learning complex distributions in audio synthesis? Are there any potential applications of WaveFluid in other areas, such as music synthesis or audio editing? The authors should provide more details on the reparameterization techniques used to reduce the memory footprint and"}
{"paper_id": "WYsLU5TEEo", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a unified framework for generating counterfactuals that improve both interpretability and robustness of image classifiers.\\n2. The approach is novel and shows promising results on two datasets.\\n3. The paper provides a clear and well-structured presentation of the method and experiments.\",\n  \"Weaknesses\": \"1. The paper relies heavily on existing architectures for the GAN components, which might not be the most novel contribution.\\n2. The evaluation could be more comprehensive, including more datasets and metrics.\\n3. The paper does not provide a clear explanation of how the cycle-consistent loss term improves the results.\",\n  \"Questions\": \"1. How does the framework handle cases where the generator produces samples that are not counterfactuals, but rather random or uninformative samples?\\n2. Can the discriminator's output be used as a measure of prediction uncertainty in real-world applications?\\n3. How does the framework perform on more complex image classification tasks, such as object detection or segmentation?\"\n}"}
{"paper_id": "KKZaj2QS3G", "review": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 9, \"Confidence\": 4, \"Strengths\": \"The proposed CoInception framework effectively addresses the challenges of noise in time series data, providing a noise-resilient sampling strategy and a scalable and robust encoder design. The use of a hierarchical triplet loss to enforce noise resilience and alignment in embeddings is an interesting approach. The framework's ability to outperform existing state-of-the-art methods across multiple benchmarks for forecasting, classification, and anomaly detection is impressive.\\n\\nThe paper is well-organized and clearly presented, with a logical flow of ideas and a concise summary of the proposed method and its benefits.\", \"Weaknesses\": \"The paper assumes a certain level of knowledge about time series analysis and signal processing, which may make it less accessible to a broader audience. The authors could have provided more detailed explanations of the noise-resilient sampling strategy and the hierarchical triplet loss. Additionally, while the framework's performance is impressive, it would be beneficial to have more in-depth analysis of the effects of different types of noise on the performance of CoInception.\\n\\nThe related work section could be expanded to include more recent and relevant studies on"}
{"paper_id": "LWDRiFzbHQ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel approach to evaluating model generalization capacity on out-of-distribution test data without relying on ground truth labels. The proposed Vicinal Risk Proxy (VRP) method is well-motivated and effectively addresses the problem of spurious model responses. The experimental results demonstrate the effectiveness of VRP in improving the correlation with model accuracy, especially on severe OOD test sets.\",\n  \"Weaknesses\": \"The paper could benefit from more extensive experimental comparisons with existing methods, particularly those that have been specifically designed for OOD generalization evaluation. Additionally, the authors should provide more details about the similarity metric used in the Vicinal Risk Proxy calculation. The paper assumes a fixed similarity metric, but it is not clear how sensitive the results are to the choice of metric.\",\n  \"Questions\": \"How does the Vicinal Risk Proxy perform when applied to other types of models, such as those with different architectures or training objectives? Can the authors provide more insights into the interpretability of the Vicinal Risk Proxy, e.g., how does it"}
{"paper_id": "Yp01vcQSNl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper introduces a novel graph transformer architecture that effectively captures edge directionality using dual encodings and dynamic attention mechanisms. The proposed Directed Graph Transformer (DiGT) outperforms state-of-the-art models on directed graph datasets, demonstrating the importance of considering edge directionality in graph learning. The method's ability to incorporate edge channels as a bias and focusing on k-hop neighborhood localization is a significant contribution to the field. The exploration of alternative ways to incorporate directionality into existing graph transformer architectures is also valuable.\",\n  \"Weaknesses\": \"The paper assumes that the directed graph is represented as a sequence of nodes and edges, which may not be suitable for all applications. The reliance on a k-hop neighborhood localization may not be flexible enough to capture complex relationships in large graphs. The experiments on new directed graph datasets are limited to a single type of dataset, and it would be beneficial to evaluate the method on a diverse range of directed graph datasets.\",\n  \"Questions\": \"How does the DiGT architecture handle graphs with varying node and edge densities? Can the"}
{"paper_id": "LndMyiBl3n", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The authors propose a new approach for graph adversarial attacks, specifically targeting the restricted black-box attack (RBA) setting, which is a challenging problem. This is a significant contribution to the field of graph adversarial attacks.\\n2. The proposed method, SheAttack, uses a modified silhouette score (MSS) to evaluate the quality of node embeddings, which is a novel and effective approach.\\n3. The authors demonstrate the effectiveness of SheAttack in degrading GNNs across both homophilic and heterophilic graphs, without relying on prior label or model knowledge.\",\n  \"Weaknesses\": \"1. The authors assume that the attacker has access to the node features and graph structure, which might not be the case in real-world scenarios.\\n2. The proposed method relies on the Modified Silhouette Score (MSS), which might not be effective in all cases, especially when the graph has a complex structure.\\n3. The authors do not provide a comprehensive evaluation of the robustness of SheAttack against different types"}
