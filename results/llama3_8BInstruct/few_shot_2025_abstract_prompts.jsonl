{"paper_id": "u3xwwfHmBC", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach to capture spatio-temporal dependencies in traffic data through the use of three tense-specific attention modules.\\n2. The introduction of the Tri-Tense Former (TTformer) is clear and well-explained.\\n3. The paper provides a thorough comparison with existing models and demonstrates the effectiveness of TTformer in improving traffic forecasting accuracy.\",\n  \"Weaknesses\": \"1. The paper assumes that the traffic data is available for all three tenses (past, present, and future), which may not always be the case in real-world scenarios.\\n2. The authors do not provide a clear explanation of how the contrastive learning with negative filtering technique is implemented and how it improves the robustness of the model.\\n3. The paper could benefit from more detailed analysis of the results, including the impact of each tense-specific attention module on the overall performance of the model.\",\n  \"Questions\": \"1. How does the TTformer handle missing data in the traffic flow for each tense?\\n2."}
{"paper_id": "YKvBiRWdQC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper introduces a novel benchmark for studying reinforcement learning agents' zero-shot cooperation abilities in the Overcooked-AI environment, which is a significant contribution to the field. The authors provide a clear and well-structured presentation of the challenge and its evaluation. The paper is well-written and easy to follow. The use of state-of-the-art dual curriculum design (DCD) methods to generate auto-curricula for training general agents in Overcooked is an interesting approach. The authors' findings that state-of-the-art DCD algorithms fail to produce useful policies on this novel challenge are also noteworthy.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that it may be challenging for some readers to understand the Overcooked-AI environment and its relevance to real-world human-AI cooperation without prior knowledge. Additionally, the paper could benefit from more detailed explanations of the DCD methods used and their limitations. The authors may also want to consider providing more information about the potential applications of the OGC beyond the Overcooked-AI environment.\",\n  \"Questions\":"}
{"paper_id": "tePFpDgyqg", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a novel approach to addressing the data shortage issue for long context training by introducing a data pipeline called LongPack. The pipeline constructs long documents by packing shorter ones based on referral relationships, which is a crucial aspect for long-context training. The paper demonstrates the scalability and effectiveness of LongPack through experiments, showing a 32.7% higher score than previous state-of-the-art methods. The idea of using hyper-link connections as a native signal for referral relationships is also an interesting contribution.\",\n  \"Weaknesses\": \"The paper could be strengthened by providing more detailed analysis on the referral relationships and how they impact the quality of the constructed documents. Additionally, the evaluation of LongPack's performance on other datasets and tasks would provide a more comprehensive understanding of its generalizability.\",\n  \"Questions\": \"How does the paper define the quality of the constructed documents, and what metrics are used to evaluate it? Are there any plans to release the LongPack dataset or make it publicly available for other researchers to use?\"\n}"}
{"paper_id": "gx3LMRB15C", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel augmentation-free self-supervised learning method for tabular data, T-JEPA, which is well-suited for pre-training deep classifiers. The use of Joint Embedding Predictive Architecture (JEPA) and regularization tokens is a good idea. The experimental results demonstrate a substantial improvement in both classification and regression tasks, outperforming models trained directly on samples in their original data space. The characterization of the obtained representations and the identification of relevant features for downstream tasks without access to labels is also a significant contribution.\",\n  \"Weaknesses\": \"The paper assumes that the data is tabular, which might limit its applicability. The choice of regularization tokens is not thoroughly justified. The experimental results are mostly based on a single dataset, which raises concerns about the generalizability of the proposed method. The paper does not provide a clear explanation of how the regularization tokens are used in the training process.\",\n  \"Questions\": \"How does the proposed method compare to other augmentation-free self-supervised learning methods for tabular data? Is"}
{"paper_id": "kTH3bEH6hW", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel augmentation method for contrastive learning in tabular domains, which is effective for few-shot classification. This is a significant contribution to the field.\\n2. The authors introduce a new benchmark, FeSTa, which consists of 42 tabular datasets and 31 algorithms. This is a valuable resource for the community.\\n3. The paper presents a comprehensive evaluation of the proposed method, including comparisons with various existing approaches.\",\n  \"Weaknesses\": \"1. The paper assumes that the feature-specific ranges are known, which may not always be the case in real-world scenarios.\\n2. The authors do not provide a detailed explanation of how the ranges are determined.\\n3. The paper relies heavily on the FeSTa benchmark, which may be biased towards the proposed method.\",\n  \"Questions\": \"1. How do the authors determine the feature-specific ranges for the augmentation method?\\n2. Can the proposed method be extended to other domains beyond tabular data?\\n3. How does the FeST"}
{"paper_id": "EqcLAU6gyU", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a comprehensive analysis of the design principles of agent-oriented planning, including solvability, completeness, and non-redundancy, which is a significant contribution to the field.\\n2. The proposed framework for agent-oriented planning is well-structured and easy to follow, with a clear explanation of the fast task decomposition and allocation process, as well as the evaluation via a reward model.\\n3. The paper provides extensive experiments to demonstrate the effectiveness of the proposed framework in solving real-world problems.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed explanation of the reward model used in the evaluation process.\\n2. The authors could provide more information about the specific expert agents used in the experiments and how they were selected.\\n3. The paper assumes a centralized architecture for the meta-agent, which may not be suitable for all multi-agent systems.\",\n  \"Questions\": \"1. How does the proposed framework handle conflicts between the sub-tasks allocated to different agents?\\n2. Can the framework"}
{"paper_id": "qrTrnrEi9d", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a well-designed framework, TransFusion, which enables the use of English translations of low-resource language data for fine-tuning, leading to more precise predictions through annotation fusion.\\n2. The introduction of GoLLIE-TF, a cross-lingual instruction-tuned LLM for IE tasks, demonstrates a significant improvement in cross-lingual transfer over the base model.\\n3. The experiments across twelve multilingual IE datasets spanning 50 languages showcase the effectiveness of the proposed approach.\\n4. The use of TransFusion with GPT-4, a proprietary model, and the improvement in low-resource language named entity recognition, is a notable contribution.\",\n  \"Weaknesses\": \"1. The paper assumes that English translations of low-resource language data are available, which may not always be the case.\\n2. The evaluation of the proposed approach is limited to twelve multilingual IE datasets, and it is unclear how well the approach would generalize to other languages or domains.\\n3. The comparison with other"}
{"paper_id": "Bp0HBaMNRl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel differentiable causal discovery algorithm for nonlinear latent hierarchical models, which is a significant contribution to the field. The theoretical results on identifiability are also valuable. The experiments demonstrate the practical utility of the approach, and the scalability of the algorithm is impressive.\",\n  \"Weaknesses\": \"The paper assumes that the latent variables are continuous, which might not be the case in all scenarios. The algorithm's performance on high-dimensional data is impressive, but it's not clear how it would perform on data with a large number of latent variables. The paper could benefit from a more detailed discussion on the assumptions and limitations of the approach.\",\n  \"Questions\": \"How does the algorithm handle cases where the latent variables are not continuous? What is the computational cost of the algorithm for large numbers of latent variables? Are there any other assumptions or constraints that the algorithm relies on, and if so, how do they affect the performance?\"\n}"}
{"paper_id": "JzLcKWtGnl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a well-designed approach to enhance spatial perception and reasoning for 3D vision-language tasks. The incorporation of a progressive spatial awareness scheme and the introduction of two novel tasks are notable contributions. The experimental results demonstrate state-of-the-art performance across a range of 3D vision-language tasks. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"One potential weakness is the reliance on a specific dataset, MODEL, for evaluation. It would be beneficial to include results from other datasets to ensure generalizability. Additionally, the paper could benefit from a more in-depth discussion of the limitations and potential applications of the proposed approach.\",\n  \"Questions\": \"How does the progressive spatial awareness scheme compare to other spatial reasoning approaches in the literature? Are there any potential issues with the assumption that the perception field expands uniformly? What are the implications of using a specific dataset, MODEL, for evaluation, and how does this impact the generalizability of the results?\"\n}"}
{"paper_id": "hXJrQWIoR3", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel framework for learning and explaining graph representations through graph pattern analysis, which is a significant contribution to the field of explainable graph learning.\\n2. The authors provide a clear and concise explanation of the method and its theoretical analyses.\\n3. The experiments demonstrate the effectiveness of the proposed method in both supervised and unsupervised learning tasks.\",\n  \"Weaknesses\": \"1. The paper assumes that the graph patterns are known in advance, which may not be the case in real-world scenarios.\\n2. The method may not be scalable to large graphs due to the complexity of the pattern counting vector.\\n3. The authors do not provide a clear explanation of how to select the weights for the weighted sum of the graph patterns.\",\n  \"Questions\": \"1. How does the method handle graphs with varying numbers of nodes and edges?\\n2. Can the method be extended to handle graphs with different types of edges (e.g., directed vs. undirected)?\\n3. How does the method compare to"}
{"paper_id": "67sSPPAZiG", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a comprehensive approach to egocentric video understanding, addressing data scarcity by generating QA samples and creating a challenging benchmark. The novel 'Memory Pointer Prompting' mechanism and de-biasing evaluation method are significant contributions. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The paper relies heavily on the quality of the generated QA samples, which may introduce bias. The evaluation of the model's performance is limited to a single benchmark, and the comparison to other state-of-the-art models is lacking. The paper assumes the availability of human-annotated data for generating QA samples, which may not be feasible in all scenarios.\",\n  \"Questions\": \"How does the paper ensure the quality and diversity of the generated QA samples? Can the de-biasing evaluation method be applied to other multimodal models? What is the computational cost of the 'Memory Pointer Prompting' mechanism, and how does it compare to other architectures?\"\n}"}
{"paper_id": "kjmLabjSE2", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors provide a thorough and rigorous analysis of the differential privacy guarantee of hidden-state Noisy-SGD algorithms, addressing an important open question in the field. The paper is well-written and the results are clearly presented. The use of Holder continuous gradient is a nice touch, and the improvement over previous results is significant. The analysis relies on the improvement of shifted divergence analysis, which is a valuable contribution to the field.\",\n  \"Weaknesses\": \"The paper assumes that the losses have Holder continuous gradient, which might be a strong assumption in some cases. The analysis is focused on non-convex non-smooth losses, and it would be interesting to see how the results generalize to other types of losses. The paper could benefit from more discussion on the implications of the results and how they relate to practical applications of differential privacy.\",\n  \"Questions\": \"What are the implications of the results for the design of Noisy-SGD algorithms in practice? How do the assumptions of Holder continuous gradient affect the applicability of the results? Are there any other types"}
{"paper_id": "I18MA5DjoP", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The Neuron Predictability Lens (NPL) framework is an innovative and timely contribution to the field of transformer-based large language models. The authors provide a clear and well-organized presentation of their work, making it easy to follow and understand. The concept of Neuron Predictability is well-motivated and has the potential to shed light on the internal workings of LLMs. The experiments conducted on LLaMA-2 and GPT-J demonstrate the effectiveness of NPL in analyzing and understanding transformer-based models.\",\n  \"Weaknesses\": \"The paper assumes a good understanding of transformer-based models and their architectures, which may limit its accessibility to a broader audience. The concept of Neuron Predictability is not entirely new, as similar ideas have been explored in the context of neural networks. The authors could have provided more detailed comparisons with existing works in this area. Additionally, the paper could benefit from more extensive experimentation and evaluation on a wider range of models and tasks.\",\n  \"Questions\": \"How does NPL compare to existing methods for analyzing"}
{"paper_id": "qpDqO7qa3R", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method is well-designed and demonstrates excellent generalization across various degradation types and datasets. The use of a hierarchical latent warping strategy and token merging with a hybrid correspondence mechanism is innovative and effective. The paper provides a thorough evaluation with quantitative metrics and visual comparisons, showcasing the superiority of the proposed method over traditional video restoration methods. The fact that the method works with any 2D restoration diffusion model is a significant advantage.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion on the choice of hyperparameters for the hybrid correspondence mechanism. Additionally, the evaluation on more diverse datasets and degradation types would strengthen the claims of the paper. The paper assumes that the pre-trained image restoration diffusion model is available, but it would be helpful to discuss the implications of not having access to such a model.\",\n  \"Questions\": \"How does the proposed method handle cases where the pre-trained image restoration diffusion model is not available or is not suitable for the specific video restoration task? Are there any plans to explore the use of other types of correspondence"}
{"paper_id": "dSQtMx6dPE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel and well-motivated approach to address privacy concerns in graph prompt learning. The experimental evaluation is comprehensive and demonstrates the effectiveness of the proposed algorithms. The use of differential privacy guarantees is a significant contribution to the field.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion on the limitations of the proposed approach, such as the potential impact of the small number of sensitive data points used to learn the prompts. Additionally, the comparison to other existing methods could be more thorough.\",\n  \"Questions\": \"How does the proposed approach handle the case where the pre-trained GNN model is not available? What are the potential implications of using a fixed number of iterations for the PATE framework?\"\n}"}
{"paper_id": "GbgCRJedQ7", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors have proposed a novel method for reducing the performance gap between PEFT and FT while also reducing computational and memory costs. The experiments demonstrate the effectiveness of SMT in surpassing other PEFT baselines and reducing GPU memory footprint by 67% compared to FT. The method's ability to avoid the performance decline seen in LoRA and DoRA as the number of trainable parameters increases is a significant contribution.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the motivation behind selecting the most significant sub-matrices in the gradient update. Additionally, the authors could provide more information about the specific tasks and datasets used in the experiments to better understand the generalizability of the results. The comparison with other PEFT methods could be more comprehensive, including a discussion of the trade-offs between computational cost and accuracy.\",\n  \"Questions\": \"How does the selection of the most significant sub-matrices impact the performance of SMT, and are there any specific heuristics or criteria used for this selection? Are there any"}
{"paper_id": "ZZVOrId3yN", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a comprehensive and rigorous mathematical analysis of CrossModalNet, providing a solid foundation for future research in multimodal learning and domain adaptation. The introduction of the Cross-Modal Information Flow (CMIF) metric and the Joint Adversarial Domain Adaptation (JADA) framework are significant contributions to the field. The experimental results on the MM-WHS dataset demonstrate the superiority of CrossModalNet.\",\n  \"Weaknesses\": \"The paper assumes that the multimodal data streams are available, but it does not discuss how to handle cases where the data is not available or is incomplete. Additionally, the paper does not provide a clear explanation of how the CMIF metric is used to justify the progressive integration of multimodal information through the network layers.\",\n  \"Questions\": \"How does the CrossModalNet architecture handle cases where the multimodal data streams are not available or are incomplete? Can the CMIF metric be used to justify the progressive integration of multimodal information through the network layers in a more explicit way?\"\n}"}
{"paper_id": "gLHuAYGs6a", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel structural multi-view clustering network via heterogeneous random walks, which is an excellent contribution to the field of multi-view clustering. The proposed method is well-designed and effectively leverages information from multiple views to enhance clustering performance. The use of a unified sample-level structure is a good idea, and the multi-step random walk strategy is a good approach to explore high-order sample structures across various views. The experimental results on five real-world datasets demonstrate the superiority of the proposed method.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that it may be difficult to extend the proposed method to handle more complex and large-scale datasets. Additionally, the paper could benefit from a more thorough analysis of the computational complexity of the proposed method, especially for large-scale datasets.\",\n  \"Questions\": \"How does the proposed method handle the case where the number of views is very large? Are there any plans to extend the proposed method to handle more complex and large-scale datasets? How does the proposed method compare to other state-of-the-art methods in terms"}
{"paper_id": "x5l5PRtvul", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a comprehensive analysis of the hybrid Stein variational gradient descent (h-SVGD) algorithm, including its ability to alleviate variance collapse without incurring additional computational cost. The theoretical results, including the existence of a solution to the hybrid Stein partial differential equation and a descent lemma, are well-established. The special case where h-SVGD is a kernelised Wasserstein gradient flow on a functional other than the Kullback-Leibler divergence is also interesting. The experimental results demonstrate the competitiveness of h-SVGD on high-dimensional inference tasks while alleviating variance collapse.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanations of the theoretical results, particularly the descent lemma and the large particle limit result. Additionally, the paper assumes a certain level of background knowledge in the field of variational inference, which may make it challenging for readers without a strong background in the area to fully understand the contributions. The experiments could also be more comprehensive, including more baselines and a more detailed analysis of the results.\",\n  \"Questions"}
{"paper_id": "pOUAVXnOQP", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel activation function that addresses the spectral bias issue in ReLU networks, which is a well-motivated and timely contribution. The experiments demonstrate the effectiveness of STAF in improving the reconstruction quality and training efficiency of continuous signals. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that the target signal has a periodic nature, which may not be the case for all signals. The authors should discuss the limitations of their approach and potential applications where STAF may not be effective. Additionally, the comparison with other state-of-the-art networks is limited to a few examples, and more comprehensive evaluation would strengthen the paper.\",\n  \"Questions\": \"How does the authors' approach to modeling the sinusoidal activation function compare to other sinusoidal-based activation functions, such as the ones used in SIREN? What are the computational costs associated with training and evaluating STAF, and how do they compare to ReLU networks?\"\n}"}
{"paper_id": "iN64nSYt0z", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a simple and effective method, GUIDE, to increase attention scores in instruction tokens, which improves the accuracy of following certain instructions. The Influence metric is a useful tool for understanding how user instructions propagate with transformer layers and impact the LLM output. The results show significant improvements over natural prompting alternatives.\",\n  \"Weaknesses\": \"The paper relies on a limited set of experiments and datasets, which may not be representative of all possible scenarios. The evaluation metrics used are not fully clear, and the comparison with other methods is not comprehensive. The authors should provide more details about the Influence metric and how it is calculated.\",\n  \"Questions\": \"How does GUIDE handle cases where the user instructions are ambiguous or conflicting? Can the Influence metric be used to analyze the propagation of user instructions in other types of models, not just transformer-based LLMs?\"\n}"}
{"paper_id": "gRuZkEy49k", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The connection between GFlowNets and maximum-entropy RL is well-established and leveraged to adapt MCTS for GFlowNet training. The proposed method is well-structured and easy to follow. The paper provides a clear and concise presentation of the results and their implications.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanations of the theoretical results and their implications. The experiments could be more comprehensive, including more domains and environments. The discussion of the practical strategies for employing MCTS as a sampling tool could be more in-depth.\",\n  \"Questions\": \"How does the proposed method generalize to other types of generative models beyond GFlowNets? What are the computational requirements for the MCTS algorithm in practice, and how do they compare to other sampling methods?\"\n}"}
{"paper_id": "d23EVDRJ6g", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel approach to motion generation using a localized masked modeling paradigm, which is well-suited for animation tasks where large datasets are not available. The proposed method, MotionDreamer, effectively learns motion internal patterns from a given motion and generates diverse yet natural animations. The paper demonstrates the effectiveness of MotionDreamer through comprehensive experiments and showcases its ability to perform downstream tasks such as temporal motion editing and crowd motion synthesis. The implementation, learned models, and results are to be made publicly available upon paper acceptance, which is a significant contribution to the research community.\",\n  \"Weaknesses\": \"The paper assumes that the given motion has arbitrary topology and duration, which may not be the case in all animation tasks. The authors should provide more details on how MotionDreamer handles motions with complex topologies or varying durations. Additionally, the paper does not provide a thorough comparison with other state-of-the-art methods that are not GAN or Diffusion-based, which may limit the generalizability of the results.\",\n  \"Questions\": \"How does Motion"}
{"paper_id": "kQ5s9Yh0WI", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a clear explanation of the problem and proposes a novel solution to the issue of long context LLMs. The introduction of AgentWrite and LongWriter-6k dataset is a significant contribution to the field. The results show that the proposed method can successfully scale the output length of existing models to over 10,000 words while maintaining output quality. The development of LongBench-Write is also a valuable addition to the research community.\",\n  \"Weaknesses\": \"The paper assumes that the scarcity of long-output examples in existing SFT datasets is the primary reason for the model's output limitation. However, it would be more convincing if the authors could provide more evidence to support this claim. Additionally, the paper could benefit from a more thorough analysis of the limitations of AgentWrite and LongWriter-6k dataset.\",\n  \"Questions\": \"How does the AgentWrite pipeline handle the potential loss of contextual information when decomposing ultra-long generation tasks into subtasks? Can the authors provide more details about the DPO technique used to improve the"}
{"paper_id": "9CqkpQExe2", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed Ada-K routing strategy is a significant improvement over conventional Top-K routing, achieving a good balance between computational efficiency and model performance. The use of learnable and lightweight allocator modules is a novel and effective approach. The experimental results demonstrate the superiority of Ada-K routing, with notable reductions in FLOPs and inference speedup while maintaining performance. The training efficiency of Ada-K is also impressive, especially for large models like Mixtral-8x22B. The publicly available training code and model checkpoints are a significant contribution to the research community.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed analysis of the allocator modules, such as their architecture and training process. The experimental results are mostly focused on the performance of Ada-K routing, and it would be beneficial to have more information on the allocator modules' impact on the overall model performance. Additionally, the paper assumes that the allocator modules are fully pluggable, but it would be helpful to provide more information on how to integrate them with different MoE-based LLMs"}
{"paper_id": "PpYy0dR3Qw", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper introduces a novel algorithm, LoCoDL, that combines local training and compression to reduce communication complexity in distributed optimization and federated learning. The theoretical analysis is sound and the experimental results demonstrate the effectiveness of the proposed method.\",\n  \"Weaknesses\": \"The paper assumes a strongly convex function, which might not be the case in all scenarios. The authors should discuss the potential limitations of their approach in non-convex cases. Additionally, the paper could benefit from more detailed experimental results, such as a more comprehensive comparison with other state-of-the-art methods.\",\n  \"Questions\": \"How does LoCoDL handle non-convex functions? What are the implications of using local training and compression in such cases? Can the authors provide more details about the compressor used in the experiments?\"\n}"}
{"paper_id": "UatDdAlr2x", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a clear and well-motivated investigation of the effects of architectural design choices on the solutions that a transformer can implement and learn. The histogram task is a good choice for this study, and the analysis reveals a rich phenomenology. The characterization of two different counting strategies that small transformers can implement theoretically is a significant contribution. The emergence of either strategy is heavily influenced by subtle synergies among hyperparameters and components, which is a key insight. The findings highlight that even in simple settings, slight variations in model design can cause significant changes to the solutions a transformer learns.\",\n  \"Weaknesses\": \"The paper is well-written and easy to follow, but some of the notation and terminology used in the abstract and introduction could be clarified. The connection to other works in the field is not explicitly stated, which might make it difficult for readers to understand the significance of the findings. The study is limited to a specific task (histogram task) and a specific type of model (transformer), which might limit the generalizability of the results"}
{"paper_id": "IwgmgidYPS", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The dataset is comprehensive and large-scale, covering over 25 million images across 10 modalities. The multigranular annotations are a significant strength, encompassing both global and local information. The automated pipeline for generating visual and textual annotations is innovative and scalable. The LLaVA-Tri model achieves state-of-the-art performance on several multimodal tasks. The dataset and model can support large-scale pre-training of multimodal medical AI models.\",\n  \"Weaknesses\": \"The reliance on domain-specific expert models for ROI identification may introduce bias. The use of a single large language model (LLaVA) for retrieval-augmented generation may limit the diversity of generated descriptions. The evaluation metrics and baselines could be more comprehensive. The paper could benefit from more discussion on the potential limitations and challenges of the dataset and model.\",\n  \"Questions\": \"How does the automated pipeline handle cases where the domain-specific expert models are uncertain or disagree? What are the potential risks of relying on a single large language model for retrieval-augmented generation? How"}
{"paper_id": "WG7GzGx3G9", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method, Rotated Runtime Smooth (RRS), is well-designed and effective in addressing the issue of outliers in activations for large language models. The authors' observation that outliers can be classified into channel-wise and spike outliers is insightful. The introduction of Runtime Smooth (RS) and the Rotation operation is a good approach to eliminate channel-wise outliers and narrow the gap between spike outliers and normal values, respectively. The experimental results show that RRS outperforms the state-of-the-art method and improves WikiText-2 perplexity significantly.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanations of the Rotation operation and its effect on spike outliers. Additionally, the authors should provide more information on the trade-off between accuracy and latency in the proposed method. It would also be helpful to compare the proposed method with other existing approaches that address outliers in activations, such as those that separate outliers and normal values into two matrices or migrate outliers from activations to weights.\",\n  \"Questions\": \"How does the Rotation operation specifically narrow the gap between spike"}
{"paper_id": "BhECSDSkAE", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper introduces a novel approach to one-shot medical video object segmentation using static image datasets. The proposed framework is well-structured and easy to follow. The authors provide a comprehensive evaluation of their method on the OS-I2V-Seg dataset, demonstrating its efficacy in the low-data regime.\",\n  \"Weaknesses\": \"The paper assumes that the static images are available for training, which may not always be the case in real-world scenarios. The method relies heavily on the quality of the pre-trained one-shot segmentation model, which may not generalize well to all medical video datasets.\",\n  \"Questions\": \"How does the proposed method handle cases where the static images are not available or are of poor quality? What are the limitations of the OS-I2V-Seg dataset, and how does it compare to other medical video datasets?\"\n}"}
{"paper_id": "mnB4hDTIDr", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel diversity evaluation method, DCScore, for LLM-generated datasets from a classification perspective. The method treats diversity evaluation as a sample classification task, considering mutual relationships among samples. The theoretical verification of the diversity-related axioms satisfied by DCScore demonstrates its principled nature. The method enjoys lower computational costs compared to existing methods.\",\n  \"Weaknesses\": \"The paper could benefit from a more thorough discussion of the limitations of LLM-generated datasets and how DCScore addresses these limitations. The experimental results are limited to a single evaluation metric, and it would be beneficial to explore other metrics to validate the effectiveness of DCScore. Additionally, the paper could provide more insights into the potential applications of DCScore in real-world scenarios.\",\n  \"Questions\": \"How does DCScore handle cases where the LLM-generated dataset has a large number of samples with similar characteristics? Can DCScore be adapted for other types of datasets, such as image or video datasets?\"\n}"}
{"paper_id": "Y0qmwm6tgy", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The authors identify a significant issue with existing model pruning methods, which is the sensitivity to weight perturbations.\\n2. The proposed method, MoreauPruner, is based on the Moreau envelope and is shown to be robust against weight perturbations.\\n3. The evaluation on several large language models is extensive and comprehensive.\\n4. The use of $\\ell_1$-norm regularization techniques is a good approach to induce sparsity in the pruning task.\",\n  \"Weaknesses\": \"1. The authors could have provided more detailed analysis on the Moreau envelope and its relation to the neural network's behavior.\\n2. The evaluation on the robustness of MoreauPruner is mostly based on numerical results, and more visualizations or qualitative analysis would be helpful.\\n3. The paper could have benefited from a more detailed comparison with existing pruning methods, especially in terms of their robustness against weight perturbations.\",\n  \"Questions\": \"1. How does the Moreau envelope relate to the neural network"}
{"paper_id": "aAcOaJYbUg", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant issue in the field of OOD robustness, which is becoming increasingly important with the advent of web-scale datasets.\\n2. The proposed LAION-C benchmark is well-designed and comprehensive, with six novel distortion types and five severity levels.\\n3. The psychophysical experiment is a great addition, providing a comparison of models to lab-quality human robustness data.\\n4. The paper presents a paradigm shift in OOD generalization, with the best models now matching or outperforming the best human observers.\",\n  \"Weaknesses\": \"1. The paper assumes that the ImageNet-C corruption types are no longer OOD relative to today's large datasets, but this assumption might not be universally true.\\n2. The paper does not provide a clear explanation of how the LAION-C dataset was created and validated.\\n3. The paper assumes that the best models will be able to outperform humans in OOD generalization, but this might not be the case in all scenarios.\",\n  \"Questions\":"}
{"paper_id": "duGygkA3QR", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents an original and interesting approach to GNNs by connecting them to Koopman theory and DMD. The idea of using DMD to estimate a low-rank, finite-dimensional linear operator is well-motivated and has the potential to capture complex dynamics within the graph accurately and efficiently. The theoretical connection between the DMD-estimated operator and the original dynamic operator is well-established. The introduction of DMD-GNN models is a natural extension of the DMD-estimated operator, and the discussion on incorporating domain-specific constraints is a nice addition. The extensive experiments on various learning tasks demonstrate the effectiveness of the proposed approach.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that the authors do not provide a thorough comparison with existing GNN architectures, such as GraphSAGE or GCN, to demonstrate the superiority of their approach. Additionally, the paper assumes that the DMD-estimated operator is a good approximation of the original dynamic operator, but it is not clear how to handle cases where this assumption is violated."}
{"paper_id": "s6nYndMwG7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors provide a clear and concise explanation of the limitations of the traditional approach to influence functions and propose a novel method to choose the hyperparameters for LiSSA. The use of random sketching to evaluate the spectral properties of the Hessian is an interesting and practical approach. The comparison to PBRF is also a good addition. The writing is clear and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that the Hessian is positive definite, which might not always be the case. The authors should discuss the implications of this assumption. The paper could benefit from more experimental results, such as comparisons to other methods or more detailed analysis of the trade-offs between the different hyperparameters.\",\n  \"Questions\": \"How does the choice of scaling factor affect the convergence of LiSSA? Is there a way to adapt the proposed method to handle non-positive definite Hessians? What are the computational costs of the random sketching approach compared to other methods?\"\n}"}
{"paper_id": "9BVMD3keG8", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a thorough analysis of the online learning problem of brokerage between traders, considering the role of contextual information. The problem is well-formulated and the results are solid. The authors provide a clear and concise presentation of their findings.\\n2. The paper provides a good overview of the existing literature on online learning and the brokerage problem, and the authors' contributions are well-motivated and clearly explained.\\n3. The authors provide a well-structured and well-written paper, with clear and concise language.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed discussion of the implications of the results, particularly in terms of the potential applications and limitations of the algorithms presented.\\n2. The authors could provide more insight into the practicality of the algorithms, including their computational complexity and scalability.\\n3. The paper could benefit from a more thorough comparison with existing algorithms and methods for the brokerage problem.\",\n  \"Questions\": \"1. How do the authors plan to extend their results to more complex scenarios,"}
{"paper_id": "mnna9LUg7P", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed method for SSM quantization is novel and effectively addresses the issue of sensitive feature maps within the selective scan mechanism and massive outliers in the output activations.\\n2. The experimental results demonstrate the effectiveness and practical applicability of the approach for deploying SSM-based models of all sizes on both cloud and edge platforms.\\n3. The use of Hadamard transform to quantize the output activations in an outlier-free space is a good idea.\",\n  \"Weaknesses\": \"1. The paper assumes that the SSM model is already trained, but it does not provide any information on how to train the SSM model with quantization.\\n2. The paper only evaluates the proposed method on two models (Mamba 2.8B and Jamba 52B) and it would be good to see more experiments on other models.\\n3. The paper does not discuss the trade-off between accuracy and latency, and how to adjust the quantization parameters to achieve the best trade-off.\",\n  \"Questions\": \"1."}
{"paper_id": "3X3LuwzZrl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a comprehensive analysis of the influence correlations between labels in GNNs, which is a significant contribution to the field of multi-label node classification. The proposed Label Influence Propagation (LIP) model is well-motivated and effectively addresses the limitations of existing approaches. The experimental results demonstrate the superiority of LIP over state-of-the-art methods. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that the label influence graph is a complete graph, which might not be the case in real-world scenarios. The authors should discuss the limitations of this assumption and provide some insights on how to handle incomplete label influence graphs. Additionally, the paper could benefit from a more thorough discussion on the computational complexity of the proposed method, especially for large-scale graphs.\",\n  \"Questions\": \"How does the proposed method handle the case where the label influence graph is incomplete or contains missing edges? Are there any plans to extend the method to handle more complex graph structures, such as directed or weighted graphs? Can the authors"}
{"paper_id": "IqaQZ1Jdky", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed framework, VBn-KAN, is well-motivated and leverages the Weierstrass approximation theorem to extend function basis within KANs. The use of Bernstein polynomials for their robustness is a good choice. The comprehensive experiments across three fields demonstrate the effectiveness of the method. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes a good understanding of the underlying mathematics, which might make it challenging for some readers. The use of Bernstein polynomials might not be the only choice for robustness, and alternative options could be explored. The paper could benefit from a more detailed discussion on the choice of the function basis and its impact on the results.\",\n  \"Questions\": \"How does the choice of the function basis affect the performance of the model? Are there any limitations or potential issues with using Bernstein polynomials for robustness? Can the framework be extended to other types of neural networks or applications?\"\n}"}
{"paper_id": "OdMqKszKSd", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses a challenging problem in 3D object creation and provides a novel solution using a diffusion model and part connectivity graph.\\n2. The method is well-structured and follows a clear pipeline from high-level structure to geometric details.\\n3. The experiments demonstrate the effectiveness of the proposed method in generating realistic and realistic articulated objects.\\n4. The paper provides a good balance between theoretical contributions and experimental results.\",\n  \"Weaknesses\": \"1. The paper assumes that the object is in a resting state, which might not be the case in real-world scenarios.\\n2. The method requires a large amount of training data and computational resources.\\n3. The paper does not provide a clear comparison with other state-of-the-art methods in terms of scalability and practicality.\\n4. The authors do not discuss the limitations of the method and potential future work.\",\n  \"Questions\": \"1. How does the method handle cases where the object is not in a resting state?\\n2. Can the method be extended to handle"}
{"paper_id": "LOBhVTtVnc", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel and well-structured approach to leveraging Transformers for physical dynamics simulation, which is a significant contribution to the field.\\n2. The use of equivariant spatiotemporal blocks and the Temporal Difference Graph (TDG) mechanism is innovative and effectively addresses the challenges of long-term prediction tasks.\\n3. The paper demonstrates state-of-the-art performance across multiple challenging physical systems at various scales, showcasing the robust dynamics simulation capabilities of GSTs.\",\n  \"Weaknesses\": \"1. The paper assumes that the input data is already pre-processed and formatted into a spatiotemporal graph, which may not be the case in real-world applications.\\n2. The authors do not provide a detailed analysis of the computational complexity of the proposed method and its scalability to large-scale systems.\\n3. The paper relies heavily on the performance of the underlying Transformer architecture, which may not be robust to noise or missing data.\",\n  \"Questions\": \"1. How does the proposed method handle cases where the input"}
{"paper_id": "oa5UeyUVMm", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors provide a novel and well-motivated approach to applying diffusion probabilistic models to graph representation learning, which is a promising direction for future research. The theoretical foundations of the approach are well-established and the empirical results are convincing. The authors also provide a clear and well-organized presentation of the paper.\",\n  \"Weaknesses\": \"One potential concern is that the authors do not provide a detailed comparison with other state-of-the-art methods for graph representation learning. While they mention that Graffe achieves state-of-the-art performance on 9 of the 11 real-world datasets, it would be helpful to have a more comprehensive comparison with other methods. Additionally, the authors do not discuss the potential limitations of their approach, such as the computational cost of training the diffusion model.\",\n  \"Questions\": \"1. Can the authors provide more details on the computational cost of training the diffusion model and how it compares to other methods for graph representation learning? 2. How does the authors' approach handle the issue of over-smoothing in graph neural networks,"}
{"paper_id": "5swfKRkCx7", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed approach of using a memory-based mechanism to dynamically control the degree of knowledge integration with the extra head is novel and effective.\\n2. The experiments on both general and specific-domain QA tasks demonstrate the effectiveness of the approach.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper assumes that the LLM has strong knowledge comprehension and reasoning capabilities, which may not always be the case.\\n2. The approach relies on the quality of the retrieved knowledge, which can be a challenge in certain domains.\\n3. The paper could benefit from more detailed explanations of the memory-based mechanism and its components.\",\n  \"Questions\": \"1. How does the memory-based mechanism handle cases where the retrieved knowledge is inconsistent or conflicting?\\n2. Can the approach be applied to other types of tasks beyond QA, such as text classification or machine translation?\\n3. What are the computational requirements of the memory-based mechanism, and how does it impact the overall efficiency of the approach?\"\n}"}
{"paper_id": "5s1qpjrNvZ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel approach to calculate the sampling rate of the guide policy to guarantee that the mean return of the learning policy will surpass a user-defined performance degradation threshold.\\n2. The proposed algorithm is simple to implement on top of existing algorithms, robust to hyperparameter choices, and effective in warm-starting online learning.\\n3. The paper introduces a roll-back feature in guided RL with roll-back (GRL-RB) to adaptively balance the trade-off between performance degradation and rapid transfer of control to the learner.\",\n  \"Weaknesses\": \"1. The assumptions made in the paper may not hold in all scenarios, which could limit the applicability of the proposed approach.\\n2. The paper does not provide a thorough comparison with other guided RL methods, which makes it difficult to assess the performance of the proposed approach.\\n3. The roll-back feature may not be effective in all cases, and more experimentation is needed to fully understand its benefits and limitations.\",\n  \"Questions\": \"1. How does the proposed approach"}
{"paper_id": "WNb4P8aG66", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed Diffusion on Diffusion (DoD) framework is an innovative multi-stage generation approach that addresses the limitation of conventional class-guided diffusion models in generating texture details.\\n2. The introduction of a latent embedding module that employs a compression-reconstruction approach to discard redundant detail information from the conditional samples is a good idea.\\n3. The evaluation of DoD on the ImageNet-$256 \\times 256$ dataset with reduced training cost and improved performance is impressive.\\n4. The largest model DoD-XL achieving an FID-50K score of 1.83 with only 1 million training steps is a significant result.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with the diffusion models and their limitations, which might make it hard for non-experts to understand the contribution of the paper.\\n2. The evaluation is limited to a single dataset, and it would be better to include more datasets to demonstrate the generalizability of the proposed approach.\\n3. The"}
{"paper_id": "UfczlMudN6", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a unified framework for dynamics generalization in deep reinforcement learning, addressing both in-distribution and out-of-distribution scenarios.\\n2. The introduction of a robust adaptation module and a joint training pipeline is an interesting and novel approach.\\n3. The experimental results on simulated locomotion tasks with a quadruped robot demonstrate strong generalization performance.\\n4. The paper is well-written and clearly presented.\",\n  \"Weaknesses\": \"1. The paper assumes that the in-distribution and out-of-distribution scenarios can be separated, which may not always be the case in real-world settings.\\n2. The evaluation of the algorithm on real-world scenarios is limited to simulated locomotion tasks, which may not be representative of all real-world applications.\\n3. The paper does not provide a clear comparison with existing methods for dynamics generalization in deep reinforcement learning.\\n4. The robust adaptation module and joint training pipeline are not thoroughly explained, making it difficult to understand their inner workings.\",\n  \"Questions\": \"1."}
{"paper_id": "60Vd7QOXlM", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel approach to privacy auditing of large language models, which sets a new standard for detection of privacy leakage. The authors demonstrate the effectiveness of their method through extensive experiments on multiple families of fine-tuned LLMs. The results show that their approach largely surpasses the prior SOTA, achieving a TPR of 26.0% at 1% FPR on the Qwen2.5-0.5B model.\",\n  \"Weaknesses\": \"The paper assumes that the attacker cannot train shadow models, insert gradient canaries, or access the model at every iteration, which might not be a realistic assumption in practice. Additionally, the paper does not provide a clear explanation of how their approach can be scaled to larger models or more complex threat models.\",\n  \"Questions\": \"How does the authors' approach handle the case where the attacker has access to a small subset of the training data? Can the authors provide more details on how their approach can be used to provide a privacy audit of \u03b5 \u2248 1 for a"}
{"paper_id": "GqhvJ1o8m5", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel framework, ImmersePro, for transforming single-view videos into stereo videos using a dual-branch architecture with spatial-temporal attention mechanisms. This approach enables implicit disparity guidance, reducing potential errors associated with disparity estimation models.\\n2. The authors introduce the YouTube-SBS dataset, a comprehensive collection of 423 stereo videos, featuring over 7 million stereo pairs, which can facilitate training and benchmarking of stereo video generation models.\\n3. The experiments demonstrate the effectiveness of ImmersePro in producing high-quality stereo videos, offering significant improvements over existing methods.\",\n  \"Weaknesses\": \"1. The paper assumes the availability of a large-scale dataset, YouTube-SBS, which might not be accessible to all researchers.\\n2. The authors do not provide a detailed comparison with other state-of-the-art methods, such as DeepStereo and StereoGAN, in terms of computational complexity and memory requirements.\\n3. The paper could benefit from more qualitative evaluation metrics, such as user studies or perceptual studies"}
{"paper_id": "ua5MHdsbck", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper introduces a novel approach to protein design by leveraging triplet relations, which is a significant improvement over existing methods that rely on pairs alone. The authors demonstrate the effectiveness of their method in designing AAV and GFP proteins, and the results show a significant improvement in extrapolation tasks.\",\n  \"Weaknesses\": \"The paper assumes that noisy training pairs are not sufficiently informative to capture the fitness gradient, but it would be beneficial to provide more evidence to support this claim. Additionally, the authors should provide more details about the preference alignment models used in large language models and how they were adapted for protein design.\",\n  \"Questions\": \"How does the proposed method handle cases where the triplet relations are not available or are noisy? Can the authors provide more information about the preference alignment models used in large language models and how they were adapted for protein design?\"\n}"}
{"paper_id": "56Zn3halhq", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method, AutoTSAug, is well-motivated and effectively addresses the issue of fixed-size training sets in time series forecasting. The use of reinforcement learning to learn data augmentation is a novel and interesting approach. The empirical analysis to identify marginal samples is also a good idea. The variational masked autoencoders are a good choice for augmentation, and the REINFORCE algorithm is a suitable choice for training the augmentation model.\",\n  \"Weaknesses\": \"The paper could benefit from more extensive experimentation with different types of time series data and more diverse models in the model zoo. The choice of variational masked autoencoders as the augmentation model seems to be somewhat arbitrary, and it would be good to explore other options. The paper could also benefit from a more thorough discussion of the limitations of the proposed method and potential future work.\",\n  \"Questions\": \"How does the proposed method perform on more complex time series data, such as those with multiple seasonal components or non-linear trends? How does the choice of variational masked autoencoders"}
{"paper_id": "aPTGvFqile", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed AlignCLIP approach is well-motivated and effectively addresses the modality gap issue in CLIP. The semantically-regularized separation objective function is an interesting and useful addition. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper could benefit from more thorough analysis of the modality gap and its impact on CLIP's performance. The evaluation results are not entirely clear, and some of the metrics used are not well-explained. The authors should provide more details on the implementation and hyperparameters used in the experiments.\",\n  \"Questions\": \"How does the semantically-regularized separation objective function affect the learning dynamics of the model? Are there any potential risks of over-regularization or under-regularization? Can the authors provide more insights on how the modality gap affects the performance of CLIP in different downstream tasks?\"\n}"}
{"paper_id": "GOwNImvCWf", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper identifies a limitation of weight-space autoencoders (AEs) and proposes a novel behavioral loss to address it.\\n2. The authors demonstrate the effectiveness of combining structural and behavioral losses in reconstructing and generating models that match the performance of the original models.\\n3. The paper explores representation learning in deep weight spaces and shows a strong synergy between structural and behavioral features.\",\n  \"Weaknesses\": \"1. The paper assumes that the original models are available for comparison, which might not always be the case in real-world scenarios.\\n2. The evaluation is limited to three model zoos, and it would be beneficial to explore more diverse datasets.\\n3. The paper does not provide a clear explanation of why the behavioral loss is necessary, and more intuition behind its design would be helpful.\",\n  \"Questions\": \"1. How does the behavioral loss capture the features essential for reconstructing high-performing models?\\n2. Can the proposed method be applied to other types of neural networks, such as recurrent neural networks ("}
{"paper_id": "C9BA0T3xhq", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel algorithm, Extended Implicit Q-Learning (EIQL), which effectively addresses the exploration-exploitation trade-off in offline reinforcement learning. The idea of incorporating actions beyond the dataset constraints by allowing selection actions with maximum Q is innovative and well-motivated.\\n2. The experimental results demonstrate the efficacy of EIQL in environments with sparse rewards or suboptimal and incomplete trajectories.\\n3. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"1. The paper assumes that the dataset contains a representative set of actions, which might not always be the case in practice.\\n2. The authors do not provide a detailed analysis of the computational complexity of EIQL.\\n3. The paper could benefit from a more comprehensive comparison with other offline RL algorithms.\",\n  \"Questions\": \"1. How does EIQL handle the case where the dataset contains conflicting actions with different Q-values?\\n2. Can EIQL be extended to online reinforcement learning settings?\\n3. How does EIQL perform"}
{"paper_id": "EeqlkPpaV8", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a thorough analysis of the adaptive complexity of sampling, with a clear and well-structured presentation of the results. The authors demonstrate a good understanding of the topic and provide a novel characterization of the output for the hardness potentials. The use of classical smoothing techniques is a nice touch.\",\n  \"Weaknesses\": \"The paper is dense and assumes a high level of background knowledge in the field. Some readers may find the notation and terminology used to be unfamiliar or unclear. Additionally, the paper could benefit from more concrete examples or illustrations to help readers understand the results.\",\n  \"Questions\": \"The authors claim that an almost linear iteration algorithm cannot return a sample with a specific exponentially small error under total variation distance. Can they provide a more detailed explanation of why this is the case? Additionally, what are the implications of this result for the design of sampling algorithms in practice?\"\n}"}
{"paper_id": "oK1zJCWBqf", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel method for aligning generative models with human preferences without requiring a reward model. The Soft Preference Optimization (SPO) method is theoretically grounded and demonstrates a clear advantage in simplicity and alignment precision. The authors provide a comprehensive comparison with existing methods and showcase the effectiveness of SPO in various scenarios.\",\n  \"Weaknesses\": \"The paper assumes the Bradley-Terry (BT) model assumption, which might limit its applicability to other preference models. The authors should provide more detailed analysis on how SPO performs under different preference models. Additionally, the paper could benefit from more experimental results to demonstrate the robustness of SPO in various settings.\",\n  \"Questions\": \"How does SPO perform when the preference data is noisy or incomplete? Can the authors provide more details on the scalability of SPO to large-scale preference datasets?\"\n}"}
{"paper_id": "Dq9VrVuLzV", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors propose a novel approach to generating photorealistic and geometric-controlled images for 3D occupancy prediction, which is a critical task in autonomous driving. The use of 3D semantic multi-plane images (MPIs) as conditional input to a 2D diffusion model is innovative and effective. The extensive qualitative and quantitative evaluations on the nuScenes dataset demonstrate the effectiveness of SyntheOcc. The approach can generate an unlimited amount of diverse, annotated, and controllable datasets for training perception models and simulation.\",\n  \"Weaknesses\": \"The paper assumes that the 3D semantic MPIs are available, which may not be the case in all scenarios. The authors do not provide a clear explanation of how to obtain these MPIs. Additionally, the paper does not discuss the potential limitations of using a 2D diffusion model for 3D occupancy prediction. Furthermore, the evaluation metrics used are not explicitly stated, and the comparison with other state-of-the-art methods is limited.\",\n  \"Questions\": \"How do the authors plan to obtain"}
{"paper_id": "73Q9U0vcja", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed approach of integrating a generative diffusion model with sequential experimental design is novel and well-motivated. The use of the diffusion model to capture the structure of the underlying data distribution and generate conditional data samples is a clever idea. The application to X-ray computed tomography imaging is a good choice, and the results demonstrate significant reductions in data acquisition requirements and improved image reconstruction quality.\",\n  \"Weaknesses\": \"The paper assumes that the forward model of the inverse problem is known, which might not always be the case in real-world applications. The method relies heavily on the quality of the pre-trained diffusion model, and it is not clear how sensitive the results are to the choice of the diffusion model architecture and hyperparameters. The paper could benefit from more detailed analysis of the computational complexity and scalability of the proposed approach.\",\n  \"Questions\": \"How does the proposed approach handle cases where the forward model of the inverse problem is not known or is uncertain? Are there any theoretical guarantees on the convergence of the proposed method? How does the method perform on"}
{"paper_id": "UiEjzBRYeI", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel and generalized methodology for equipping vision foundation models with multi-grained semantic perception abilities. The proposed approach, SAM-CP, is simple and effective, achieving state-of-the-art performance in open-vocabulary segmentation. The paper provides a clear and well-organized presentation of the method and its evaluation.\",\n  \"Weaknesses\": \"The paper assumes that the SAM patches are already available, which might not be the case in all scenarios. The evaluation of the method is limited to a few datasets, and it would be beneficial to include more comprehensive evaluation. The paper does not provide a clear explanation of how the affinity between queries and SAM patches is calculated.\",\n  \"Questions\": \"How does the proposed method handle cases where the SAM patches are not available or are not aligned with the text labels? Can the method be extended to handle more complex scenarios, such as multiple objects or scenes? How does the affinity calculation between queries and SAM patches impact the overall performance of the method?\"\n}"}
{"paper_id": "lessla98Wp", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes a novel approach to address the ill-posed problem of automatic video colorization using language descriptions, which is a significant contribution to the field.\\n2. The model's ability to generate semantically accurate colors, unrestricted creative correspondence, and temporally robust consistency is impressive.\\n3. The use of cross-modality pre-fusion module and temporally deformable attention is innovative and effective.\",\n  \"Weaknesses\": \"1. The paper assumes that the user-provided language descriptions are accurate and relevant, which may not always be the case.\\n2. The model's performance may degrade if the language descriptions are incomplete or ambiguous.\\n3. The evaluation metrics used in the paper may not capture all aspects of the colorization task, such as color harmony and aesthetics.\",\n  \"Questions\": \"1. How does the model handle cases where the language descriptions are contradictory or inconsistent?\\n2. Can the model be adapted to work with other types of language inputs, such as natural language queries or spoken language?\\n3"}
{"paper_id": "UstOpZCESc", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel and well-motivated approach to lifelong learning and privacy-aware unlearning, which is a critical and largely unaddressed challenge in the field. The proposed solution, PALL, is well-designed and effectively addresses the contradicting objectives of preventing catastrophic forgetting and allowing forward knowledge transfer. The use of task-specific sparse subnetworks with parameter sharing within a single architecture is a clever idea, and the episodic memory rehearsal mechanism is a nice touch. The empirical results demonstrate the scalability of PALL across various architectures in image classification, and the paper provides a state-of-the-art solution that uniquely integrates lifelong learning and privacy-aware unlearning mechanisms for responsible AI applications.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that it assumes a fixed set of tasks and does not address the case where new tasks are added or removed over time. Additionally, the paper does not provide a clear explanation of how the episodic memory rehearsal mechanism is implemented and how it affects the overall performance of the model. Furthermore, the paper assumes that"}
{"paper_id": "LnRegTxsa4", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed Cross-view and Cross-attention Module (CVCAM) is a good approach to capture spatial dependencies of query objects between different views.\\n2. The Multi-head Spatial Attention Module (MHSAM) is a useful addition to extract multi-scale spatial features from the feature maps containing implicit correspondences.\\n3. The creation of a new dataset called G2D for the \\\"Ground\u2192Drone\\\" localization task is a significant contribution to the field.\\n4. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper assumes that the two views are registered and aligned, which might not always be the case in real-world scenarios.\\n2. The authors do not provide a clear explanation of how the CVCAM and MHSAM modules are combined to produce the final output.\\n3. The paper relies heavily on the performance of the backbone network, and it would be interesting to see how the proposed modules perform on other backbone networks.\\n4. The G2D dataset"}
{"paper_id": "2ErS9Bkc3O", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a clear and concise explanation of the adversarial robustness of deep neural networks using matrix-theoretic and information-theoretic approaches. The authors' results show that neural networks' adversarial robustness can degrade as the input dimension increases, which is consistent with earlier feature-compression-based explanations. The paper's theoretical results are well-supported by experiments and provide a new perspective on the adversarial robustness of neural networks.\",\n  \"Weaknesses\": \"The paper's main contribution is the matrix-theoretic explanation of the adversarial robustness of deep neural networks, which is an interesting and novel result. However, the paper could benefit from more extensive experiments to demonstrate the practical implications of the theoretical results. Additionally, the authors could provide more details on how their results can be applied to improve the adversarial robustness of neural networks in practice.\",\n  \"Questions\": \"How do the authors plan to extend their results to more complex neural network architectures, such as residual networks or transformers? Can the matrix-theoretic explanation be used to"}
{"paper_id": "pK3oe2bubc", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed training approaches for vision transformers allow for randomizing the execution order of attention modules at training time, enabling adaptation to arbitrary layer execution orders at test time with a 20% reduction in accuracy. The analysis of feature representations and layer contributions is thorough and insightful.\",\n  \"Weaknesses\": \"The paper assumes a reduction in accuracy of about 20% at the same model size, which may be a significant trade-off for some applications. The analysis of the impact of layer pruning on performance is limited to a single experiment.\",\n  \"Questions\": \"How does the proposed method generalize to other types of neural networks, such as recurrent neural networks or convolutional neural networks? Are there any potential applications of this method beyond distributed neural network architectures?\"\n}"}
{"paper_id": "sec09tLQUl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel approach, FairDropout, to address the issue of spurious correlations in deep neural networks. The idea of redirecting memorization to specific neurons that are subsequently dropped out during inference is clever and has the potential to improve generalization. The empirical evaluation on the subpopulation benchmark suite is thorough and demonstrates the effectiveness of FairDropout. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that it assumes that memorization can be localized to a limited number of neurons, which may not always be the case. Additionally, the paper does not provide a clear explanation of how the number of neurons to be dropped out is determined. Finally, the paper could benefit from a more detailed analysis of the trade-off between the benefits of FairDropout and the potential decrease in performance due to the dropout.\",\n  \"Questions\": \"How does the paper handle the case where the memorization is not localized to a limited number of neurons? What is the impact of the number of neurons"}
{"paper_id": "96jZFqM5E0", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper presents a novel framework for pre-training 3D hand pose estimation from in-the-wild hand images, leveraging contrastive learning and adaptively weighting the contrastive learning loss based on inter-sample distance.\\n2. The method is scalable and achieves significant improvements over the state-of-the-art method (PeCLR) in various datasets.\\n3. The paper provides a clear and concise presentation of the method and its results.\",\n  \"Weaknesses\": \"1. The paper assumes that the hand images are well-aligned, which may not be the case in all in-the-wild videos.\\n2. The method may not generalize well to hands with different shapes or sizes.\\n3. The paper does not provide a detailed comparison with other pre-training methods.\",\n  \"Questions\": \"1. How does the paper handle cases where the hand images are not well-aligned?\\n2. Can the method be extended to handle hands with different shapes or sizes?\\n3. What are the computational requirements for training the model?\"\n}"}
{"paper_id": "uNd289HjLi", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed Corruption2Self (C2S) framework is well-designed and effectively addresses the limitations of existing self-supervised denoising methods. The use of generalized ambient denoising score matching (GADSM) loss is a novel and promising approach. The detail refinement extension is also a good idea to balance noise reduction with the preservation of fine spatial features. The paper provides a clear and concise description of the method and its components.\",\n  \"Weaknesses\": \"The paper could benefit from a more thorough comparison with other self-supervised denoising methods, including a discussion of their limitations and how C2S addresses them. The use of reparameterization of noise levels is not clearly explained, and more details about its impact on training and convergence would be helpful. The extension to multi-contrast denoising is an interesting direction, but it would be beneficial to see more results and analysis on this aspect.\",\n  \"Questions\": \"How does the reparameterization of noise levels affect the performance of the model, and can it be generalized"}
{"paper_id": "e2ONKX6qzJ", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed adaptive projected guidance (APG) method is well-motivated and effectively addresses the issue of oversaturation in classifier-free guidance. The connection to gradient ascent is insightful and the rescaling and momentum method is a nice addition. The method is easy to implement and introduces minimal computational overhead. The extensive experiments demonstrate the effectiveness of APG in improving FID, recall, and saturation scores while maintaining precision comparable to CFG.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the decomposition of the CFG update rule and the motivation behind down-weighting the parallel component. Additionally, the impact of the proposed method on the sampling process and the computational resources required should be further explored. The experiments could be more comprehensive by including more conditional diffusion models and samplers.\",\n  \"Questions\": \"How does the proposed APG method compare to other methods for addressing oversaturation in CFG, such as those based on regularization techniques or adaptive learning rates? Are there any potential issues with the rescaling and momentum method that could impact the"}
{"paper_id": "a2eBgp4sjH", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a thorough and systematic study of MultiFilterANN, a problem with strong practical motivation. The theoretical results are solid, including provable algorithms with the best known space-time tradeoffs and lower bounds for popular algorithms. The empirical approach is also well-motivated and competitive with existing state-of-the-art solutions. The release of novel datasets for MultiFilterANN is a significant contribution. The writing is clear and concise, making it easy to follow the paper's arguments.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that the empirical evaluation is limited to a small number of datasets and algorithms. It would be beneficial to include more comprehensive experiments to further establish the robustness of the proposed approach. Additionally, the paper assumes that the filter constraints are discrete, which may not be the case in all applications. It would be interesting to explore how the proposed approach generalizes to continuous filter constraints.\",\n  \"Questions\": \"1. How does the proposed approach handle cases where the filter constraints are not discrete? Would it require significant modifications to"}
{"paper_id": "EIXZXPz7jU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel sampling point selection method that addresses the challenges posed by singularities in PDEs for PINNs. The approach is based on diffusion models and the optimal transport coupling flow-matching technique, which is an interesting and innovative idea. The method is well-motivated and the results demonstrate its effectiveness in improving the solution quality for the linear elasticity equation with complex geometry of inclusion. The comparison with other approaches is also provided, which is a good practice.\",\n  \"Weaknesses\": \"The paper assumes that the PDE residuals are available, which might not always be the case in practice. Additionally, the method relies on the accuracy of the diffusion model, which can be a limitation. The paper could benefit from more discussion on the potential limitations and future work directions. The writing is clear and concise, but some sentences are a bit long and could be broken up for better readability.\",\n  \"Questions\": \"How does the method handle cases where the PDE residuals are not available? Are there any plans to extend the method to other types"}
{"paper_id": "QO4bF6MHza", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a new benchmark, MathHay, to evaluate the long-context mathematical reasoning capabilities of LLMs, which is a significant contribution to the field.\\n2. The paper conducts extensive experiments on MathHay to assess the performance of eight top-performing LLMs, providing valuable insights into their strengths and weaknesses.\\n3. The paper highlights the significant room for improvement on the MathHay benchmark, which can guide future research directions.\",\n  \"Weaknesses\": \"1. The paper does not provide a clear definition of the long-context mathematical reasoning capabilities and how they differ from other types of reasoning.\\n2. The paper does not discuss the potential limitations of the MathHay benchmark, such as its reliance on a specific dataset or the potential for bias in the evaluation metrics.\\n3. The paper does not provide a clear comparison with other benchmarks, such as Needle in a Haystack, to demonstrate the novelty and value of MathHay.\",\n  \"Questions\": \"1. How does the MathHay benchmark address the issue of long"}
{"paper_id": "GULx8rzzjC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach to evaluating the relative utility of different data sources, which is a significant contribution to the field of machine learning.\\n2. The method is data-efficient and can converge rapidly to the performance of the full-information baseline.\\n3. The experimental results are comprehensive and demonstrate the robustness of the approach to noise.\",\n  \"Weaknesses\": \"1. The paper assumes that the data sources are independent and identically distributed, which may not always be the case in practice.\\n2. The method relies on feature space distances and gradient matching, which may not be applicable to all types of data.\\n3. The paper does not provide a detailed explanation of the feature space distances and gradient matching techniques used.\",\n  \"Questions\": \"1. How does the method handle cases where the data sources are not independent and identically distributed?\\n2. Can the method be extended to handle non-Euclidean data spaces?\\n3. What are the computational requirements of the method, and how does it scale to large"}
{"paper_id": "PnZ2lbQaao", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a new framework for cross-domain recommendation systems, addressing the cold-start item problem. The framework is well-motivated and provides interpretability for cross-domain knowledge transfer. The empirical results on synthetic and real-world datasets are comprehensive and demonstrate the effectiveness of the proposed method.\",\n  \"Weaknesses\": \"The paper could benefit from a more in-depth analysis of the adversarial Bayesian framework. The authors should provide more details about the domain indexing process and how it improves the recommendation performance. Additionally, the paper could be improved by including more experiments with different datasets and evaluation metrics.\",\n  \"Questions\": \"How does the domain indexing process work in the proposed framework? What is the significance of using an adversarial Bayesian approach in cross-domain recommendation systems? How does the proposed framework compare to other state-of-the-art methods in terms of scalability and computational efficiency?\"\n}"}
{"paper_id": "AT64R0ivUO", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed method, SteerPrompt, effectively enables models to grasp essential contextual information, leading to substantially improved problem-solving performance.\\n2. The method is applied at inference time and does not require changing any model parameters, making it a practical solution.\\n3. The paper presents a clear and concise explanation of the method and its benefits.\",\n  \"Weaknesses\": \"1. The paper does not provide a thorough comparison with other existing methods for improving model reading comprehension, such as iterative prompting or other attention-steering techniques.\\n2. The evaluation is limited to a single dataset, open-book QA, and it would be beneficial to see the method's performance on other datasets or tasks.\\n3. The paper does not discuss potential limitations or challenges of the proposed method, such as its applicability to very long contexts or complex tasks.\",\n  \"Questions\": \"1. How does SteerPrompt handle cases where the context is extremely long or contains multiple distracting information?\\n2. Can SteerPrompt be applied to other tasks or domains"}
{"paper_id": "nGiGXLnKhl", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel architecture, Vision-RWKV, which is an efficient alternative to the Vision Transformer for visual perception tasks. The model's ability to handle high-resolution images without window operations is a significant advantage. \\n2. The paper provides a clear and concise description of the proposed method and its key components. The evaluation results demonstrate the model's performance and efficiency in various tasks. \\n3. The authors provide a well-organized and easy-to-follow presentation of the paper.\",\n  \"Weaknesses\": \"1. The paper assumes that the RWKV architecture from the NLP field is a suitable starting point for vision tasks, but it is not clear why this architecture is chosen specifically for vision tasks. \\n2. The paper does not provide a detailed explanation of the modifications made to the RWKV architecture for vision tasks. \\n3. The evaluation results are based on a limited number of tasks and datasets, which may not be representative of the model's performance in all scenarios.\",\n  \"Questions\": \"1. How"}
{"paper_id": "TBJCtWTvXJ", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors provide a thorough analysis of Adam's effectiveness and ineffectiveness in training DNNs, revealing its similarity to SignSGD and its susceptibility to loss spikes. The proposed SignSoftSGD (S3) optimizer is well-motivated and theoretically sound. The incorporation of Nesterov's accelerated gradient technique and the use of a flexible p-th order momentum in the denominator of the update are also valuable contributions. The extensive experimentation across various tasks demonstrates the effectiveness of S3.\",\n  \"Weaknesses\": \"The paper assumes that the loss function is L-smooth, which might not be the case in all scenarios. The authors should discuss the robustness of S3 to non-L-smooth loss functions. Additionally, the comparison with AdamW is not entirely fair, as AdamW is a variant of Adam that is specifically designed to handle non-convex optimization problems.\",\n  \"Questions\": \"How does S3 perform on more complex optimization problems, such as those with multiple local minima or non-convex loss functions? The"}
{"paper_id": "8gSrJOL2oc", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed TRIDENT framework addresses the limitations of existing CZSL methods by leveraging feature adaptive aggregation modules, learnable condition masks, and multimodal large language model embeddings. The use of attribute smoothing through auxiliary attributes generated by LLM is a novel and effective approach. The extensive experiments demonstrate the state-of-the-art performance on three challenging datasets. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The authors assume that the background does not have a significant impact on the attribute-object recognition, which may not be true in all cases. The use of learnable condition masks may lead to overfitting if not properly regularized. The evaluation of TRIDENT on more datasets and with different types of attributes and objects would strengthen the claims. The paper could benefit from more detailed analysis of the impact of each component of TRIDENT on the performance.\",\n  \"Questions\": \"How does the feature adaptive aggregation module handle cases where the background has a significant impact on the attribute-object recognition? Can the learnable condition masks be used in other computer vision"}
{"paper_id": "yheQRc5xWB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The proposed Mamba-CDSP model effectively tackles the over-balancing problem in TCP and achieves state-of-the-art performance on both synthetic and real-world datasets.\\n2. The paper provides a clear and concise introduction to the problem and the proposed solution, making it easy to follow.\\n3. The experiments are well-designed and demonstrate the effectiveness and efficiency of the proposed approach.\\n4. The paper is well-organized and easy to read.\",\n  \"Weaknesses\": \"1. The paper assumes that the reader is familiar with the basics of TCP and SSMs, which may make it difficult for non-experts to understand.\\n2. The paper does not provide a detailed explanation of the Mamba architecture and how it is modified to work with TCP.\\n3. The paper assumes that the covariates, treatments, and outcomes are all available and well-defined, which may not be the case in all real-world applications.\\n4. The paper does not discuss the potential limitations and challenges of using SSMs for"}
{"paper_id": "HxKSzulSD1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper tackles a critical issue in superalignment, namely the weak-to-strong deception phenomenon, which is a significant concern in the development of Large Language Models. The authors provide a clear and well-structured presentation of the problem, and their experimental results are thorough and well-presented. The paper contributes to the field by shedding light on the potential risks of superalignment and proposing a mitigation strategy.\",\n  \"Weaknesses\": \"The paper assumes a specific scenario with conflicting alignment targets, which might limit its generalizability to other settings. The authors could have explored more diverse scenarios to strengthen their findings. Additionally, the paper could benefit from a more detailed discussion on the implications of the weak-to-strong deception phenomenon and potential future research directions.\",\n  \"Questions\": \"How do the authors plan to extend their work to more general settings, where alignment targets do not conflict with each other? What are the potential consequences of the weak-to-strong deception phenomenon in real-world applications of superalignment? How can the proposed mitigation strategy be adapted to other domains and settings?\"\n}"}
{"paper_id": "wH8XXUOUZU", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors propose a novel approach to autoencoder design that addresses the challenge of high spatial compression ratios. The Residual Autoencoding and Decoupled High-Resolution Adaptation techniques are well-motivated and effectively improve the autoencoder's performance.\\n2. The experimental results demonstrate significant speedup without accuracy drop, which is impressive and showcases the potential of the proposed approach.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper assumes that the input data is available in a format that can be easily transformed into a space-to-channel representation. However, this might not be the case for all types of data.\\n2. The authors do not provide a thorough analysis of the trade-offs between the two proposed techniques and how they interact with each other.\\n3. The paper does not discuss the potential limitations of the proposed approach in terms of scalability and generalizability.\",\n  \"Questions\": \"1. How do the authors plan to extend the proposed approach to other types of data"}
{"paper_id": "QWIg5e6mtT", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors provide a novel and well-motivated approach to solving heterogeneous team games, which is a significant contribution to the field. The proposed H-PSRO framework is well-defined and theoretically sound. The experiments demonstrate the effectiveness of H-PSRO in heterogeneous team games and homogeneous settings.\",\n  \"Weaknesses\": \"The paper assumes that the teammates' policies can be parameterized, which might not be the case in all scenarios. The authors should discuss the limitations of this assumption. Additionally, the paper could benefit from a more detailed comparison with other PSRO frameworks, such as Team PSRO.\",\n  \"Questions\": \"How does the authors' assumption about parameterizing teammates' policies affect the applicability of H-PSRO in real-world scenarios? Can the authors provide more details about the computational complexity of H-PSRO compared to Team PSRO?\"\n}"}
{"paper_id": "kbSU5bwoRv", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed method, SaMoye, is the first open-source high-quality zero-shot SVC model that can convert singing to human and non-human timbre.\\n2. The disentanglement of singing voice features into content, timbre, and pitch features is an innovative approach.\\n3. The use of multiple ASR models and compression of content features to reduce timbre leakages is a good idea.\\n4. The establishment of a large-scale dataset with over 1,815 hours of pure singing voice and 6,367 speakers is impressive.\",\n  \"Weaknesses\": \"1. The paper assumes that the ASR models used for content feature extraction are accurate, but it is unclear how well they perform in practice.\\n2. The method relies heavily on the quality of the speaker embedding and the top-3 similar speakers, which may not always be available or accurate.\\n3. The paper does not provide a clear explanation of how the timbre features are enhanced by unfreezing the speaker encoder and mixing"}
{"paper_id": "KyqtKhv6q1", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed Differentiable Map Priors framework is simple and effective in learning spatial priors from historic traversals, which can be easily integrated into leading 3D perception systems at little to no extra computational costs. The improvement in 3D object detection and semantic map segmentation tasks on the nuScenes dataset across several architectures is significant and consistent.\",\n  \"Weaknesses\": \"The paper assumes that the historic traversals are available, which might not always be the case. The authors should discuss the scenarios where this assumption may not hold. Additionally, the paper does not provide a clear explanation of how the Differentiable Map Priors are learned from the historic traversals.\",\n  \"Questions\": \"How do the authors handle the case where the historic traversals are not available? What is the computational cost of learning the Differentiable Map Priors, and how does it compare to other methods?\"\n}"}
{"paper_id": "lTL4t68BNc", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors identify a strong connection between the Information Bottleneck (IB) principle and the robustness of graph attention-based GNNs. This is a valuable contribution to the field. The proposed RGA-IB method is well-designed and effectively minimizes the IB loss, leading to improved robustness against graph adversarial attacks. The experiments demonstrate the effectiveness of RGA-IB in semi-supervised node classification.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the IB loss and its relation to robustness. The authors assume that the reader is familiar with the IB principle, which may not be the case for all readers. Additionally, the experimental section could be improved by including more baseline models and a more thorough analysis of the results.\",\n  \"Questions\": \"How does the RGA-IB method compare to other robust GNNs in terms of computational efficiency? Are there any potential applications of RGA-IB in other domains beyond node classification?\"\n}"}
{"paper_id": "hWmwL9gizZ", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel deep learning solution, ProVaccine, for immunogenicity prediction that integrates pre-trained latent vector representations of protein sequences and structures with a dual attention mechanism. The model is evaluated on a comprehensive immunogenicity dataset with over 9,500 antigen sequences, structures, and immunogenicity labels. The results show that ProVaccine outperforms existing methods across a wide range of evaluation metrics. The post-hoc validation protocol is a valuable addition to the paper. The work provides an effective tool for vaccine design and sets valuable benchmarks for future research.\",\n  \"Weaknesses\": \"The paper relies heavily on pre-trained models and does not provide a clear explanation of how the dual attention mechanism is used to integrate the protein sequence and structure information. The evaluation metrics used are not clearly defined, and the post-hoc validation protocol could be more extensively explained. The paper could benefit from more discussion on the practical implications of the results and the limitations of the approach.\",\n  \"Questions\": \"How does the dual attention mechanism specifically integrate the"}
{"paper_id": "IL85Ebjg9j", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed trust-based discounting method is a novel approach to addressing conflicts in multi-view classification.\\n2. The evaluation metrics used, including Top-1 Accuracy, Fleiss\u2019 Kappa, and Multi-View Agreement with Ground Truth, are well-suited for assessing the reliability of multi-view classification models.\\n3. The experimental results demonstrate the effectiveness of the proposed method in resolving conflicts and improving the reliability of predictions.\",\n  \"Weaknesses\": \"1. The paper assumes that the reliability of predictions made by individual views can be captured using a trust discounting mechanism, which may not always be the case in real-world scenarios.\\n2. The evaluation of the proposed method is limited to six real-world datasets, and it would be beneficial to evaluate it on a broader range of datasets.\\n3. The paper does not provide a clear explanation of how the trust discounting mechanism is learned and updated during the training process.\",\n  \"Questions\": \"1. How does the trust discounting mechanism handle cases where the reliability of predictions made"}
{"paper_id": "cnKhHxN3xj", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed measure of neuronal entanglement using the Wasserstein distance is novel and effective. The framework for disentangling polysemantic neurons is well-designed and well-executed. The experimental results are strong and provide valuable insights into the behavior of large language models under weight sparsity.\",\n  \"Weaknesses\": \"The paper could benefit from more discussion on the limitations of the proposed measure and the framework. The experimental results are mostly focused on the effect of disentanglement on model accuracy, but it would be interesting to explore other aspects, such as the interpretability of the disentangled neurons.\",\n  \"Questions\": \"How does the proposed measure of neuronal entanglement compare to other existing measures, such as the mutual information between neuron activations? Can the framework for disentangling polysemantic neurons be applied to other types of neural networks, such as convolutional neural networks? What are the computational costs associated with computing the Wasserstein distance for each neuron's output distribution?\"\n}"}
{"paper_id": "dgR6i4TSng", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel and efficient approach to parameter-efficient fine-tuning (PEFT) using quantum computations. The use of Pauli parameterization leads to a significant reduction in the number of trainable parameters, making it more efficient than existing methods like LoRA. The experiments demonstrate the effectiveness of Quantum-PEFT in language and vision transfer learning tasks. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes a good understanding of quantum computing and quantum parameterization, which may limit its accessibility to readers without a background in quantum computing. The experiments are limited to a few benchmarks, and it would be beneficial to see more extensive evaluations. The paper could also benefit from a more detailed comparison with other PEFT methods.\",\n  \"Questions\": \"How does the authors plan to scale up the quantum computations to larger models and datasets? What are the potential applications of Quantum-PEFT beyond transfer learning tasks? How does the authors ensure that the quantum parameterization does not introduce any bias or artifacts in the fine-tuning process"}
{"paper_id": "ye1mxb79lw", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel algorithm for Bilevel Bayesian Optimization (BILBO) that tackles the challenges of noisy, constrained, and derivative-free settings. The algorithm uses confidence bounds to construct trusted sets of feasible and lower-level optimal solutions, which is a significant contribution. The theoretical guarantee of sublinear regret bound and the empirical evaluation on synthetic and real-world problems are also impressive. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"The paper assumes that the upper- and lower-level function evaluations come from different simulators or experiments, which might not be the case in all scenarios. The authors should discuss this assumption and its implications. Additionally, the paper could benefit from more detailed explanations of the confidence bounds and their construction.\",\n  \"Questions\": \"How does the algorithm handle the case where the upper- and lower-level function evaluations come from the same simulator or experiment? Can the authors provide more details on the confidence bounds and their construction? What are the implications of the sublinear regret bound in practice?\"\n}"}
{"paper_id": "1fwZJzGdKj", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed multi-agent collaborative data selection mechanism is well-designed and effectively tackles the inherent conflicts between different data selection methods. The experimental results demonstrate a significant improvement in data efficiency and convergence speed, and the average performance gain of 10.5% is impressive. The agent console is a nice touch, allowing for dynamic adjustment of agent impacts and integration of information throughout the LLM training process.\",\n  \"Weaknesses\": \"The paper could benefit from a more in-depth analysis of the individual agents' performance and how they contribute to the overall improvement in data efficiency. Additionally, the authors could provide more details about the specific data selection methods used and how they are prioritized by the agents.\",\n  \"Questions\": \"How do the agents update their prioritization rules, and what is the specific mechanism for adjusting the impacts of different agents? Is there any potential for overfitting or over-adaptation in the agents' decisions? What are the computational costs associated with implementing this multi-agent framework, and how does it compare to other data selection methods?\"\n}"}
{"paper_id": "pISLZG7ktL", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a comprehensive empirical study on data scaling in imitation learning for robotic manipulation, collecting over 40,000 demonstrations and executing more than 15,000 real-world robot rollouts. The findings reveal a power-law relationship between the number of environments and objects and the policy's generalization performance. The proposed data collection strategy is efficient and effective. The paper showcases the importance of diversity in environments and objects for policy generalization.\",\n  \"Weaknesses\": \"The paper assumes that the robot can operate in various environments and with different objects, which might not be the case in real-world scenarios. The study is limited to a specific robotic system and tasks, and it is unclear whether the findings can be generalized to other robotic systems and tasks. The paper does not provide a detailed analysis of the power-law relationship between the number of environments and objects and the policy's generalization performance.\",\n  \"Questions\": \"How can the findings of this paper be applied to real-world robotic systems and tasks? Can the proposed data collection strategy be adapted to other robotic systems"}
{"paper_id": "dML3XGvWmy", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel and innovative approach to agent design, leveraging large language models to enable self-evolving agents. The G\u00f6del Agent framework is well-motivated and has the potential to revolutionize the field of AI. The experimental results are impressive, demonstrating the ability of G\u00f6del Agent to surpass manually crafted agents in performance, efficiency, and generalizability. The paper is well-written and clearly presented.\",\n  \"Weaknesses\": \"One potential concern is the reliance on large language models, which may not be widely available or accessible to all researchers. Additionally, the paper could benefit from a more detailed discussion of the limitations and potential risks of self-evolving agents. The authors should also provide more information on how G\u00f6del Agent can be scaled up to more complex tasks and domains.\",\n  \"Questions\": \"How does the G\u00f6del Agent framework handle situations where the high-level objectives are ambiguous or conflicting? Can the G\u00f6del Agent framework be applied to other domains beyond mathematical reasoning and complex agent tasks? What are the potential risks and challenges associated with the"}
{"paper_id": "xlxGsX1pc7", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors have addressed a significant gap in the current evaluation of mathematical skills in LLMs by introducing a novel benchmark, U-MATH, with a large and diverse set of university-level problems. The inclusion of visual elements and open-ended tasks adds a new dimension to the evaluation. The release of \u03bc-MATH for evaluating LLMs' capabilities in judging solutions is also a valuable contribution.\",\n  \"Weaknesses\": \"The paper relies heavily on the performance of LLMs in judging solutions, which may not be a direct measure of their mathematical skills. The evaluation of LLMs' performance on visual problems is limited to 45%, which raises concerns about the robustness of the benchmark. The paper could benefit from a more detailed analysis of the results and a discussion of the implications for the development of LLMs.\",\n  \"Questions\": \"How does the authors' LLM judge the correctness of generated solutions, and what are the potential biases in this approach? What are the limitations of the U-MATH benchmark, and how can they be"}
{"paper_id": "AjXkRZIvjB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper provides a comprehensive analysis of the limitations of current LLMs in mathematical reasoning, which is a significant contribution to the field.\\n2. The introduction of GSM-Symbolic is a major strength, as it allows for more controllable evaluations and provides a more nuanced understanding of LLMs' capabilities and limitations.\\n3. The paper's findings, including the variance in LLMs' performance when responding to different instantiations of the same question and the fragility of mathematical reasoning in these models, are well-supported and thought-provoking.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed explanation of the GSM-Symbolic benchmark and its advantages over existing evaluations.\\n2. While the paper highlights the limitations of current LLMs, it does not provide a clear direction for future research or potential solutions to address these limitations.\\n3. The paper relies heavily on the GSM8K benchmark, which may not be representative of real-world mathematical reasoning tasks.\",\n  \"Questions\": \"1"}
{"paper_id": "IJiTI0fB0e", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper provides a clear and concise introduction to the problem of quantifying diversity in prompt-based generative models.\\n2. The proposed method, based on information-theoretic measures, is well-motivated and provides a novel approach to addressing the problem.\\n3. The authors provide a clear and thorough explanation of their method and its application to several numerical experiments.\",\n  \"Weaknesses\": \"1. The paper assumes that the text variable T is a single value, whereas in practice, it may be a sequence of values.\\n2. The authors do not provide a clear explanation of how their method can be extended to handle cases where the text variable T is a sequence.\\n3. The paper assumes that the generative model is a fixed model, whereas in practice, the model may be trained on a dataset with varying levels of diversity.\",\n  \"Questions\": \"1. How does the Conditional-Vendi score handle cases where the text variable T is a sequence?\\n2. Can the authors provide more details on how to"}
{"paper_id": "fs2Z2z3GRx", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a well-motivated and theoretically justified algorithm for linear inverse problems. The proposed Flow with Interpolant Guidance (FIG) efficiently guides the reverse-time sampling process with measurement interpolants, leading to competitive results on various linear image reconstruction tasks. The authors demonstrate the effectiveness of FIG on natural image datasets, improving upon state-of-the-art baseline algorithms, especially for challenging tasks. The code will be released, making it easier for others to build upon this work.\",\n  \"Weaknesses\": \"The paper assumes that the measurement noise is Gaussian, which might not be the case in real-world scenarios. It would be beneficial to explore the performance of FIG under non-Gaussian noise. Additionally, the authors could provide more insights into the theoretical guarantees of FIG, such as convergence rates or stability analysis.\",\n  \"Questions\": \"How does the FIG algorithm handle non-linear inverse problems? Can the authors provide more details on the measurement interpolant schemes and their theoretical justification? How does the performance of FIG compare to other state-of-the-art algorithms on non-linear"}
{"paper_id": "H4FSx06FCZ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a well-designed and secure 3D steganography framework, SecureGS, which effectively addresses the challenges of 3D Gaussian splatting (3DGS).\\n2. The proposed hybrid decoupled Gaussian encryption mechanism and density region-aware anchor growing and pruning strategy are innovative and well-implemented.\\n3. The paper provides a comprehensive evaluation of SecureGS, demonstrating its superiority over existing GS steganography methods in terms of rendering fidelity, speed, and security.\",\n  \"Weaknesses\": \"1. The paper assumes that the anchor points are publicly accessible, which might not be the case in all scenarios.\\n2. The security analysis of SecureGS is limited to the security of the geometric structure of the visualized point cloud, and it is unclear how the framework would perform in the presence of more sophisticated attacks.\\n3. The paper does not provide a detailed analysis of the computational complexity of SecureGS and its scalability for large-scale 3D scenes.\",\n  \"Questions\": \"1. How does"}
{"paper_id": "OGtUfA6Amo", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper addresses a significant problem in multivariate time series analysis, specifically the issue of dealing with irregularly sampled data. The proposed hierarchical patch graph network, Hi-Patch, is a well-designed architecture that effectively captures both local and global temporal and inter-variable dependencies. The experimental results are comprehensive and demonstrate the superiority of Hi-Patch over state-of-the-art models. The code is also made available, which is a plus.\",\n  \"Weaknesses\": \"One potential issue is that the paper assumes that the patches are non-overlapping, which might not always be the case in real-world data. Additionally, the authors do not provide a clear explanation of how the patch-level node representations are aggregated to produce the final output. The paper could benefit from more detailed discussions on the choice of hyperparameters and their impact on the results.\",\n  \"Questions\": \"How does the authors handle the case where the patches overlap? Are there any considerations for the patch size and the number of patches in the hierarchical architecture? What is the computational complexity of the proposed method,"}
{"paper_id": "ujpAYpFDEA", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper addresses a crucial aspect of watermarking in LLMs, namely imperceptibility. The proposed Water-Probe algorithm is well-designed and effectively detects watermarks. The introduction of the Water-Bag strategy to improve imperceptibility is a significant contribution.\",\n  \"Weaknesses\": \"The paper relies heavily on the assumption that current watermarking algorithms expose consistent biases under the same watermark key. However, this assumption may not always hold in real-world scenarios. Additionally, the evaluation of Water-Probe's performance on non-watermarked LLMs is limited to a minimal false positive rate, which may not be sufficient to guarantee its effectiveness in all cases.\",\n  \"Questions\": \"How does the Water-Probe algorithm handle cases where the watermarking algorithm does not expose consistent biases? What are the potential implications of the Water-Bag strategy on the overall security of the watermarked LLMs?\"\n}"}
{"paper_id": "7UKHNQIErp", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel formalism for characterizing preference losses for single model and reference model based approaches, which is a significant contribution to the field of human AI alignment. The authors' ability to systematically derive symbolic expressions for a number of commonly used DPA variants is impressive. The formal view of preference learning sheds new light on the size and structure of the DPA loss landscape, making it possible to rigorously characterize the relationships between recent loss proposals and systematically explore the landscape to derive new loss functions from first principles. The authors' hope that their framework and findings will provide useful guidance to those working on human AI alignment is well-justified.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the motivation behind the proposed formalism. The authors assume a certain level of background knowledge in the field of human AI alignment and direct preference alignment algorithms, which may make it difficult for readers without this background to fully understand the significance of the work. Additionally, the paper could be improved by providing more concrete examples of how the formal"}
{"paper_id": "CpgWRFqxhD", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a novel approach to generate identity-consistent and expressive talking videos. The use of memory-guided temporal module and emotion-aware audio module is innovative and well-justified. The experiments demonstrate the effectiveness of the proposed method, and the results are impressive. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"The paper assumes that the input audio is of good quality, but it's not clear how the method would perform with noisy or low-quality audio. Additionally, the paper does not provide a thorough analysis of the computational cost of the proposed method, which is a significant concern for practical applications. The paper could benefit from more detailed discussion on the potential limitations and future directions of the work.\",\n  \"Questions\": \"How does the method handle cases where the audio and video are not synchronized, e.g., due to lip syncing issues? Can the method be extended to handle multiple speakers or complex scenes? How does the method perform on datasets with diverse cultural or linguistic backgrounds?\"\n}"}
