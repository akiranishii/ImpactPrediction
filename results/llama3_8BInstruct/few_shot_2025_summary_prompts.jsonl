{"paper_id": "u3xwwfHmBC", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel approach to traffic forecasting by integrating tense-specific attention modules and contrastive learning, which significantly improves the model's ability to capture spatio-temporal relationships. The use of Graph Attention Networks and negative filtering technique is innovative and effective. The evaluation on real traffic datasets demonstrates the model's superiority over existing models.\",\n  \"Weaknesses\": \"The paper assumes that the traffic data is available for the entire time period, which might not be the case in real-world scenarios. The authors should discuss the limitations of their approach when dealing with incomplete data. Additionally, the paper could benefit from more comprehensive ablation studies to better understand the contributions of each component.\",\n  \"Questions\": \"How does the proposed approach handle missing values in the traffic data? Can the tense-specific attention modules be applied to other types of sequential data? How does the negative filtering technique compare to other methods for handling incomplete data?\"\n}"}
{"paper_id": "YKvBiRWdQC", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The Overcooked Generalisation Challenge (OGC) is an innovative and much-needed benchmark for evaluating the ability of reinforcement learning agents to generalise cooperation with novel partners and in unseen environments. The use of unsupervised environment design (UED) and dual curriculum design (DCD) algorithms is a significant step forward in addressing the current limitations of existing benchmarks. The inclusion of various DCD methods and neural network architectures adds depth to the challenge.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the environment and level design process. Additionally, the evaluation metrics used are not clearly defined, making it difficult to assess the performance of the agents. Furthermore, the paper could have provided more insight into the challenges faced by state-of-the-art DCD algorithms in the OGC.\",\n  \"Questions\": \"What are the specific criteria for evaluating the performance of the agents in the OGC? How do the authors plan to address the challenges faced by state-of-the-art DCD algorithms in the OGC? What are the potential applications of"}
{"paper_id": "tePFpDgyqg", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a clear problem definition and a novel solution to the challenge of long-context training in LLMs. The proposed LongPack data pipeline is well-structured and well-explained, and the results are convincing. The paper highlights the importance of long-distance referrals in LLM training and demonstrates a scalable solution for constructing high-quality long documents.\",\n  \"Weaknesses\": \"The paper assumes that hyperlinks in web pages are a reliable source of referral signals, but this may not be the case for all types of documents. Additionally, the paper does not explore the impact of noise or biases in the web pages on the quality of the constructed documents.\",\n  \"Questions\": \"How does the paper handle cases where hyperlinks are not present or are not accurate? Are there any plans to extend the LongPack pipeline to other types of documents or sources of referral signals?\"\n}"}
{"paper_id": "gx3LMRB15C", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method, T-JEPA, effectively addresses the challenge of self-supervised learning for tabular data without relying on data augmentations. The method's ability to learn rich representations of tabular data without labels is a significant contribution. The use of regularization tokens to prevent representation collapse is also a notable aspect of the approach. The experimental results demonstrate the effectiveness of T-JEPA in improving performance on downstream classification and regression tasks.\",\n  \"Weaknesses\": \"The paper's reliance on the Joint Embedding Predictive Architecture (JEPA) may limit its applicability to other types of data. The choice of regularization tokens and their impact on the model's performance could be further explored. The comparison to traditional methods like Gradient Boosted Decision Trees is not comprehensive, and more baselines should be included to better understand the method's strengths and weaknesses.\",\n  \"Questions\": \"How does T-JEPA handle cases where the target features to be predicted are highly correlated with the context features? Would the method still be effective in such scenarios? The paper"}
{"paper_id": "kTH3bEH6hW", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel range-limited augmentation strategy for tabular data, which is a significant contribution to the field of few-shot learning. The approach is simple yet effective, and the results on the FeSTa benchmark demonstrate its robustness across various scenarios. The paper is well-organized and clearly presents the method and its evaluation.\",\n  \"Weaknesses\": \"The paper assumes that the distribution of each feature is known a priori, which might not always be the case in real-world scenarios. The evaluation is limited to a single benchmark, FeSTa, and it would be beneficial to include more comprehensive experiments. Additionally, the paper does not provide a detailed analysis of the selection ratio and its impact on the performance.\",\n  \"Questions\": \"How does the proposed range-limited augmentation strategy compare to other methods that rely on more complex augmentations, such as those that incorporate domain knowledge or learnable transformations? Can the authors provide more insights into the selection ratio and its effect on the performance? How does the proposed method handle scenarios where the distribution"}
{"paper_id": "EqcLAU6gyU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed framework is well-designed and addresses a significant gap in the existing literature on multi-agent systems. The use of a meta-agent powered by a large language model (LLM) is a novel and effective approach to task decomposition and allocation. The framework's ability to evaluate intermediate results and make timely adjustments for enhanced problem-solving capabilities is a significant improvement over existing methods. The experimental results demonstrate the framework's effectiveness in solving real-world problems across various datasets.\",\n  \"Weaknesses\": \"One potential weakness of the proposed framework is its reliance on a meta-agent powered by a large language model. The performance of the meta-agent may degrade if the LLM is not well-trained or if the task is too complex. Additionally, the framework assumes that the tasks can be decomposed into sub-tasks, which may not always be the case in real-world scenarios. The framework's performance may also be affected by the quality of the reward model and the detector, which are critical components of the framework.\",\n  \"Questions\": \"1. How does the framework handle cases"}
{"paper_id": "qrTrnrEi9d", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method, TransFusion, is a novel approach to improve the performance of instruction-tuned LLMs in cross-lingual information extraction tasks. The framework of translating low-resource language text to English, annotating it, and fusing these annotations with the original text is a good idea. The experiments demonstrate a significant improvement in the performance of instruction-tuned LLMs in cross-lingual information extraction tasks. The framework enhances low-resource language NER and adapts well to unseen tasks. It offers a substantial performance boost over existing methods and models, such as GPT-4, using the advantages of machine translation to enrich input data and outperforming trans-train methodologies on low-resource datasets.\",\n  \"Weaknesses\": \"The paper assumes that English translations of low-resource language data are available, which may not always be the case. The method relies heavily on the quality of the machine translation and the annotation process. The evaluation metrics used are not entirely clear, and the comparison with other methods and models is limited. The authors"}
{"paper_id": "Bp0HBaMNRl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The authors establish new identifiability conditions for nonlinear latent models, increasing the applicability to real-world data without deterministic assumptions.\\n2. The proposed method outperforms existing approaches in both accuracy and scalability.\\n3. The method is successful in learning meaningful hierarchical latent structures from image data, enhancing the understanding and transferability in domain-shift tasks.\",\n  \"Weaknesses\": \"1. The paper's notation is somewhat non-standard, and it would be beneficial to provide a clear explanation of the symbols used throughout the paper.\\n2. The experiments on real-world data are limited to MNIST, which may not be representative of all high-dimensional image datasets.\\n3. The method's performance on transfer learning tasks is not thoroughly evaluated, and it would be helpful to include more detailed results in this regard.\",\n  \"Questions\": \"1. How do the authors determine the number of latent variables in the hierarchical causal model?\\n2. The paper mentions that the method can handle non-linear relationships, but it would be helpful to provide more"}
{"paper_id": "JzLcKWtGnl", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes an innovative approach to enhance spatial awareness in 3D vision-language models, which is a significant contribution to the field. The progressive spatial awareness scheme is well-designed and effectively captures multi-level spatial information. The experiments on the new MODLE dataset demonstrate the model's superiority in 3D spatial perception and reasoning tasks.\",\n  \"Weaknesses\": \"The paper assumes that the 3D scene encoder is frozen, which might limit the model's ability to adapt to new scenes or tasks. Additionally, the use of Graph Convolutional Network (GCN) for inter-referent message passing might not be the most effective choice for capturing spatial relationships.\",\n  \"Questions\": \"How does the model handle cases where the 3D scene encoder is not frozen, and new scenes or tasks require adaptation? Can the authors provide more insights into the choice of GCN for inter-referent message passing, and explore other alternatives for capturing spatial relationships?\"\n}"}
{"paper_id": "hXJrQWIoR3", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors provide a clear and well-structured explanation of the problem of explainable graph representation learning, and propose a framework to address this problem. The proposed methods are well-motivated and theoretically sound. The experiments demonstrate the effectiveness of the proposed methods in both accuracy and interpretability.\",\n  \"Weaknesses\": \"The paper assumes that the graph patterns are known a priori, which might not be the case in many real-world scenarios. The authors should discuss this assumption and provide more details on how to handle unknown graph patterns. Additionally, the experiments are mainly focused on graph classification tasks, and it would be interesting to see the results on other tasks, such as graph regression or graph clustering.\",\n  \"Questions\": \"How do the authors plan to handle the case when the graph patterns are unknown or changing over time? What are the computational complexities of the proposed methods, especially for large graphs? Are there any potential applications of the proposed methods in other domains, such as social network analysis or bioinformatics?\"\n}"}
{"paper_id": "67sSPPAZiG", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed approach to create a large-scale egocentric QA dataset is novel and valuable.\\n2. The MM-Ego model's Memory Pointer Prompting mechanism is an effective way to focus on important frames in a video.\\n3. The introduction of a de-biasing mechanism to address language bias in evaluation is a good idea.\",\n  \"Weaknesses\": \"1. The authors could provide more information about the quality of the human-annotated video narration used to create the dataset.\\n2. The evaluation of the MM-Ego model on other benchmark datasets would strengthen the paper's claims.\\n3. The impact of the 'global glimpse' and 'fallback' mechanisms on the model's performance could be further explored.\",\n  \"Questions\": \"1. How does the authors ensure that the 'narration to egocentric QA' strategy is not biased towards specific types of human activities?\\n2. Can the MM-Ego model be adapted to other types of videos, such as those with different camera angles or lighting"}
{"paper_id": "kjmLabjSE2", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a significant extension to the previous works by relaxing the assumptions of smoothness and strong convexity, which is a major contribution to the field of differential privacy. The proposed method provides a better privacy guarantee than the state-of-the-art for smooth strongly convex losses and a convergent privacy loss bound for a wider class of problems.\",\n  \"Weaknesses\": \"The paper assumes that the loss functions have H\\\"older continuous gradients, which might be a restrictive assumption in some cases. The authors should provide more discussion on the implications of this assumption and how it affects the applicability of the proposed method.\",\n  \"Questions\": \"How does the proposed method perform in practice, especially when the loss functions do not have H\\\"older continuous gradients? What are the computational costs associated with the proposed method, and how does it compare to existing methods?\"\n}"}
{"paper_id": "I18MA5DjoP", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The study makes a novel and interesting contribution by exploring the predictability of neuron activations in transformer-based LLMs, which can lead to a better understanding of the models' inner workings and potentially improve their performance. The Neuron Predictability Lens (NPL) framework developed in the paper is a useful tool for analyzing LLMs. The experiments conducted on LLaMA-2 and GPT-J models demonstrate the effectiveness of the NPL framework in predicting neuron activations, which can be used for both global and local analysis of neuron behavior.\",\n  \"Weaknesses\": \"One potential weakness of the paper is that it relies on the assumption that neuron activations are predictable, which may not always be the case. Additionally, the paper does not explore the implications of neuron predictability on the model's performance and robustness. The paper could be improved by providing more insights into the potential applications of the NPL framework and its limitations. The authors could also provide more details on the hyperparameters used for training the neuron mappings and the evaluation metrics used to assess predict"}
{"paper_id": "qpDqO7qa3R", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method is well-motivated, and the hierarchical latent warping and hybrid flow-guided spatial-aware token merging are elegant solutions to the problem of temporal consistency in video restoration. The method is versatile and can work with any pre-trained 2D image restoration diffusion model, making it a powerful tool for various video enhancement tasks.\\nThe paper is well-written and the experiments are comprehensive, comparing the proposed method to state-of-the-art models on various video super-resolution and denoising tasks.\",\n  \"Weaknesses\": \"None\",\n  \"Questions\": \"Can the proposed method be extended to handle more complex video restoration tasks, such as video inpainting or video deblurring?\\nHow does the proposed method handle cases where the pre-trained image restoration diffusion model is not well-suited for the specific video restoration task at hand?\\nCan the authors provide more details on the computational efficiency of the proposed method compared to existing state-of-the-art models?\"\n}"}
{"paper_id": "dSQtMx6dPE", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors have made a strong contribution by developing privacy-preserving algorithms for graph prompt learning, which is a crucial aspect of graph neural networks. The use of the PATE framework is innovative and effective. The evaluation on various datasets and GNN architectures is comprehensive.\",\n  \"Weaknesses\": \"The authors assume that the private data is split into disjoint subsets, which may not always be feasible in real-world scenarios. The evaluation of the algorithms on a wider range of datasets and scenarios is necessary to confirm the robustness of the proposed methods.\",\n  \"Questions\": \"How do the authors plan to extend the proposed methods to handle non-disjoint subsets of private data? What are the potential limitations of the PATE framework in graph prompt learning, and how can they be addressed?\"\n}"}
{"paper_id": "GbgCRJedQ7", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors provide a novel approach to parameter-efficient fine-tuning (PEFT) by selecting sparse sub-matrices for tuning, which is a significant contribution. The method, Sparse Matrix Tuning (SMT), is designed to minimize the performance gap between PEFT and full fine-tuning (FT) methods while reducing computational and memory costs. SMT's ability to overcome performance plateau issues associated with low-rank adaptation methods like LoRA is a notable advantage. The approach's focus on attention layers, particularly V vectors, is also insightful. The paper presents a clear and well-organized structure, and the experiments demonstrate SMT's effectiveness across various tasks.\",\n  \"Weaknesses\": \"One potential limitation of SMT is its reliance on gradient-based information for selecting sparse sub-matrices, which might not be effective for all models or tasks. The authors should provide more insight into how SMT handles cases where the gradient-based selection fails or is not informative. Additionally, the paper could benefit from a more detailed discussion on the choice of the sp"}
{"paper_id": "ZZVOrId3yN", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed architecture of CrossModalNet is well-structured and employs advanced mathematical frameworks and innovative domain adaptation techniques. The use of a dual-stream cross-network design, U-shaped parallel feature network, Swin Transformer, and Cross Transformer is a significant contribution to the field of multimodal medical image segmentation. The Joint Adversarial Domain Adaptation framework is also a notable feature of this architecture. The experimental results on the MM-WHS dataset demonstrate superior performance and generalization.\",\n  \"Weaknesses\": \"The paper lacks clear explanations of the theoretical foundations and mathematical derivations of the proposed architecture. The use of advanced mathematical frameworks and innovative domain adaptation techniques is not fully justified, and more details are needed to understand the significance of these components. The experimental results are impressive, but it is unclear how the proposed architecture generalizes to other medical image segmentation tasks and datasets.\",\n  \"Questions\": \"How does the proposed architecture handle the domain shift between different imaging modalities and datasets? What are the computational complexities of the proposed architecture, and how do they compare to existing"}
{"paper_id": "gLHuAYGs6a", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed SMvCNet architecture effectively integrates multi-view information by capturing high-order sample structures across views, leading to superior clustering performance compared to existing methods. The use of heterogeneous random walks and a lightweight network to learn from both within-view and cross-view structures is an innovative approach. The experiments demonstrate the model's effectiveness on five diverse datasets.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanations of the random walk strategy and the lightweight network architecture. Additionally, the evaluation metrics used (ACC, NMI, ARI) are standard in the field, but it would be helpful to include more diverse metrics to assess the model's performance.\",\n  \"Questions\": \"1. How does the model handle cases where the views have vastly different structural characteristics? 2. Can the authors provide more insight into the choice of the number of random walk steps and its impact on the clustering performance? 3. Are there any plans to extend the model to handle more than two views?\"\n}"}
{"paper_id": "x5l5PRtvul", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes an effective solution to the variance collapse problem in SVGD, which is a significant contribution to the field of approximate Bayesian inference. The theoretical analysis and experimental results are thorough and well-presented. The paper provides a clear and concise explanation of the hybrid kernel SVGD (h-SVGD) and its advantages over traditional SVGD.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanations of the theoretical results, particularly the proof of the existence of solutions to the hybrid Stein partial differential equation. Additionally, the experiments could be more comprehensive, including a wider range of high-dimensional inference tasks and a more detailed analysis of the computational cost of h-SVGD compared to traditional SVGD.\",\n  \"Questions\": \"How does the choice of kernel for the driving and repulsive terms in h-SVGD affect the performance of the algorithm? Are there any specific guidelines for choosing the optimal kernel for a given problem? Can the authors provide more details on the implementation of the kernelised Wasserstein gradient flow in practice?\"\n}"}
{"paper_id": "pOUAVXnOQP", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel activation function, STAF, which can mitigate spectral bias in neural networks.\\n2. The use of Fourier series with trainable amplitude, frequency, and phase provides a more expressive activation function, which can enhance the ability to learn high-frequency details in signals.\\n3. The experiments conducted on various datasets demonstrate the superiority of STAF over existing methods like WIRE and SIREN in terms of PSNR and convergence speed.\",\n  \"Weaknesses\": \"1. The paper assumes that the spectral bias in ReLU-based networks is the primary limitation, but it may not be the only issue.\\n2. The authors do not provide a detailed analysis of the time complexity of the proposed method, which may be a concern for large-scale applications.\\n3. The comparison with other methods is limited to WIRE and SIREN, and it would be beneficial to include more comprehensive benchmarks.\",\n  \"Questions\": \"1. How does the proposed STAF activation function perform on other types of neural networks, such as"}
{"paper_id": "iN64nSYt0z", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a well-structured and clear solution to improve LLMs' instruction-following accuracy without requiring additional training or extensive computational resources. The proposed GUIDE method is easy to understand, and the Influence metric is a valuable tool for Transformer explainability and alignment. The authors provide a comprehensive evaluation and comparison with traditional methods, fine-tuning approaches, and other state-of-the-art models.\",\n  \"Weaknesses\": \"The authors could have explored more diverse and challenging tasks to demonstrate the robustness and generalizability of GUIDE. Additionally, the paper lacks a detailed analysis of the potential limitations and edge cases of the proposed method.\",\n  \"Questions\": \"How does GUIDE perform on tasks with complex or multi-step instructions? Can the Influence metric be applied to other NLP tasks beyond instruction-following? Are there any potential risks or biases associated with using a tagging system to bias attention scores?\"\n}"}
{"paper_id": "gRuZkEy49k", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach, MCDS, for improving the training efficiency of GFlowNets by leveraging MCTS.\\n2. The method demonstrates superiority over conventional on-policy approaches in various discrete environments.\\n3. The paper provides a clear and concise explanation of the proposed method and its implementation.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more thorough discussion of the connections between GFlowNets and maximum-entropy RL, as it is not entirely clear how MCDS leverages these connections.\\n2. The experimental results, while showing promising improvements, could be more comprehensive and include more diverse environments.\\n3. The paper assumes a basic understanding of GFlowNets and MCTS, which may not be familiar to all readers.\",\n  \"Questions\": \"1. How does MCDS handle situations where the search DAG is very large or complex?\\n2. The paper mentions that MCDS can be used as an off-policy sampling tool, but it's not clear how this affects the overall training"}
{"paper_id": "d23EVDRJ6g", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper introduces a novel approach to motion synthesis using a localized generative masked transformer, which shows promise in capturing and generating diverse motion patterns from a single reference.\\n2. The evaluation framework and metrics used to assess the model's performance are comprehensive and well-justified.\\n3. The model's ability to improve quantitative scores and excel in user perceptual assessments is a significant contribution to the field.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed discussion on the limitations and potential pitfalls of using localized masked transformers, particularly in terms of scalability and generalizability.\\n2. The comparison with existing methods could be more thorough, including a more detailed analysis of the trade-offs between the proposed approach and other state-of-the-art methods.\\n3. The paper assumes that the reference motion is available, but in many real-world scenarios, this may not be the case, and the model's performance in such cases is not evaluated.\",\n  \"Questions\": \"1. How does the model handle the case where"}
{"paper_id": "kQ5s9Yh0WI", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper addresses a significant limitation of current LLMs and provides a novel solution to extend their output length. The proposed AgentWrite pipeline and LongWriter-6k dataset are well-designed and can potentially benefit the field. The use of Direct Preference Optimization (DPO) for further improvement is also a strength.\",\n  \"Weaknesses\": \"The paper assumes that the scarcity of longer samples in SFT datasets is the primary limitation, but it may be a symptom of a more fundamental issue. The paper could have explored other possible reasons for the limited output length of LLMs. Additionally, the evaluation on LongBench-Write may not be comprehensive enough to fully demonstrate the capabilities of the proposed method.\",\n  \"Questions\": \"How do the authors address the potential overfitting issue when training models on the LongWriter-6k dataset? Are there any experiments to show that the AgentWrite pipeline is not just a simple reimplementation of existing techniques? Can the authors provide more details about the Direct Preference Optimization (DPO) method and its relation"}
{"paper_id": "9CqkpQExe2", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed Ada-K routing method is a significant improvement over traditional Top-K routing, offering better computational efficiency and model performance. The use of lightweight, pluggable allocator modules and the Proximal Policy Optimization algorithm are innovative and effective. The extensive evaluations on various MoE-based LLMs demonstrate the method's effectiveness. The paper is well-organized and clearly presented, with a logical flow of ideas.\",\n  \"Weaknesses\": \"The paper assumes that the MoE architecture is the best choice for large language models, which may not be the case for all applications. The method's performance may be sensitive to the choice of allocator modules and the PPO algorithm. The paper does not provide a detailed analysis of the computational overhead of the allocator modules.\",\n  \"Questions\": \"How does the method handle cases where the input sequence has varying token complexities, and how does it adapt to different task difficulties? Can the method be extended to other types of neural network architectures, such as transformer-based models? What is the impact of the number of allocator modules on"}
{"paper_id": "PpYy0dR3Qw", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors propose a novel algorithm, LoCoDL, that effectively combines Local Training and Communication Compression to achieve a doubly-accelerated communication complexity. The algorithm's theoretical framework is well-defined, and the authors provide comprehensive proofs for linear convergence. The LoCoDL algorithm is particularly suited for diverse data distributions, maintaining high efficiency across different regimes.\",\n  \"Weaknesses\": \"The paper focuses primarily on the theoretical aspects of LoCoDL, which might make it challenging for non-experts to follow. Additionally, the evaluation is limited to a single experiment, and it would be beneficial to provide more comprehensive results to support the claims.\",\n  \"Questions\": \"What are the specific implications of using unbiased compressors from the class U(\u03c9) in LoCoDL? How does the algorithm handle non-i.i.d. data distributions, and what are the potential challenges or limitations in such scenarios?\"\n}"}
{"paper_id": "UatDdAlr2x", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a clear and well-structured analysis of transformer architectures and their influence on counting tasks. The theoretical constructions developed to demonstrate how different strategies can implement counting tasks are well-explained and insightful. The experiments conducted on a generated dataset are comprehensive and validate the theoretical insights.\",\n  \"Weaknesses\": \"The study's focus on a single task, histogram counting, may limit its generalizability to other tasks. The analysis could benefit from a more extensive examination of the role of other transformer components, such as layer normalization or skip connections.\",\n  \"Questions\": \"How do the findings of this study relate to other tasks that require counting, such as arithmetic operations or sequence classification? Are there any potential applications of this research in areas like natural language processing or computer vision?\"\n}"}
{"paper_id": "IwgmgidYPS", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The MedTrinity-25M dataset creation pipeline is well-designed and scalable, utilizing domain-specific models to identify ROIs and retrieval-augmented generation to fetch medical knowledge. The dataset itself is a valuable resource for multimodal medical tasks and pretraining for medical AI models.\",\n  \"Weaknesses\": \"The reliance on multimodal large language models and medical knowledge retrieval might lead to biases in the generated annotations. The paper could benefit from a more detailed discussion on the potential impact of these biases and how they were mitigated.\",\n  \"Questions\": \"How was the quality of the generated annotations evaluated, and what steps were taken to ensure their accuracy? The paper mentions the use of retrieval-augmented generation, but it's unclear how this was implemented and what the specific retrieval models were. Additionally, it would be helpful to see more information on the specific domain-specific models used for ROI identification and how they were trained.\"\n}"}
{"paper_id": "WG7GzGx3G9", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed Rotated Runtime Smooth (RRS) method is a novel and effective approach to addressing activation outliers in low-bit quantization. The method's combination of runtime smoothing and rotation is a good strategy for managing outliers. The experimental results demonstrate a significant improvement in INT4 inference accuracy compared to state-of-the-art methods.\",\n  \"Weaknesses\": \"The paper assumes that the outlier removal process is a well-established task, but the proposed method is not thoroughly justified. The impact of the proposed method on the overall accuracy and computational cost needs to be further investigated. The comparison with other methods is limited to two state-of-the-art approaches, and more comprehensive comparisons are necessary. The authors do not discuss the scalability of the proposed method to larger models and more complex applications.\",\n  \"Questions\": \"How does the proposed method handle the case where the outliers are not channel-wise or spike outliers? What is the effect of the rotation operation on the accuracy and computational cost of the method? How does the proposed method compare with other outlier removal methods, such as the"}
{"paper_id": "BhECSDSkAE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel framework for adapting a pre-trained one-shot segmentation model to medical videos using a memory mechanism and self-distillation, which is a significant contribution to the field. The approach effectively segments medical videos with minimal data using static image datasets, outperforming alternative few-shot and cross-domain methods in low-data settings. The use of a memory bank and self-distillation is innovative and well-explained.\",\n  \"Weaknesses\": \"The paper assumes that the pre-trained one-shot segmentation model is available, which may not be the case in practice. Additionally, the method requires a large static image dataset for training, which may not be feasible in some medical domains. The paper also assumes that the memory bank is sufficient to capture the temporal characteristics of video data, which may not always be the case.\",\n  \"Questions\": \"Can the authors provide more details on how to select the key-value pairs for the memory bank? How does the memory bank handle the case where the video has a different number of frames than the training data? Can the"}
{"paper_id": "mnB4hDTIDr", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 3,\n  \"Strengths\": \"The paper presents a novel method, DCScore, for evaluating the diversity of LLM-generated datasets from a classification perspective. The method is computationally efficient and correlates well with human judgment and generation temperature. The paper provides a clear presentation of the method and its evaluation.\",\n  \"Weaknesses\": \"The paper assumes that the diversity evaluation can be treated as a classification task, which might not be universally applicable. The method's reliance on a kernel matrix and classification function may not capture all aspects of diversity. The paper does not discuss the potential limitations of using human judgment as a pseudo-truth for evaluating diversity.\",\n  \"Questions\": \"How does DCScore handle datasets with very high or very low diversity? Would it be beneficial to incorporate more diversity-related axioms or constraints into the classification function? Are there any potential applications of DCScore beyond evaluating LLM-generated datasets?\"\n}"}
{"paper_id": "Y0qmwm6tgy", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed MoreauPruner method provides a novel approach to pruning large language models (LLMs) by utilizing the Moreau envelope to estimate weight importance, leading to more stable and effective pruning outcomes. This method addresses the issue of weight perturbations caused by quantization errors or precision format changes, which can lead to inconsistent pruning results. The experimental results demonstrate the effectiveness of MoreauPruner in maintaining model performance across different weight formats and improving zero-shot performance.\",\n  \"Weaknesses\": \"The paper assumes that the Moreau envelope can accurately capture the importance of model weights, which might not be the case in all scenarios. Additionally, the method relies on a proximal gradient descent algorithm, which can be computationally expensive. Furthermore, the post-pruning fine-tuning phase with LoRA may not be necessary for all models, and its effect on the overall pruning outcome is not thoroughly explored.\",\n  \"Questions\": \"How does the Moreau envelope perform on models with different architectures, such as transformer-based models with different attention mechanisms? Can the"}
{"paper_id": "aAcOaJYbUg", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a new benchmark, LAION-C, that better evaluates the robustness of modern vision models, which is a significant contribution to the field. The creation of this benchmark and the evaluation of 58 vision models using it are well-executed. The paper also provides insights into the performance of modern models on this new benchmark, which is valuable.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanations of the distortion types used in LAION-C and the psychophysical experiments. Additionally, the authors could provide more information about the models used in the evaluation and the specific metrics used to compare their performance.\",\n  \"Questions\": \"How did the authors select the six synthetic distortion types used in LAION-C, and what was the motivation behind these choices? What are the implications of the results on the generalizability of modern vision models, and how do they compare to human-like strategies?\"\n}"}
{"paper_id": "duGygkA3QR", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 3,\n  \"Strengths\": \"The authors have effectively integrated Dynamic Mode Decomposition (DMD) with Graph Neural Networks (GNNs) to improve GNN performance across various graph learning tasks. This integration provides a sound theoretical basis and data-driven insights via the calculation of low-rank linear operators. The approach allows for incorporating domain-specific constraints like symmetry to reflect known physical properties of the systems involved. The experiments conducted on tasks including directed graphs, large-scale graphs, long-range interactions, spatial-temporal graphs, and link prediction demonstrate the effectiveness of the proposed method.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed explanation of the mathematical formulation of the DMD-GNN framework, particularly in the context of Koopman theory. The authors mention that the approach allows for incorporating domain-specific constraints, but the specific examples of such constraints are limited. Furthermore, the paper does not provide a comprehensive comparison with other methods that utilize dynamical systems theories in GNNs.\",\n  \"Questions\": \"How does the proposed DMD-GNN framework handle non-linear interactions between"}
{"paper_id": "s6nYndMwG7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method for setting hyperparameters for LiSSA is well-motivated and appears to be effective. The use of spectral properties of the Hessian to set the hyperparameters is a clever approach. The empirical validation of the proposed method across different models is also a strength.\",\n  \"Weaknesses\": \"The paper assumes that the Hessian is approximately low-rank, which may not hold for all models. The authors should provide more discussion on this assumption and its implications. Additionally, the paper could benefit from more detailed analysis of the computational cost of the proposed method.\",\n  \"Questions\": \"How do the authors handle the case where the Hessian is not approximately low-rank? What is the impact of this assumption on the accuracy of the influence function calculations? How does the proposed method compare to other methods for approximating the Hessian, such as the truncated SVD method?\"\n}"}
{"paper_id": "9BVMD3keG8", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The problem of brokerage in online learning is well-posed, and the authors provide a clear framework for understanding the role of contextual information in the problem. The proposed algorithms for full and two-bit feedback settings are well-justified and the results are convincing. The paper has a good narrative flow and is well-structured.\",\n  \"Weaknesses\": \"The paper's contribution is somewhat limited by the assumption of zero-mean noise with bounded density. The impact of this assumption on the generality of the results is not fully explored. The paper could benefit from a more detailed discussion of the implications of this assumption and the potential limitations of the proposed approach.\",\n  \"Questions\": \"What are the implications of relaxing the bounded density assumption on the learnability of the brokerage problem? How do the proposed algorithms perform in scenarios where the traders' valuations are not zero-mean or are not bounded?\"\n}"}
{"paper_id": "mnna9LUg7P", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 3,\n  \"Strengths\": \"The paper presents a well-structured and clear approach to quantizing State Space Models (SSMs) for efficient deployment on cloud and edge hardware. The proposed Quamba method is effective in reducing model size and improving deployment efficiency without significant accuracy loss. The experiments demonstrate the potential of SSMs in various applications.\",\n  \"Weaknesses\": \"The paper relies on a specific case study (Mamba 2.8B SSM) and does not provide a comprehensive comparison with other quantization methods. The evaluation on zero-shot tasks may not fully capture the model's performance in more complex scenarios. The paper could benefit from more detailed analysis on the impact of Hadamard transform on outlier suppression.\",\n  \"Questions\": \"How does the Quamba method perform on more complex SSMs with larger state spaces? What is the trade-off between accuracy and deployment efficiency for different quantization precision levels? How does the proposed method compare to other quantization techniques in terms of computational complexity and memory usage?\"\n}"}
{"paper_id": "3X3LuwzZrl", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper addresses an important task in MLNC, and the proposed method is an improvement over existing approaches. The framework is well-defined and easy to follow.\\n2. The paper provides a clear and concise explanation of the method, and the experimental results are presented in a clear and organized manner.\\n3. The method is well-motivated and the authors provide a clear justification for their approach.\",\n  \"Weaknesses\": \"1. The paper assumes that the label influence graph is known, which may not always be the case in real-world scenarios.\\n2. The method may not be scalable for large graphs due to the complexity of the label influence graph.\\n3. The paper could benefit from more detailed analysis of the trade-offs between different components of the method.\",\n  \"Questions\": \"1. How does the method handle cases where the label influence graph is not known or is incomplete?\\n2. Can the method be applied to other types of graph-structured data beyond MLNC?\\n3. How does the method compare"}
{"paper_id": "IqaQZ1Jdky", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The use of Bernstein polynomials to enhance the flexibility and robustness of KANs is an interesting and innovative approach. The proposed VBn-KAN framework demonstrates improved performance and adaptability across various datasets. The application of Bayesian optimization to dynamically adapt the polynomial degree is a valuable addition. The paper provides a clear and well-structured presentation of the method and its evaluation.\",\n  \"Weaknesses\": \"The paper assumes that the reader is familiar with the concept of KANs and the Weierstrass approximation theorem, which may not be the case for all readers. The use of Bernstein polynomials may not be intuitive for some readers, and a more detailed explanation of their benefits and limitations would be helpful. The experiments are conducted on a limited number of datasets, and it would be beneficial to include more diverse datasets to further establish the generality of the proposed method.\",\n  \"Questions\": \"How does the choice of Bernstein polynomials as the basis for the KAN framework impact the model's performance and interpretability? Are there any potential limitations or"}
{"paper_id": "OdMqKszKSd", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed method of using a diffusion-based generative model for articulated object generation from a single image is novel and effective. It demonstrates a good understanding of the problem and the state-of-the-art methods.\\n2. The method's ability to capture part shapes and motions from a single view while producing realistic 3D reconstructions is impressive.\\n3. The use of large pre-trained models like DINOv2 and GPT-4o enhances the generation's plausibility and detail accuracy.\\n4. The systematic evaluations on multiple datasets and user studies that affirm improved realism and consistency are well-designed and thorough.\",\n  \"Weaknesses\": \"1. The method relies heavily on the quality of the input image, which may not always be available or of high quality.\\n2. The authors do not provide a clear comparison with existing methods on the same datasets, making it difficult to evaluate the true novelty of their approach.\\n3. The method's scalability and applicability to real-world applications are not fully explored.\",\n  \"Questions"}
{"paper_id": "LOBhVTtVnc", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed GSTs model is well-designed to handle spatiotemporal graphs and respects physical symmetries, making it suitable for simulating long-term physical dynamics. The introduction of TDG is a significant contribution, as it enhances the model's ability to predict long-term trajectories accurately and efficiently. The experiments conducted on various scales demonstrate the model's effectiveness in mitigating cumulative errors and leveraging variable-length historical data.\",\n  \"Weaknesses\": \"The paper assumes that the input graphs are always connected and does not provide a clear explanation of how the model handles disconnected graphs. Additionally, the authors do not provide a thorough comparison with other state-of-the-art methods that respect physical symmetries, making it difficult to assess the model's novelty and impact.\",\n  \"Questions\": \"How does the model handle the case where the input graph is disconnected or has a complex topology? Can the authors provide more details on the implementation of the equivariant spatiotemporal blocks and the TDG module? Additionally, what are the computational requirements and scalability of the"}
{"paper_id": "oa5UeyUVMm", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a solid contribution to the field of graph representation learning, introducing Graffe, a self-supervised diffusion model that demonstrates competitive results in node and graph classification tasks. The theoretical insights provided form a solid foundation for future exploration of diffusion models in graph contexts. The experimental setup is well-designed, using 11 real-world datasets and GNNs like GAT and GIN for encoding, with a custom diffusion decoder enhancing the representational capacity.\",\n  \"Weaknesses\": \"The paper would benefit from more detailed explanations of the proposed Diff-InfoMax principle and its implications. Additionally, the authors could provide more insight into the choice of GNN architectures for encoding and the design of the diffusion decoder. The experiments are well-conducted, but it would be helpful to include more visualizations or quantitative comparisons to illustrate the effectiveness of Graffe.\",\n  \"Questions\": \"How does the Diff-InfoMax principle generalize to other types of data, such as images or sequential data? Can Graffe be adapted to handle more complex graph structures or larger graph"}
{"paper_id": "5swfKRkCx7", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method effectively addresses the integration of external knowledge in LLMs for QA tasks, and the memory-based approach for layer-wise integration is novel and well-executed. The experimental evaluation demonstrates the superiority of the proposed method over traditional RAG approaches and baseline LLM models.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed analysis of the performance differences between the proposed method and traditional RAG approaches. Additionally, the impact of the external knowledge attention mechanism on the model's interpretability and explainability could be explored in more depth.\",\n  \"Questions\": \"How does the proposed method handle the potential for biased knowledge retrieved from external sources? Can the model be adapted to handle more complex or multi-step reasoning tasks that require integrating knowledge from multiple sources?\"\n}"}
{"paper_id": "5s1qpjrNvZ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed Guided Reinforcement Learning (GRL) algorithm and its variant, GRL-RB, provide a novel approach to ensuring a performance guarantee in guided RL methods. The introduction of the roll-back feature allows for dynamic adjustments of the sampling rate, which is beneficial in various tasks and environments. The experimental validation on structured environments like Combination Lock and AntMaze showcases the efficacy of the proposed methods. The algorithm's implementation on the Implicit Q-Learning framework and comparisons with other methods demonstrate its robustness and performance.\",\n  \"Weaknesses\": \"The paper assumes a specific form of the guide policy and learner policy, which might limit its applicability to more complex tasks. The evaluation of the GRL and GRL-RB algorithms is primarily conducted on structured environments, which might not be representative of real-world scenarios. The authors do not provide a detailed analysis of the computational complexity of the proposed methods.\",\n  \"Questions\": \"Can the proposed methods be extended to handle more complex tasks or environments? How does the performance guarantee of the GRL and"}
{"paper_id": "WNb4P8aG66", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 2,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents an interesting approach to enhance image generation by leveraging visual priors in a diffusion process. The use of a Latent Embedding Module (LEM) to compress and reconstruct samples is a clever idea. The multi-stage framework, DoD, is well-motivated and potentially beneficial for image generation tasks.\",\n  \"Weaknesses\": \"While the idea of using visual priors is promising, the paper could benefit from a more detailed explanation of how the LEM module handles the trade-off between retaining semantic information and discarding redundant details. Additionally, the evaluation on a single dataset (ImageNet-256x256) may not be sufficient to fully demonstrate the effectiveness of DoD. It would be beneficial to include more experiments on different datasets and metrics to support the claims made in the paper.\",\n  \"Questions\": \"How does the LEM module determine the optimal balance between retaining semantic information and discarding redundant details? Could the authors provide more insight into this process? Are there any potential limitations or challenges in applying the DoD framework to other"}
{"paper_id": "UfczlMudN6", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach to generalization in deep RL by combining contextual adaptation and adversarial RL within a unified framework.\\n2. The GRAM algorithm demonstrates strong performance in both in-distribution and out-of-distribution scenarios, showing its adaptability and robustness.\\n3. The use of an epistemic neural network to quantify uncertainty is a good idea.\",\n  \"Weaknesses\": \"1. The paper relies heavily on simulated environments, and it is unclear how the GRAM algorithm would perform in real-world scenarios.\\n2. The comparison to existing methods is limited, and it would be beneficial to include more comprehensive evaluations.\\n3. The robust adaptation module may be computationally expensive and difficult to implement.\",\n  \"Questions\": \"1. How does the GRAM algorithm handle scenarios with multiple out-of-distribution contexts?\\n2. Can the robust adaptation module be adapted for other RL tasks besides locomotion?\\n3. What are the computational costs of implementing the epistemic neural network and the adversarial RL"}
{"paper_id": "60Vd7QOXlM", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper proposes an effective method to improve privacy auditing of LLMs by designing more effective canaries, which are more likely to be memorized. This advancement represents a new standard in privacy leakage detection, allowing for accurate estimation of empirical privacy loss.\\n2. The proposed methods are tested on several LLMs across different datasets and fine-tuning settings, including differentially private models, providing a comprehensive evaluation of the approach.\\n3. The authors demonstrate the superior performance of their canaries in privacy audits compared to previous methods, highlighting the significance of their contribution.\",\n  \"Weaknesses\": \"1. The paper assumes that the attacker has knowledge of the data distribution, which may not always be the case in real-world scenarios.\\n2. The effectiveness of the new token canary relies on the specific fine-tuning settings and datasets used, which may not generalize to all scenarios.\\n3. The authors do not provide a detailed analysis of the trade-off between the increased memorization of the canaries and the privacy risks they aim"}
{"paper_id": "GqhvJ1o8m5", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed ImmersePro framework is a well-designed approach to stereo video conversion, leveraging spatial-temporal attention and implicit disparity guidance. The introduction of the YouTube-SBS dataset is a valuable contribution to the field. The quantitative metrics, such as L1, SSIM, and PSNR, demonstrate the effectiveness of the method.\",\n  \"Weaknesses\": \"The paper assumes that the input video is already aligned, which may not always be the case in practice. The impact of the proposed method on more complex scenarios, such as videos with large occluded areas or significant camera movements, is not thoroughly explored.\",\n  \"Questions\": \"How does the proposed method handle videos with large occluded areas or significant camera movements? Can the spatial-temporal attention mechanism be further optimized to improve the accuracy of the stereo video conversion?\"\n}"}
{"paper_id": "ua5MHdsbck", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed data distillation method for extrapolative protein design by embedding triplet relations into preference learning models is well-motivated and innovative. The use of triplet ranking and reverse KL divergence is an effective approach to improve the design of proteins beyond the training region. The experiments demonstrate the effectiveness of the proposed method on AAV and GFP datasets, outperforming several baseline and state-of-the-art generative methods.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed explanation of the triplet relation embedding process, especially how the triplets are created and refined. Additionally, the choice of triplet ranking and reverse KL divergence as the optimization objective might be worth further discussion. The paper assumes a clear understanding of the preference distribution, but it's not clear if this assumption is always valid.\",\n  \"Questions\": \"How do the authors plan to scale the triplet relation embedding process to larger datasets? What is the computational cost of creating and refining the triplets, and how does it impact the overall training time? Can the authors provide more insight into the preference distribution"}
{"paper_id": "56Zn3halhq", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method, AutoTSAug, effectively identifies and augments marginal samples to improve time series forecasting accuracy. The use of a variational masked autoencoder (V-MAE) and reinforcement learning to optimize the augmentation process is a novel and effective approach. The method's ability to adaptively enhance training data and reduce prediction errors across a model zoo is a significant contribution to the field. The paper provides a clear and concise explanation of the method and its implementation, making it easy to follow and understand.\",\n  \"Weaknesses\": \"The method requires a robust model zoo for optimal results, which may not always be feasible in real-world scenarios. The paper does not provide a comprehensive analysis of the computational overhead of the method, which may be a concern for large-scale applications. Additionally, the method's performance on datasets with limited training data is not thoroughly evaluated, which is a potential limitation.\",\n  \"Questions\": \"How does the method handle cases where the model zoo is not robust or has limited diversity? Are there any strategies to adapt the method"}
{"paper_id": "aPTGvFqile", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors effectively address a significant issue in CLIP by proposing a method to reduce the modality gap between text and image embeddings. The approach is well-structured and easy to follow. The experiments demonstrate the effectiveness of AlignCLIP in improving cross-modal alignment and downstream tasks.\",\n  \"Weaknesses\": \"The paper assumes that the modality gap is solely due to the parameter-sharing and intra-modality separation objectives, which might not be the only contributing factors. The authors do not provide a clear explanation of why their approach outperforms previous naive gap closure techniques. Additionally, the paper could benefit from more comprehensive ablation studies to further understand the impact of each component.\",\n  \"Questions\": \"How does the proposed approach handle the case where the modality gap is not solely due to the parameter-sharing and intra-modality separation objectives? What are the limitations of the current approach, and how can they be addressed in future work? Are there any potential risks or challenges associated with sharing learnable parameters between modality encoders?\"\n}"}
{"paper_id": "GOwNImvCWf", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed composite loss function is well-motivated and effectively captures the importance of both structural and behavioral losses in learning weight space representations. The experiments are comprehensive and well-conducted, demonstrating the effectiveness of the proposed approach in various tasks.\",\n  \"Weaknesses\": \"The paper assumes that the original model's outputs are available for calculating the behavioral loss, which might not be the case in all scenarios. The authors should discuss potential workarounds for this limitation. Additionally, the paper could benefit from a more in-depth analysis of the trade-off between structural and behavioral losses.\",\n  \"Questions\": \"How does the proposed composite loss function perform when the original model's outputs are not available? Are there any potential applications of this approach in other domains or tasks beyond neural network weight reconstruction?\"\n}"}
{"paper_id": "C9BA0T3xhq", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a novel approach to offline RL by extending the utility of the Bellman update to actions not explicitly in the dataset, which is an important contribution. The use of expectile regression and Implicit Q-Learning is a good idea. The experimental results on the D4RL benchmark are impressive and show that EIQL outperforms traditional offline RL algorithms.\",\n  \"Weaknesses\": \"The paper assumes that the data distribution is stationary, which might not always be the case in real-world scenarios. The use of a Bernoulli random variable to balance between observed in-sample actions and out-of-sample estimations might not be sufficient to handle all types of distributional discrepancies. The paper does not provide a clear analysis of the computational complexity of the algorithm.\",\n  \"Questions\": \"How does EIQL handle the case where the data distribution is non-stationary? What are the implications of using a Bernoulli random variable to balance between observed in-sample actions and out-of-sample estimations? How does EIQL compare to other offline RL algorithms that"}
{"paper_id": "EeqlkPpaV8", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The study provides a comprehensive and rigorous analysis of the adaptive complexity of sampling algorithms, establishing lower bounds for log-concave distributions. The use of novel constructions of hardness potentials and properties of random partitions is a significant technical contribution. The results have important implications for the design of sampling algorithms, highlighting the limitations of current approaches and suggesting the need for new techniques.\",\n  \"Weaknesses\": \"The paper is dense and technical, which may make it challenging for non-experts to follow. The focus on log-concave distributions may limit the generalizability of the results to other types of distributions.\",\n  \"Questions\": \"How do the authors' results relate to other sampling algorithms, such as those based on Markov chain Monte Carlo methods? Can the lower bounds established in the paper be extended to other types of distributions, such as non-log-concave distributions?\"\n}"}
{"paper_id": "oK1zJCWBqf", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 2,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed Soft Preference Optimization (SPO) method is a viable alternative to existing reward-model-free approaches like Direct Preference Optimization (DPO). The use of a natural loss function that combines preference loss with comprehensive regularization across the model's output distribution is an interesting and useful direction. The paper provides a clear and concise description of the method and its advantages over other approaches.\",\n  \"Weaknesses\": \"The paper relies heavily on the Bradley-Terry model, which may not be familiar to all readers. The authors should provide a more detailed explanation of the model and its implications. Additionally, the paper could benefit from a more thorough comparison with other state-of-the-art methods, including RLHF, to demonstrate the superiority of SPO. The use of a 110M parameter model in the experiments may not be sufficient to demonstrate the scalability of SPO to larger models.\",\n  \"Questions\": \"How does SPO handle the case where the preference data is noisy or biased? Can the authors provide more details on the implementation of the regularization term and how it affects"}
{"paper_id": "Dq9VrVuLzV", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel approach to generating photorealistic and geometrically-controlled street view images using 3D semantic multi-plane images (MPIs). This innovation enables precise control over occupancy conditions for data augmentation and scenario testing in autonomous driving.\\nThe authors demonstrate the effectiveness of their method, SyntheOcc, on the nuScenes dataset, showing improvements in perception model training and controllable dataset generation.\\nThe paper is well-organized and clearly presented, with a good balance of theoretical and experimental results.\",\n  \"Weaknesses\": \"The paper relies on the assumption that the 3D semantic MPIs are accurately aligned with the generated pixels, which may not always be the case in real-world scenarios.\\nThe authors could have provided more extensive evaluation on the robustness of their method to variations in MPI alignment and generation.\\nThe paper does not provide a clear comparison with state-of-the-art methods for generating street view images, which would have strengthened its claims.\",\n  \"Questions\": \"How does the SyntheOcc method handle cases where the 3D semantic"}
{"paper_id": "73Q9U0vcja", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed Diffusion Active Learning (DAL) method is a well-designed solution to the problem of efficient CT reconstruction. The integration of generative diffusion models with active learning is an interesting and novel approach. The experiments demonstrate a significant reduction in X-ray exposure and improved reconstruction quality. The method's potential for broader application in scientific and materials imaging contexts is promising.\",\n  \"Weaknesses\": \"The paper would benefit from a more detailed discussion on the trade-offs and limitations of DAL in high-stakes applications like medical imaging. The authors should also provide more insights into the potential risks associated with reduced X-ray exposure. Additionally, the paper could be improved by including a more comprehensive comparison with existing methods and a clearer explanation of the generative diffusion model's role in the active learning process.\",\n  \"Questions\": \"How does the DAL method handle cases where the underlying data distribution is highly complex or non-stationary? What are the potential risks associated with relying on a pre-trained generative diffusion model in real-world applications? Can the authors provide more information on the"}
{"paper_id": "UiEjzBRYeI", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed SAM-CP method effectively addresses the limitations of SAM in semantic-aware segmentation tasks by introducing composable prompts, achieving state-of-the-art performance in open-vocabulary segmentation on various datasets.\\n2. The unified affinity framework and two types of composable prompts (Type-I and Type-II) demonstrate a novel and efficient approach to handling segmentation tasks.\\n3. The paper provides a clear and concise explanation of the method and its components, making it easy to follow and understand.\",\n  \"Weaknesses\": \"1. The authors rely on pre-trained models like SAM and BERT, which might limit the generalizability of the results.\\n2. The method assumes that the text labels are available, which might not be the case in all scenarios.\\n3. The paper lacks a thorough comparison with other state-of-the-art methods in semantic-aware segmentation tasks.\",\n  \"Questions\": \"1. How does the proposed method handle cases where the text labels are not available or are incomplete?\\n2. Can the composable prompts be adapted to"}
{"paper_id": "lessla98Wp", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 3,\n  \"Strengths\": \"The idea of using language descriptions to guide video colorization is innovative and promising. The model's ability to maintain temporal consistency and semantic accuracy is impressive. The use of a cross-modality pre-fusion module and temporally deformable attention is a good design choice.\",\n  \"Weaknesses\": \"The paper assumes that the user-provided text descriptions are accurate and relevant, which might not always be the case. The model's reliance on a large dataset with 100K text-video pairs might limit its generalizability to other domains or datasets. The evaluation metrics used seem to focus mainly on aesthetic quality, which might not capture other important aspects of video colorization.\",\n  \"Questions\": \"How does the model handle cases where the user-provided text descriptions are incomplete, inaccurate, or irrelevant? Can the model be adapted to work with different types of text inputs, such as voice commands or user queries? How does the model handle edge cases, such as videos with extreme lighting conditions or complex scenes?\"\n}"}
{"paper_id": "UstOpZCESc", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed framework effectively bridges lifelong learning with privacy-aware unlearning, achieving state-of-the-art performance and preventing catastrophic forgetting. The method demonstrates scalability across architectures and datasets, offering a memory-efficient solution for responsible AI applications.\\nThe authors provide a clear and well-structured presentation of the proposed approach, making it easy to follow and understand.\",\n  \"Weaknesses\": \"One potential limitation of the proposed method is its reliance on episodic memory rehearsal, which may not be feasible in all scenarios, especially when dealing with large-scale datasets. Additionally, the method's performance may degrade over time due to the accumulation of forgotten tasks.\",\n  \"Questions\": \"How does the method handle the case where the dataset is not sequentially observed, but rather in batches or chunks? Would the episodic memory rehearsal mechanism be effective in such scenarios?\\n\\nCan the authors provide more details on how the sparse subnetworks are learned and updated during the training process? Is there a mechanism to prevent over-specialization of the subnetworks for each task?\\n\\nHow does the"}
{"paper_id": "LnRegTxsa4", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel dual attention approach for cross-view object geo-localization, which significantly enhances geo-localization accuracy by improving feature interaction and representation. The creation of a new dataset, G2D, addresses the scarcity of comprehensive datasets for the cross-view object geo-localization task. The proposed method outperforms previous state-of-the-art methods across different datasets.\",\n  \"Weaknesses\": \"The paper lacks a clear explanation of the motivation behind the CVCAM and MHSAM components. The authors could have provided more details about the training process and hyperparameter tuning. The paper assumes that the reader is familiar with the cross-view object geo-localization task and does not provide a comprehensive overview of the related work.\",\n  \"Questions\": \"How does the CVCAM component handle the drastic viewpoint changes and scale variations between ground-view and aerial-view images? Can the authors provide more insights into the effectiveness of the MHSAM component in capturing multi-scale spatial features?\"\n}"}
{"paper_id": "2ErS9Bkc3O", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a solid theoretical analysis of neural network fragility, offering a comprehensive explanation for the phenomenon of adversarial attacks. The authors' matrix-theoretic approach is novel and sheds light on the relationship between input dimension and robustness. The theoretical predictions are validated through numerical experiments, which adds to the paper's credibility.\",\n  \"Weaknesses\": \"The paper's focus on a specific aspect of neural network fragility may limit its scope, making it less impactful in the broader context of deep learning research. The extension of the analysis to more general data distributions and network parameters would be beneficial. Additionally, the paper could benefit from a more detailed discussion on the implications of the results and potential avenues for future work.\",\n  \"Questions\": \"The authors claim that neural networks' adversarial robustness degrades with increasing input dimension. However, is there any empirical evidence to support this claim for real-world datasets, rather than just synthetic ones? Furthermore, how do the authors envision the relationship between adversarial robustness and network parameters such as depth and width"}
{"paper_id": "pK3oe2bubc", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 7,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel approach to training vision transformers for robustness to layer order changes, which is a relevant and timely topic.\\n2. The LayerShuffle method is well-motivated and has the potential to improve the performance of vision transformers in distributed computing environments.\\n3. The authors provide a clear and well-structured presentation of their approach and results.\",\n  \"Weaknesses\": \"1. The paper assumes that the layer order changes are random and uniformly distributed, which may not be the case in real-world scenarios.\\n2. The authors do not provide a thorough analysis of the computational overhead of the LayerShuffle method.\\n3. The paper could benefit from more extensive experimental evaluation, including more datasets and scenarios.\",\n  \"Questions\": \"1. How do the authors plan to extend the LayerShuffle method to handle more complex scenarios, such as multiple layer order changes or node failures?\\n2. What are the potential limitations of the LayerShuffle method in terms of computational overhead and memory requirements?\\n3"}
{"paper_id": "sec09tLQUl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper addresses a critical problem in deep learning, namely, the exploitation of spurious features and poor generalization on minority groups. The proposed FairDropout method is well-motivated and technically sound. The experiments demonstrate its effectiveness across multiple datasets and tasks. The method's ability to reduce reliance on spurious correlations without requiring explicit group annotations is a significant contribution.\",\n  \"Weaknesses\": \"The paper assumes that all memorization occurs in specific neurons, which may not always be the case. The method may not be effective if memorization is spread across multiple neurons. Additionally, the study acknowledges the potential beneficial aspects of memorization, which are not fully explored in this work. The scalability of FairDropout to larger models and datasets should be further investigated.\",\n  \"Questions\": \"How does FairDropout compare to other methods that aim to reduce memorization, such as regularization techniques? Can FairDropout be combined with other methods to further improve generalization? The paper mentions the potential beneficial aspects of memorization, but it is unclear how to"}
{"paper_id": "96jZFqM5E0", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 3,\n  \"Strengths\": \"The proposed SiMHand framework leverages large-scale, diverse datasets and more informative contrastive pairs in the self-supervised pre-training scheme, achieving significant improvements in 3D hand pose estimation. The adaptive weighting mechanism for the contrastive loss enhances the learning from truly similar samples. The method's effectiveness is demonstrated on various datasets, including FreiHand, DexYCB, and AssemblyHands.\",\n  \"Weaknesses\": \"The method relies on identifying similar hand poses using PCA-based dimension reduction, which might not be robust to variations in hand appearance and background. The use of Ego4D and 100DOH datasets might be limited to specific scenarios, and the method's generalizability to other domains is not thoroughly explored. The adaptive weighting mechanism may not handle cases where the hand poses are not similar enough to produce informative contrastive pairs.\",\n  \"Questions\": \"How does the method handle cases where the hand appearance and background vary significantly, potentially affecting the similarity of hand poses? What are the implications of relying on specific datasets like Ego"}
{"paper_id": "uNd289HjLi", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The Corruption2Self (C2S) framework is novel and extends the denoising score matching (DSM) to the ambient noise setting, which is a significant contribution.\\n2. The authors provide a clear explanation of the method and its components, including the Generalized Ambient Denoising Score Matching (GADSM) loss and the reparameterization of noise levels.\\n3. The experimental results are comprehensive and demonstrate the effectiveness of C2S in denoising MRI images without requiring high-SNR labels.\",\n  \"Weaknesses\": \"1. The paper assumes that the MRI data is corrupted by additive Gaussian noise, which might not be the case in all real-world scenarios.\\n2. The method requires a large number of corrupted observations to learn the denoising function, which can be computationally expensive.\\n3. The authors do not provide a clear explanation of how the detail refinement extension balances noise reduction and spatial detail preservation.\",\n  \"Questions\": \"1. How does the Corruption2Self framework handle non"}
{"paper_id": "e2ONKX6qzJ", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a clear and well-defined problem of oversaturation in high guidance scales for classifier-free guidance. The proposed solution, Adaptive Projected Guidance (APG), is well-justified and effectively addresses the issue. The experiments demonstrate the improved performance of APG compared to traditional CFG. The method is also compatible with multiple models and samplers, making it a practical solution.\",\n  \"Weaknesses\": \"The paper assumes a good understanding of diffusion models and classifier-free guidance, which may not be the case for all readers. The impact of the proposed method on other tasks, such as text-to-image synthesis, is not explored. The paper could benefit from more thorough ablation studies to evaluate the effectiveness of each component of APG.\",\n  \"Questions\": \"How does APG perform on more complex image synthesis tasks, such as generating diverse and realistic images? Can the authors provide more insights into the effect of the orthogonal projection and rescaling components on the performance of APG?\"\n}"}
{"paper_id": "a2eBgp4sjH", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a comprehensive theoretical framework for MultiFilterANN, addressing the gap in existing solutions. The novel graph-based algorithms provide the best-known space-time trade-offs, and the empirical evaluation demonstrates competitive performance. The release of new datasets will facilitate future research.\",\n  \"Weaknesses\": \"The paper focuses on the theoretical aspects and empirical evaluation, but it would be beneficial to provide more insights into the practical applications and potential limitations of the proposed methods.\",\n  \"Questions\": \"How does the proposed approach handle scenarios with a large number of filters, and what are the computational complexities of the algorithms in such cases? What are the potential extensions to other types of filter constraints, such as categorical or numerical filters?\"\n}"}
{"paper_id": "EIXZXPz7jU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors propose a novel sampling method for PINNs that addresses the challenges of singular PDE source functions. The approach is well-motivated and the experiments demonstrate its effectiveness. The paper is well-written and the presentation is clear.\",\n  \"Weaknesses\": \"The paper could benefit from a more thorough analysis of the computational cost of the proposed method. The authors mention that the approach is computationally efficient, but a more detailed comparison with traditional methods would be helpful. Additionally, the paper could benefit from a more extensive discussion of the limitations of the proposed method and potential future work.\",\n  \"Questions\": \"How does the proposed method handle high-dimensional PDEs? The authors mention that PINNs can struggle with high-dimensional problems, but it's not clear how the proposed method addresses this challenge. Can the authors provide more details on the computational cost of the proposed method and compare it with traditional methods? What are the potential limitations of the proposed method, and how can they be addressed in future work?\"\n}"}
{"paper_id": "QO4bF6MHza", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 3,\n  \"Strengths\": \"1. The paper proposes a new benchmark, MathHay, for evaluating long-context mathematical reasoning capabilities of large language models. This is a significant contribution to the field of natural language processing.\\n2. The benchmark is comprehensive and well-structured, consisting of multiple steps for collecting and generating reasoning questions, applying quality control checks, and creating challenging information environments.\\n3. The paper provides a clear and concise explanation of the benchmark and its evaluation process.\",\n  \"Weaknesses\": \"1. The paper does not provide a detailed comparison with existing benchmarks, making it difficult to understand the novelty of the proposed benchmark.\\n2. The evaluation of the benchmark only includes eight top-performing LLMs, which may not be representative of all LLMs.\\n3. The paper does not discuss potential limitations or challenges of the benchmark, such as the potential for overfitting or the need for more diverse and representative data.\",\n  \"Questions\": \"1. How does the MathHay benchmark handle cases where the reasoning question requires information from multiple documents,"}
{"paper_id": "GULx8rzzjC", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper addresses a timely and relevant problem in machine learning, specifically the issue of acquiring useful data subsets from external data owners under restricted data-sharing conditions. The proposed method, Mycroft, is well-motivated and provides a practical solution to this challenge. The experimental results demonstrate the efficacy of Mycroft in identifying useful data subsets and improving model performance with minimal data exposure. The paper is well-written and easy to follow.\",\n  \"Weaknesses\": \"One of the main limitations of the paper is the assumption that data owners are willing to share data based on functional similarity and feature similarity. In reality, data owners may have different priorities and constraints that are not captured by these metrics. Additionally, the paper does not provide a thorough analysis of the trade-off between data quality and data quantity. Furthermore, the paper assumes that the model trainer has a prior understanding of the task-related data samples, which may not always be the case.\",\n  \"Questions\": \"How does Mycroft handle cases where the data owner's priorities and constraints conflict with the model trainer's"}
{"paper_id": "PnZ2lbQaao", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed framework provides a clear and innovative approach to cross-domain recommendations by introducing domain indexing through an adversarial Bayesian framework. The method effectively improves recommendation performance and provides insights into domain relationships, enhancing interpretability.\",\n  \"Weaknesses\": \"The paper assumes that the domain indexing can be inferred accurately, which might not always be the case. Additionally, the evaluation on real-world datasets is limited to a single dataset, which may not be representative of all scenarios.\",\n  \"Questions\": \"How does the proposed method handle cases where the domain indexing is not accurate? Is there any analysis on the effect of domain index size on the performance of the method? Can the method be extended to handle multiple domains with different structures?\"\n}"}
{"paper_id": "AT64R0ivUO", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes an effective method to improve the comprehension of large language models (LLMs) in open-book QA tasks by steering their attention scores. This approach has the potential to significantly enhance the performance of LLMs in these tasks.\\n2. The method is well-motivated and the experimental results are impressive, showing substantial performance improvements over baseline prompting methods.\\n3. The paper is well-organized and clearly presented.\",\n  \"Weaknesses\": \"1. The paper assumes that the input context is already pre-processed, which might limit the applicability of the method in real-world scenarios where the input context is not pre-processed.\\n2. The paper does not provide a thorough analysis of the computational overhead of the method, especially for large input contexts.\\n3. The method relies on the LLM's attention heads, which might not be optimal for all tasks.\",\n  \"Questions\": \"1. How does the method perform when the input context is not pre-processed?\\n2. Can the method be extended"}
{"paper_id": "nGiGXLnKhl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel and efficient architecture for Vision Transformers (ViTs), addressing the high computational complexity of ViTs.\\n2. The incorporation of linear attention mechanisms like RWKV is an interesting approach to achieve scalability.\\n3. The experimental results show that VRWKV outperforms window-based ViTs in dense prediction tasks and offers a promising alternative for vision tasks.\",\n  \"Weaknesses\": \"1. The paper lacks a clear explanation of why the quad-directional shift (Q-Shift) is necessary, and how it improves the model's performance.\\n2. The bidirectional global attention (Bi-WKV) is mentioned as a key component, but its specific contributions to the model's efficiency and performance are not thoroughly explained.\\n3. The comparison with existing ViT models could be more comprehensive, including a discussion of the trade-offs between computational complexity and performance.\",\n  \"Questions\": \"1. How does the Q-Shift mechanism handle the trade-off between semantic expansion and computational complexity?\\n2. Are there any"}
{"paper_id": "TBJCtWTvXJ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper makes a clear and well-supported argument about the limitations of Adam and proposes a new optimizer, S3, that addresses these limitations. The theoretical analysis and experimental results are thorough and convincing. The paper is well-organized and well-written, making it easy to follow the authors' arguments and understand the contributions of the paper.\",\n  \"Weaknesses\": \"None apparent.\",\n  \"Questions\": \"How does the choice of p (in the generalized sign-like formulation) affect the performance of S3? Is there a principled way to choose p, or is it a hyperparameter that needs to be tuned? The paper mentions that S3 uses the same exponential moving average coefficient for both numerator and denominator momentums, but it's not clear why this is necessary. Is there a theoretical justification for this choice, or is it an empirical observation?\"\n}"}
{"paper_id": "8gSrJOL2oc", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes an effective framework for addressing the limitations of existing compositional zero-shot learning methods. The use of Multimodal Large Language Model (MLLM) embeddings and attribute smoothing is a novel approach. The evaluation on challenging datasets like MIT-States, C-GQA, and VAW-CZSL is comprehensive and systematic. The results demonstrate state-of-the-art performance and indicate the efficacy of the proposed method.\",\n  \"Weaknesses\": \"The paper assumes that the MLLM embeddings are superior to existing word embeddings, but it would be beneficial to compare them with other state-of-the-art word embeddings. The authors also mention that the attribute-object disentanglement is improved, but it would be helpful to provide more details about the disentanglement process and its impact on the overall performance. The paper could also benefit from a more detailed discussion on the limitations of the proposed method and potential future directions.\",\n  \"Questions\": \"How do the MLLM embeddings perform on other tasks and domains? Can the proposed method be extended to other"}
{"paper_id": "yheQRc5xWB", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a novel approach to time-varying counterfactual prediction using state-space models. The use of Covariate-based Decorrelation towards Selective Parameters (CDSP) to de-correlate current treatments from historical information is an interesting and potentially effective way to mitigate confounding bias. The empirical evaluation on various datasets demonstrates the superiority of the proposed method in terms of prediction accuracy and computational efficiency. The authors have also provided a clear and concise description of the methodology and results.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion on the theoretical foundations of the proposed approach. The authors mention that the state-space model is used to address the limitations of traditional methods in sequence modeling and bias correction, but a more explicit connection to the relevant literature would be helpful. Additionally, the paper could benefit from a more thorough analysis of the sensitivity of the results to the choice of hyperparameters and the specific dataset used.\",\n  \"Questions\": \"How does the proposed approach handle the issue of overfitting, especially when dealing with large"}
{"paper_id": "HxKSzulSD1", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a thought-provoking analysis of the weak-to-strong deception phenomenon in multi-objective alignment scenarios. The authors' use of a series of experiments involving different models and alignment settings is well-designed and helps to demonstrate the existence of this phenomenon. The paper also highlights the importance of addressing the reliability of superalignment in intelligent systems.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion on the implications of weak-to-strong deception, particularly in the context of real-world applications. Additionally, the authors could provide more information on the specific datasets used in the experiments and how they were preprocessed.\",\n  \"Questions\": \"1. How do the authors plan to address the weak-to-strong deception phenomenon in future work? Are there any potential solutions or mitigations that they propose?\\n2. Can the authors provide more information on the specific datasets used in the experiments, including how they were preprocessed and any relevant preprocessing steps?\\n3. How does the weak-to-strong deception phenomenon relate to other areas of research, such as"}
{"paper_id": "wH8XXUOUZU", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The proposed Deep Compression Autoencoder (DC-AE) addresses a significant challenge in latent diffusion models by efficiently managing high spatial compression ratios up to 128x without degrading reconstruction quality.\\n2. The techniques of Residual Autoencoding and Decoupled High-Resolution Adaptation demonstrate a clear improvement in optimization difficulty and generalization penalties.\\n3. The paper presents a well-structured and clear methodology for implementing DC-AE, along with a comprehensive comparison to existing models.\",\n  \"Weaknesses\": \"1. The authors could provide more detailed analysis of the impact of Residual Autoencoding and Decoupled High-Resolution Adaptation on the overall model performance and its components.\\n2. Although the paper presents some qualitative results, more quantitative comparisons with other state-of-the-art models would strengthen the evaluation.\\n3. The authors could discuss potential limitations and future directions for further improving the performance of DC-AE.\",\n  \"Questions\": \"1. How do the authors choose the specific values for the number of residual blocks and the"}
{"paper_id": "QWIg5e6mtT", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper introduces a novel framework, Heterogeneous-PSRO, that addresses a significant gap in existing research by allowing for heterogeneous policies in team games. The method's ability to achieve monotonic improvement in team rewards is a significant contribution. The experimental results on complex benchmarks demonstrate the effectiveness of the proposed approach.\",\n  \"Weaknesses\": \"The reliance on sequential optimization might be a limiting factor in extremely large-scale games. The adaptation to continuous action spaces needs further exploration. The paper assumes that teammates have a fixed set of roles, which might not be realistic in all scenarios.\",\n  \"Questions\": \"How does the proposed framework handle dynamic role assignments or unexpected teammate actions? Can the method be extended to handle multiple teams with diverse roles and objectives? The paper does not provide a clear comparison with other state-of-the-art methods for heterogeneous team games. Can the authors provide more details on the computational complexity of the proposed approach?\"\n}"}
{"paper_id": "kbSU5bwoRv", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors propose a novel zero-shot singing voice conversion model, SaMoye, which outperforms existing models in zero-shot SVC tasks, especially under challenging conditions. The model's ability to disentangle features into content, timbre, and pitch, and its use of a large and diverse dataset for training, are notable strengths. The work highlights the importance of comprehensive feature disentanglement in advancing SVC technology.\",\n  \"Weaknesses\": \"The paper assumes a large and diverse dataset is available for training, which may not be the case in all scenarios. The use of multiple ASR models and compression methods may add complexity to the model. The paper does not provide a clear comparison with other state-of-the-art methods in terms of computational efficiency.\",\n  \"Questions\": \"How does the model handle cases where the input singing voice is of poor quality or has significant background noise? Can the model be adapted for real-time singing voice conversion applications? What is the computational cost of the model, and how does it compare to other state-of-the"}
{"paper_id": "KyqtKhv6q1", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The authors address a significant gap in 3D perception systems by incorporating historical spatial priors, which can improve performance in tasks like 3D object detection and semantic map segmentation. The proposed Differentiable Map Priors (DMP) framework is efficient, scalable, and effectively utilizes historical context to enhance perception. The extensive experiments on the nuScenes dataset demonstrate the effectiveness of DMP across various baseline architectures.\",\n  \"Weaknesses\": \"While the paper presents a well-structured approach to incorporating historical spatial priors, it would be beneficial to provide more details on how the map representation is updated when new data is collected. Additionally, the evaluation could be strengthened by including more metrics and comparing DMP with other state-of-the-art approaches that also leverage historical data.\",\n  \"Questions\": \"How does DMP handle the trade-off between incorporating historical priors and maintaining adaptability to new, unseen environments? What are the computational costs associated with updating the map representation, and are there any strategies to mitigate these costs?\"\n}"}
{"paper_id": "lTL4t68BNc", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel method, RGA-IB, that explicitly incorporates the Information Bottleneck (IB) principle into the graph attention mechanism, which is an elegant and effective approach to improving the robustness of GNNs against adversarial attacks. The study demonstrates significant improvements in node classification accuracy, showcasing the effectiveness of the proposed method.\",\n  \"Weaknesses\": \"The paper assumes that the reader is familiar with the Information Bottleneck principle and its application to GNNs, which may not be the case for all readers. Additionally, the experimental setup, such as the choice of datasets and attack scenarios, could be explored in more depth to better understand the robustness of the proposed method.\",\n  \"Questions\": \"How does the proposed method, RGA-IB, compare to other robust GNNs and graph attention methods in terms of computational efficiency? What are the implications of the proposed method on the scalability of GNNs in large-scale graph-structured data? Can the authors provide more insights into the relationship between the Information Bottleneck principle"}
{"paper_id": "hWmwL9gizZ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a well-motivated and well-structured approach to improving the accuracy and generalizability of immunogenicity prediction. The use of dual attention mechanisms and comprehensive pre-trained protein sequence and structure embeddings is a significant improvement over existing methods. The extensive experiments and comparison with existing methods demonstrate the effectiveness of the proposed approach.\",\n  \"Weaknesses\": \"One potential weakness is the reliance on a large dataset ('Immuno') which may not be representative of all pathogens. Additionally, the paper could benefit from a more thorough discussion of the limitations of the current methods and how ProVaccine addresses them.\",\n  \"Questions\": \"How does the paper handle the potential overfitting of the model to the large dataset 'Immuno'? Are there any plans to extend the model to other types of pathogens or to include more diverse datasets? The paper mentions that the dual attention mechanism is a key innovation, but could you provide more details on how it works and how it improves the performance of the model?\"\n}"}
{"paper_id": "IL85Ebjg9j", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed computational trust-based discounting method effectively resolves conflicts in multi-view classification, enhancing model reliability and achieving higher prediction accuracy. The results demonstrate significant improvements in the consistency and reliability of predictions across datasets compared to existing methods. The use of Binomial opinion theory to transform view-specific predictions into a degree of trust is a novel approach.\",\n  \"Weaknesses\": \"The paper assumes that the reliability of view-specific predictions can be accurately captured by Binomial opinion theory, which may not always be the case in real-world scenarios. The choice of metrics used to evaluate the method's reliability, such as Top-1 Accuracy and Fleiss' Kappa, may not be comprehensive. The paper does not provide a clear explanation of how the evidence network is constructed and used in the proposed methodology.\",\n  \"Questions\": \"How does the proposed method handle cases where the views are not only conflicting but also noisy or incomplete? What is the computational cost of constructing and updating the evidence network? Can the proposed method be applied to other types of multi-view data, such as"}
{"paper_id": "cnKhHxN3xj", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a clear and concise problem statement and hypothesis. The proposed method, Sparse Expansion, is well-defined and easy to understand. The experimental framework is comprehensive and well-organized. The use of Wasserstein distance as a metric to quantify neuronal entanglement is novel and well-motivated.\",\n  \"Weaknesses\": \"The paper assumes that the mixture of experts can effectively disentangle neurons without providing a detailed explanation of how this is achieved. The experimental results, while impressive, are limited to a single dataset and a single sparsification algorithm. The paper does not discuss the computational cost of the proposed method and its scalability to larger models.\",\n  \"Questions\": \"How does the mixture of experts disentangle neurons, and what is the theoretical justification for this approach? Can the proposed method be applied to other types of neural networks, not just large language models? What is the computational cost of the Sparse Expansion method, and how does it scale to larger models?\"\n}"}
{"paper_id": "dgR6i4TSng", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a well-motivated and original approach to parameter-efficient fine-tuning, leveraging quantum-inspired modules and generalized Pauli parameterizations. The method, Quantum-PEFT, shows a significant reduction in trainable parameters compared to conventional methods like LoRA, while maintaining competitive performance. The authors provide a clear and concise explanation of the approach and its benefits. The results on GLUE and CIFAR10 datasets are impressive, demonstrating the effectiveness of the proposed method.\",\n  \"Weaknesses\": \"One potential concern is the reliance on quantum-inspired modules, which might limit the applicability of the method to certain types of models or tasks. Additionally, the authors do not provide a detailed explanation of the computational complexity of the proposed method, which could be a limitation for large-scale applications. It would be beneficial to see more extensive experiments on a broader range of tasks and datasets to further evaluate the generalizability of Quantum-PEFT.\",\n  \"Questions\": \"How does the choice of quantum-inspired modules (specifically, the use of RY and controlled"}
{"paper_id": "ye1mxb79lw", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel bilevel Bayesian optimization algorithm, BILBO, which provides a theoretical guarantee of performance in terms of regret bounds and improves sample efficiency. The algorithm's ability to jointly optimize both levels and maintain trusted sets is an interesting and useful approach. The paper demonstrates the effectiveness of BILBO in both synthetic and real-world scenarios, providing substantial improvements over baseline methods. The authors provide a clear and concise explanation of the algorithm and its benefits.\",\n  \"Weaknesses\": \"One potential concern is the assumption of Gaussian processes for modeling functions, which may not be suitable for all types of problems. Additionally, the algorithm's performance may depend on the choice of hyperparameters, and the authors do not provide a detailed analysis of this aspect. The paper could also benefit from a more extensive evaluation of the algorithm's robustness to different types of noise and constraints.\",\n  \"Questions\": \"How does the algorithm handle non-convex lower-level problems? Are there any theoretical guarantees for such cases? What is the computational cost of maintaining the trusted sets"}
{"paper_id": "1fwZJzGdKj", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"1. The paper proposes a novel multi-agent collaborative data selection mechanism for improving data efficiency in LLM pretraining. This approach is effective in resolving inherent conflicts between different data selection methods.\\n2. The experimental results demonstrate a significant improvement in data efficiency and acceleration of convergence in LLM pretraining.\\n3. The framework effectively mitigates conflicts between different data selection strategies by dynamically adjusting their integration based on model performance.\",\n  \"Weaknesses\": \"1. The paper could benefit from a more detailed discussion on the potential conflicts between different data selection methods and how the proposed framework addresses these conflicts.\\n2. The evaluation metrics used in the experiments could be more comprehensive, including additional metrics such as model interpretability and explainability.\\n3. The paper assumes that the agents are independent and do not interact with each other, which may not be the case in real-world scenarios.\",\n  \"Questions\": \"1. How does the proposed framework handle cases where multiple agents have conflicting priorities or goals?\\n2. Can the framework be extended to include other"}
{"paper_id": "pISLZG7ktL", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper makes a significant contribution to the field of robotics by establishing data scaling laws for imitation learning, which is a crucial step towards achieving zero-shot generalization. The authors' experimental results demonstrate the effectiveness of their approach, and the paper is well-written and easy to follow. The use of a standardized protocol for evaluation and the emphasis on diversity in data collection are notable strengths.\",\n  \"Weaknesses\": \"One potential weakness of the paper is the limited scope of the experiments. While the authors demonstrate the effectiveness of their approach on a specific set of tasks, it is unclear whether this approach would generalize to other tasks or environments. Additionally, the paper could benefit from a more in-depth discussion of the theoretical foundations of their approach and how it relates to existing work in the field.\",\n  \"Questions\": \"How do the authors plan to extend their approach to more complex tasks or environments? What are the potential limitations of using a power-law relationship to model generalization performance? Are there any potential applications of this work in other areas of robotics or machine learning?\""}
{"paper_id": "dML3XGvWmy", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The self-evolving framework inspired by the G\u00f6del machine is an original and innovative approach to agent design, allowing for recursive self-improvement without relying on predefined routines or fixed optimization algorithms.\\n2. The experiments demonstrate significant performance gains across various tasks compared to traditional agent systems, showcasing the G\u00f6del Agent's adaptability and efficiency.\\n3. The paper provides novel insights into agent design and offers a promising direction for developing flexible, self-improving AI systems.\",\n  \"Weaknesses\": \"1. The paper lacks a clear discussion on the limitations of the G\u00f6del Agent framework, such as potential risks of uncontrolled self-improvement or the impact on explainability.\\n2. The experiments, although promising, are limited to a few tasks, and it would be beneficial to see more comprehensive evaluations across a broader range of domains and tasks.\\n3. The paper does not provide a detailed comparison with other self-improving agent frameworks, making it difficult to fully appreciate the G\u00f6del Agent's advantages and disadvantages.\",\n  \""}
{"paper_id": "xlxGsX1pc7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper addresses a significant gap in the field by providing a comprehensive benchmark for evaluating LLMs on university-level mathematical problems, including visual elements.\\n2. The introduction of \u00b5-MATH as a supplementary dataset for meta-evaluation is a valuable contribution to the field.\\n3. The evaluation of various LLMs on the U-MATH benchmark provides a thorough assessment of their mathematical reasoning capabilities.\\n4. The paper highlights the limitations of current LLMs in processing complex and advanced mathematical problems, which is an important insight.\",\n  \"Weaknesses\": \"1. The paper relies heavily on existing LLMs for evaluation, which may not be representative of future models.\\n2. The selection of 1,100 open-ended university-level problems may not be exhaustive, and it would be beneficial to include a broader range of topics and visual elements.\\n3. The paper does not provide a clear explanation of how the LLMs are judged for solution correctness in the \u00b5-MATH benchmark.\\n4. The evaluation of L"}
{"paper_id": "AjXkRZIvjB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The GSM-Symbolic benchmark is a significant contribution to the field, providing a more comprehensive evaluation of LLMs' reasoning capabilities. The study's large-scale analysis and controlled experiments demonstrate the fragility of LLMs in reasoning tasks, highlighting the need for further research. The findings have important implications for the development of more robust and reliable LLMs.\",\n  \"Weaknesses\": \"The study's conclusions may be too broad, as they are based on a specific set of LLMs and a limited number of question types. The GSM8K dataset may not be representative of all mathematical reasoning tasks, and the results may not generalize to more complex or nuanced problems. The study's focus on symbolic templates may also limit its applicability to other areas of reasoning.\",\n  \"Questions\": \"How do the results of this study compare to those of previous evaluations of LLMs' reasoning capabilities? Are there any potential limitations or biases in the GSM-Symbolic benchmark that could impact its validity? What are the implications of the study's findings for"}
{"paper_id": "IJiTI0fB0e", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method effectively quantifies internal diversity in prompt-based generative models, separating prompt-induced from model-induced diversity. The Conditional-Vendi and Information-Vendi scores show consistency with known diversity and relevance benchmarks across different generative tasks.\",\n  \"Weaknesses\": \"The authors could provide more detailed information about the experimental setup, such as the specific text prompts used for evaluation and the range of generated samples. Additionally, the paper could benefit from a more in-depth analysis of the strengths and limitations of the proposed metrics.\",\n  \"Questions\": \"Can the Conditional-Vendi and Information-Vendi scores be applied to other types of generative models, such as those using different architectures or input modalities? How do the scores perform when evaluated on datasets with varying levels of diversity and relevance?\"\n}"}
{"paper_id": "fs2Z2z3GRx", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 3,\n  \"Strengths\": \"The proposed method, FIG, is an effective and efficient approach to linear inverse problems, particularly under challenging conditions. The use of measurement interpolants to guide the reverse-time sampling process is a novel and theoretically justified scheme. The experiments demonstrate competitive results and a good balance between unconditional generation and measurement guidance.\",\n  \"Weaknesses\": \"The paper assumes a pre-trained flow or diffusion model, which may limit the applicability of the method to specific tasks. The proposed approach may not be as effective for non-linear inverse problems. The paper could benefit from more detailed analysis of the convergence and stability of the FIG method.\",\n  \"Questions\": \"How does the choice of pre-trained flow or diffusion model impact the performance of FIG? Are there any scenarios where the FIG method may not be effective or efficient? Can the proposed method be extended to non-linear inverse problems?\"\n}"}
{"paper_id": "H4FSx06FCZ", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper proposes a novel steganography framework, SecureGS, which efficiently and securely hides 3D content within 3D Gaussian splatting. The framework's hybrid decoupled Gaussian encryption mechanism and density region-aware anchor growing and pruning strategy are innovative and effective.\\nThe paper provides extensive experiments on various datasets, demonstrating SecureGS's higher fidelity, speed, and security compared to existing methods.\",\n  \"Weaknesses\": \"The paper assumes a good understanding of 3D Gaussian splatting and steganography, which may not be accessible to all readers. The paper could benefit from a more detailed explanation of the SecureGS framework and its components. Furthermore, the paper does not discuss potential attacks or security vulnerabilities of the proposed method.\",\n  \"Questions\": \"How does the hybrid decoupled Gaussian encryption mechanism ensure the security of the hidden information? What are the potential limitations of the density region-aware anchor growing and pruning strategy? Can the authors provide more information on the computational complexity of the proposed method?\"\n}"}
{"paper_id": "OGtUfA6Amo", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed method, Hi-Patch, effectively captures both fine-grained and coarse-grained dependencies in IMTS, leveraging intra-patch and inter-patch graph layers to process scales progressively. The experimental evaluation on eight datasets indicates that Hi-Patch outperforms state-of-the-art models in both forecasting and classification tasks, demonstrating its robustness and efficacy. The hierarchical patch graph network is a novel and effective approach to handle IMTS, and the results are impressive.\",\n  \"Weaknesses\": \"While the proposed method, Hi-Patch, is a significant contribution to handling IMTS, it may not be suitable for all types of time series data. The model's performance may degrade in cases where the sampling intervals are extremely irregular or the scales of variables are highly disparate. Additionally, the authors could have provided more insights into the impact of different hyperparameters on the model's performance.\",\n  \"Questions\": \"How does the model handle cases where the sampling intervals are extremely irregular or the scales of variables are highly disparate? Can the authors provide more insights into"}
{"paper_id": "ujpAYpFDEA", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper provides a thorough investigation of watermark imperceptibility in LLMs, which is a crucial aspect of text watermarking. The proposed Water-Probe algorithm and Water-Bag strategy demonstrate the effectiveness of detecting watermarked LLMs. The study highlights the importance of improving watermark imperceptibility and introduces new strategies to achieve this goal.\",\n  \"Weaknesses\": \"The paper focuses primarily on detecting watermarked LLMs rather than addressing the issue of watermark imperceptibility directly. The Water-Bag strategy may have a trade-off in detectability, which could be a concern for practical deployment. The authors should consider exploring more effective methods to improve watermark imperceptibility.\",\n  \"Questions\": \"How do the authors plan to address the potential trade-off between watermark imperceptibility and detectability? Can the Water-Probe algorithm be adapted to detect watermarks in other types of models, such as image or video models?\"\n}"}
{"paper_id": "7UKHNQIErp", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 10,\n  \"Confidence\": 5,\n  \"Strengths\": \"1. The paper presents a clear and novel approach to formalizing DPA loss functions using symbolic logic.\\n2. The framework provides a structured landscape of DPA losses, enabling systematic exploration and derivation of new variants.\\n3. The method's ability to link symbolic expressions with the semantics of DPA losses is a significant contribution.\",\n  \"Weaknesses\": \"1. The paper relies on existing DPA losses and algorithms, and it's unclear how the framework would perform on novel or unknown losses.\\n2. The method's scalability and applicability to large-scale DPA problems are not thoroughly discussed.\\n3. The paper assumes a probabilistic logic framework, which might limit its applicability to other types of DPA losses.\",\n  \"Questions\": \"1. How does the framework handle DPA losses that are not expressible in probabilistic logic?\\n2. Can the framework be adapted to handle continuous or non-discrete DPA losses?\\n3. The paper mentions the potential for deriving new loss functions, but how does the"}
{"paper_id": "CpgWRFqxhD", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 4,\n  \"Strengths\": \"The proposed MEMO model effectively addresses the challenges of audio-driven talking video generation. The introduction of a memory-guided temporal module and an emotion-aware audio module is well-motivated and improves long-term identity consistency, motion smoothness, and natural expression generation. The ablation studies confirm the importance of these components. The paper provides a clear and well-organized presentation of the method and results.\",\n  \"Weaknesses\": \"The paper could benefit from more discussion on the limitations of the proposed method, particularly in terms of its reliance on large datasets and computational resources. Additionally, the paper does not provide a thorough comparison with other state-of-the-art methods in terms of computational efficiency and scalability.\",\n  \"Questions\": \"How does the memory-guided temporal module handle long-term temporal information in cases where the audio input is not synchronized with the video? What are the potential limitations of the emotion-aware audio module in handling complex emotional expressions or nuanced emotional cues?\"\n}"}
